Id,Abstract,Date,Primary_category,Methods
http://arxiv.org/abs/2401.17509v1,"Realistic video simulation has shown significant potential across diverse
applications, from virtual reality to film production. This is particularly
true for scenarios where capturing videos in real-world settings is either
impractical or expensive. Existing approaches in video simulation often fail to
accurately model the lighting environment, represent the object geometry, or
achieve high levels of photorealism. In this paper, we propose Anything in Any
Scene, a novel and generic framework for realistic video simulation that
seamlessly inserts any object into an existing dynamic video with a strong
emphasis on physical realism. Our proposed general framework encompasses three
key processes: 1) integrating a realistic object into a given scene video with
proper placement to ensure geometric realism; 2) estimating the sky and
environmental lighting distribution and simulating realistic shadows to enhance
the light realism; 3) employing a style transfer network that refines the final
video output to maximize photorealism. We experimentally demonstrate that
Anything in Any Scene framework produces simulated videos of great geometric
realism, lighting realism, and photorealism. By significantly mitigating the
challenges associated with video data generation, our framework offers an
efficient and cost-effective solution for acquiring high-quality videos.
Furthermore, its applications extend well beyond video data augmentation,
showing promising potential in virtual reality, video editing, and various
other video-centric applications. Please check our project website
https://anythinginanyscene.github.io for access to our project code and more
high-resolution video results.",2024-01-30,cs.CV,
http://arxiv.org/abs/2401.17505v3,"We study the probabilistic modeling performed by Autoregressive Large
Language Models (LLMs) through the angle of time directionality, addressing a
question first raised in (Shannon, 1951). For large enough models, we
empirically find a time asymmetry in their ability to learn natural language: a
difference in the average log-perplexity when trying to predict the next token
versus when trying to predict the previous one. This difference is at the same
time subtle and very consistent across various modalities (language, model
size, training time, ...). Theoretically, this is surprising: from an
information-theoretic point of view, there should be no such difference. We
provide a theoretical framework to explain how such an asymmetry can appear
from sparsity and computational complexity considerations, and outline a number
of perspectives opened by our results.",2024-01-30,cs.LG,
http://arxiv.org/abs/2401.17504v1,"Machine unlearning requires removing the information of forgetting data while
keeping the necessary information of remaining data. Despite recent
advancements in this area, existing methodologies mainly focus on the effect of
removing forgetting data without considering the negative impact this can have
on the information of the remaining data, resulting in significant performance
degradation after data removal. Although some methods try to repair the
performance of remaining data after removal, the forgotten information can also
return after repair. Such an issue is due to the intricate intertwining of the
forgetting and remaining data. Without adequately differentiating the influence
of these two kinds of data on the model, existing algorithms take the risk of
either inadequate removal of the forgetting data or unnecessary loss of
valuable information from the remaining data. To address this shortcoming, the
present study undertakes a causal analysis of the unlearning and introduces a
novel framework termed Causal Machine Unlearning (CaMU). This framework adds
intervention on the information of remaining data to disentangle the causal
effects between forgetting data and remaining data. Then CaMU eliminates the
causal impact associated with forgetting data while concurrently preserving the
causal relevance of the remaining data. Comprehensive empirical results on
various datasets and models suggest that CaMU enhances performance on the
remaining data and effectively minimizes the influences of forgetting data.
Notably, this work is the first to interpret deep model unlearning tasks from a
new perspective of causality and provide a solution based on causal analysis,
which opens up new possibilities for future research in deep model unlearning.",2024-01-30,cs.LG,
http://arxiv.org/abs/2402.01763v2,"This survey explores the synergistic potential of Large Language Models
(LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving
research area. With the proliferation of LLMs comes a host of challenges,
including hallucinations, outdated knowledge, prohibitive commercial
application costs, and memory issues. VecDBs emerge as a compelling solution to
these issues by offering an efficient means to store, retrieve, and manage the
high-dimensional vector representations intrinsic to LLM operations. Through
this nuanced review, we delineate the foundational principles of LLMs and
VecDBs and critically analyze their integration's impact on enhancing LLM
functionalities. This discourse extends into a discussion on the speculative
future developments in this domain, aiming to catalyze further research into
optimizing the confluence of LLMs and VecDBs for advanced data handling and
knowledge extraction capabilities.",2024-01-30,cs.DB,
http://arxiv.org/abs/2401.17500v2,"This paper introduces LeTO, a method for learning constrained visuomotor
policy via differentiable trajectory optimization. Our approach uniquely
integrates a differentiable optimization layer into the neural network. By
formulating the optimization layer as a trajectory optimization problem, we
enable the model to end-to-end generate actions in a safe and controlled
fashion without extra modules. Our method allows for the introduction of
constraints information during the training process, thereby balancing the
training objectives of satisfying constraints, smoothing the trajectories, and
minimizing errors with demonstrations. This ""gray box"" method marries the
optimization-based safety and interpretability with the powerful
representational abilities of neural networks. We quantitatively evaluate LeTO
in simulation and on the real robot. In simulation, LeTO achieves a success
rate comparable to state-of-the-art imitation learning methods, but the
generated trajectories are of less uncertainty, higher quality, and smoother.
In real-world experiments, we deployed LeTO to handle constraints-critical
tasks. The results show the effectiveness of LeTO comparing with
state-of-the-art imitation learning approaches. We release our code at
https://github.com/ZhengtongXu/LeTO.",2024-01-30,cs.RO,
http://arxiv.org/abs/2401.17499v2,"The multi-agent perception system collects visual data from sensors located
on various agents and leverages their relative poses determined by GPS signals
to effectively fuse information, mitigating the limitations of single-agent
sensing, such as occlusion. However, the precision of GPS signals can be
influenced by a range of factors, including wireless transmission and
obstructions like buildings. Given the pivotal role of GPS signals in
perception fusion and the potential for various interference, it becomes
imperative to investigate whether specific GPS signals can easily mislead the
multi-agent perception system. To address this concern, we frame the task as an
adversarial attack challenge and introduce \textsc{AdvGPS}, a method capable of
generating adversarial GPS signals which are also stealthy for individual
agents within the system, significantly reducing object detection accuracy. To
enhance the success rates of these attacks in a black-box scenario, we
introduce three types of statistically sensitive natural discrepancies:
appearance-based discrepancy, distribution-based discrepancy, and task-aware
discrepancy. Our extensive experiments on the OPV2V dataset demonstrate that
these attacks substantially undermine the performance of state-of-the-art
methods, showcasing remarkable transferability across different point cloud
based 3D detection systems. This alarming revelation underscores the pressing
need to address security implications within multi-agent perception systems,
thereby underscoring a critical area of research.",2024-01-30,cs.CV,
http://arxiv.org/abs/2401.17498v2,"QA models are faced with complex and open-ended contextual reasoning
problems, but can often learn well-performing solution heuristics by exploiting
dataset-specific patterns in their training data. These patterns, or ""dataset
artifacts"", reduce the model's ability to generalize to real-world QA problems.
Utilizing an ElectraSmallDiscriminator model trained for QA, we analyze the
impacts and incidence of dataset artifacts using an adversarial challenge set
designed to confuse models reliant on artifacts for prediction. Extending
existing work on methods for mitigating artifact impacts, we propose
cartographic inoculation, a novel method that fine-tunes models on an optimized
subset of the challenge data to reduce model reliance on dataset artifacts. We
show that by selectively fine-tuning a model on ambiguous adversarial examples
from a challenge set, significant performance improvements can be made on the
full challenge dataset with minimal loss of model generalizability to other
challenging environments and QA datasets.",2024-01-30,cs.CL,
http://arxiv.org/abs/2401.17497v1,"Syntax is usually studied in the realm of linguistics and refers to the
arrangement of words in a sentence. Similarly, an image can be considered as a
visual 'sentence', with the semantic parts of the image acting as 'words'.
While visual syntactic understanding occurs naturally to humans, it is
interesting to explore whether deep neural networks (DNNs) are equipped with
such reasoning. To that end, we alter the syntax of natural images (e.g.
swapping the eye and nose of a face), referred to as 'incorrect' images, to
investigate the sensitivity of DNNs to such syntactic anomaly. Through our
experiments, we discover an intriguing property of DNNs where we observe that
state-of-the-art convolutional neural networks, as well as vision transformers,
fail to discriminate between syntactically correct and incorrect images when
trained on only correct ones. To counter this issue and enable visual syntactic
understanding with DNNs, we propose a three-stage framework- (i) the 'words'
(or the sub-features) in the image are detected, (ii) the detected words are
sequentially masked and reconstructed using an autoencoder, (iii) the original
and reconstructed parts are compared at each location to determine syntactic
correctness. The reconstruction module is trained with BERT-like masked
autoencoding for images, with the motivation to leverage language model
inspired training to better capture the syntax. Note, our proposed approach is
unsupervised in the sense that the incorrect images are only used during
testing and the correct versus incorrect labels are never used for training. We
perform experiments on CelebA, and AFHQ datasets and obtain classification
accuracy of 92.10%, and 90.89%, respectively. Notably, the approach generalizes
well to ImageNet samples which share common classes with CelebA and AFHQ
without explicitly training on them.",2024-01-30,cs.CV,
http://arxiv.org/abs/2401.17486v1,"Responsible design of AI systems is a shared goal across HCI and AI
communities. Responsible AI (RAI) tools have been developed to support
practitioners to identify, assess, and mitigate ethical issues during AI
development. These tools take many forms (e.g., design playbooks, software
toolkits, documentation protocols). However, research suggests that use of RAI
tools is shaped by organizational contexts, raising questions about how
effective such tools are in practice. To better understand how RAI tools are --
and might be -- evaluated, we conducted a qualitative analysis of 37
publications that discuss evaluations of RAI tools. We find that most
evaluations focus on usability, while questions of tools' effectiveness in
changing AI development are sidelined. While usability evaluations are an
important approach to evaluate RAI tools, we draw on evaluation approaches from
other fields to highlight developer- and community-level steps to support
evaluations of RAI tools' effectiveness in shaping AI development practices and
outcomes.",2024-01-30,cs.HC,
http://arxiv.org/abs/2401.17484v3,"Understanding terrain topology at long-range is crucial for the success of
off-road robotic missions, especially when navigating at high-speeds. LiDAR
sensors, which are currently heavily relied upon for geometric mapping, provide
sparse measurements when mapping at greater distances. To address this
challenge, we present a novel learning-based approach capable of predicting
terrain elevation maps at long-range using only onboard egocentric images in
real-time. Our proposed method is comprised of three main elements. First, a
transformer-based encoder is introduced that learns cross-view associations
between the egocentric views and prior bird-eye-view elevation map predictions.
Second, an orientation-aware positional encoding is proposed to incorporate the
3D vehicle pose information over complex unstructured terrain with multi-view
visual image features. Lastly, a history-augmented learn-able map embedding is
proposed to achieve better temporal consistency between elevation map
predictions to facilitate the downstream navigational tasks. We experimentally
validate the applicability of our proposed approach for autonomous offroad
robotic navigation in complex and unstructured terrain using real-world offroad
driving data. Furthermore, the method is qualitatively and quantitatively
compared against the current state-of-the-art methods. Extensive field
experiments demonstrate that our method surpasses baseline models in accurately
predicting terrain elevation while effectively capturing the overall terrain
topology at long-ranges. Finally, ablation studies are conducted to highlight
and understand the effect of key components of the proposed approach and
validate their suitability to improve offroad robotic navigation capabilities.",2024-01-30,cs.RO,
http://arxiv.org/abs/2401.17483v1,"In this paper, we build on a recent bicategorical model called thin spans of
groupoids, introduced by Clairambault and Forest. Notably, thin spans feature a
decomposition of symmetry into two sub-groupoids of polarized -- positive and
negative -- symmetries. We first construct a variation of the original
exponential of thin spans, based on sequences rather than families. Then we
give a syntactic characterisation of the interpretation of simply-typed
lambda-terms in thin spans, in terms of rigid intersection types and rigid
resource terms. Finally, we formally relate thin spans with the weighted
relational model and generalized species of structure. This allows us to show
how some quantities in those models reflect polarized symmetries: in particular
we show that the weighted relational model counts witnesses from generalized
species of structure, divided by the cardinal of a group of positive
symmetries.",2024-01-30,cs.LO,
http://arxiv.org/abs/2401.17482v1,"Choosing and developing performant database solutions helps organizations
optimize their operational practices and decision-making. Since graph data is
becoming more common, it is crucial to develop and use them in big data with
complex relationships with high and consistent performance. However, legacy
database technologies such as MySQL are tailored to store relational databases
and need to perform more complex queries to retrieve graph data. Previous
research has dealt with performance aspects such as CPU and memory usage. In
contrast, energy usage and temperature of the servers are lacking. Thus, this
paper evaluates and compares state-of-the-art graphs and relational databases
from the performance aspects to allow a more informed selection of
technologies. Graph-based big data applications benefit from informed selection
database technologies for data retrieval and analytics problems. The results
show that Neo4j performs faster in querying connected data than MySQL and
ArangoDB, and energy, CPU, and memory usage performances are reported in this
paper.",2024-01-30,cs.DB,
http://arxiv.org/abs/2401.17481v1,"This paper addresses the challenging problem of energy-efficient and
uncertainty-aware pose estimation in insect-scale drones, which is crucial for
tasks such as surveillance in constricted spaces and for enabling non-intrusive
spatial intelligence in smart homes. Since tiny drones operate in highly
dynamic environments, where factors like lighting and human movement impact
their predictive accuracy, it is crucial to deploy uncertainty-aware prediction
algorithms that can account for environmental variations and express not only
the prediction but also confidence in the prediction. We address both of these
challenges with Compute-in-Memory (CIM) which has become a pivotal technology
for deep learning acceleration at the edge. While traditional CIM techniques
are promising for energy-efficient deep learning, to bring in the robustness of
uncertainty-aware predictions at the edge, we introduce a suite of novel
techniques: First, we discuss CIM-based acceleration of Bayesian filtering
methods uniquely by leveraging the Gaussian-like switching current of CMOS
inverters along with co-design of kernel functions to operate with extreme
parallelism and with extreme energy efficiency. Secondly, we discuss the
CIM-based acceleration of variational inference of deep learning models through
probabilistic processing while unfolding iterative computations of the method
with a compute reuse strategy to significantly minimize the workload. Overall,
our co-design methodologies demonstrate the potential of CIM to improve the
processing efficiency of uncertainty-aware algorithms by orders of magnitude,
thereby enabling edge robotics to access the robustness of sophisticated
prediction frameworks within their extremely stringent area/power resources.",2024-01-30,cs.RO,
http://arxiv.org/abs/2401.17480v1,"Crafting neural network architectures manually is a formidable challenge
often leading to suboptimal and inefficient structures. The pursuit of the
perfect neural configuration is a complex task, prompting the need for a
metaheuristic approach such as Neural Architecture Search (NAS). Drawing
inspiration from the ingenious mechanisms of nature, this paper introduces
Collaborative Ant-based Neural Topology Search (CANTS-N), pushing the
boundaries of NAS and Neural Evolution (NE). In this innovative approach,
ant-inspired agents meticulously construct neural network structures,
dynamically adapting within a dynamic environment, much like their natural
counterparts. Guided by Particle Swarm Optimization (PSO), CANTS-N's colonies
optimize architecture searches, achieving remarkable improvements in mean
squared error (MSE) over established methods, including BP-free CANTS, BP
CANTS, and ANTS. Scalable, adaptable, and forward-looking, CANTS-N has the
potential to reshape the landscape of NAS and NE. This paper provides detailed
insights into its methodology, results, and far-reaching implications.",2024-01-30,cs.NE,
http://arxiv.org/abs/2401.17477v1,"In the digital era, the prevalence of depressive symptoms expressed on social
media has raised serious concerns, necessitating advanced methodologies for
timely detection. This paper addresses the challenge of interpretable
depression detection by proposing a novel methodology that effectively combines
Large Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and
conversational agents like ChatGPT. In our methodology, explanations are
achieved by integrating BERTweet, a Twitter-specific variant of BERT, into a
novel self-explanatory model, namely BERT-XDD, capable of providing both
classification and explanations via masked attention. The interpretability is
further enhanced using ChatGPT to transform technical explanations into
human-readable commentaries. By introducing an effective and modular approach
for interpretable depression detection, our methodology can contribute to the
development of socially responsible digital platforms, fostering early
intervention and support for mental health challenges under the guidance of
qualified healthcare professionals.",2024-01-30,cs.CL,
http://arxiv.org/abs/2401.17474v1,"The Kaczmarz algorithm is an iterative technique designed to solve consistent
linear systems of equations. It falls within the category of row-action
methods, focusing on handling one equation per iteration. This characteristic
makes it especially useful in solving very large systems. The recent
introduction of a randomized version, the Randomized Kaczmarz method, renewed
interest in the algorithm, leading to the development of numerous variations.
Subsequently, parallel implementations for both the original and Randomized
Kaczmarz method have since then been proposed. However, previous work has
addressed sparse linear systems, whereas we focus on solving dense systems. In
this paper, we explore in detail approaches to parallelizing the Kaczmarz
method for both shared and distributed memory for large dense systems. In
particular, we implemented the Randomized Kaczmarz with Averaging (RKA) method
that, for inconsistent systems, unlike the standard Randomized Kaczmarz
algorithm, reduces the final error of the solution. While efficient
parallelization of this algorithm is not achievable, we introduce a block
version of the averaging method that can outperform the RKA method.",2024-01-30,cs.DC,
http://arxiv.org/abs/2402.00076v1,"The Conditional Markov Chain Search (CMCS) is a framework for automated
design of metaheuristics for discrete combinatorial optimisation problems.
Given a set of algorithmic components such as hill climbers and mutations, CMCS
decides in which order to apply those components. The decisions are dictated by
the CMCS configuration that can be learnt offline. CMCS does not have an
acceptance criterion; any moves are accepted by the framework. As a result, it
is particularly good in exploration but is not as good at exploitation. In this
study, we explore several extensions of the framework to improve its
exploitation abilities. To perform a computational study, we applied the
framework to the three-index assignment problem. The results of our experiments
showed that a two-stage CMCS is indeed superior to a single-stage CMCS.",2024-01-30,cs.AI,
http://arxiv.org/abs/2402.00075v1,"D-Nikud, a novel approach to Hebrew diacritization that integrates the
strengths of LSTM networks and BERT-based (transformer) pre-trained model.
Inspired by the methodologies employed in Nakdimon, we integrate it with the
TavBERT pre-trained model, our system incorporates advanced architectural
choices and diverse training data. Our experiments showcase state-of-the-art
results on several benchmark datasets, with a particular emphasis on modern
texts and more specified diacritization like gender.",2024-01-30,cs.CL,
http://arxiv.org/abs/2401.17464v2,"To achieve faithful reasoning that aligns with human expectations, large
language models (LLMs) need to ground their reasoning to real-world knowledge
(e.g., web facts, math and physical rules). Tools help LLMs access this
external knowledge, but there remains challenges for fine-tuning LLM agents
(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where
inter-connected tool calls require holistic and efficient tool usage planning.
  In this work, we propose a new method for LLMs to better leverage tools in
multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to
first decode reasoning chains with abstract placeholders, and then call domain
tools to reify each reasoning chain by filling in specific knowledge. This
planning with abstract chains enables LLMs to learn more general reasoning
strategies, which are robust to shifts of domain knowledge (e.g., math results)
relevant to different reasoning questions. It also allows LLMs to perform
decoding and calling of external tools in parallel, which avoids the inference
delay caused by waiting for tool responses. In mathematical reasoning and Wiki
QA domains, we show that our method consistently outperforms previous
chain-of-thought and tool-augmented baselines on both in-distribution and
out-of-distribution test sets, with an average ~6% absolute QA accuracy
improvement. LLM agents trained with our method also show more efficient tool
use, with inference speed being on average ~1.4x faster than baseline
tool-augmented LLMs.",2024-01-30,cs.CL,
http://arxiv.org/abs/2401.17461v1,"Linear programming (LP) problems are pervasive in real-life applications.
However, despite their apparent simplicity, an untrained user may find it
difficult to determine the linear model of their specific problem. We envisage
the creation of a goal-oriented conversational agent that will engage in
conversation with the user to elicit all information required so that a
subsequent agent can generate the linear model. In this paper, we present an
approach for the generation of sample dialogues that can be used to develop and
train such a conversational agent. Using prompt engineering, we develop two
agents that ""talk"" to each other, one acting as the conversational agent, and
the other acting as the user. Using a set of text descriptions of linear
problems from NL4Opt available to the user only, the agent and the user engage
in conversation until the agent has retrieved all key information from the
original problem description. We also propose an extrinsic evaluation of the
dialogues by assessing how well the summaries generated by the dialogues match
the original problem descriptions. We conduct human and automatic evaluations,
including an evaluation approach that uses GPT-4 to mimic the human evaluation
metrics. The evaluation results show an overall good quality of the dialogues,
though research is still needed to improve the quality of the GPT-4 evaluation
metrics. The resulting dialogues, including the human annotations of a subset,
are available to the research community. The conversational agent used for the
generation of the dialogues can be used as a baseline.",2024-01-30,cs.CL,
http://arxiv.org/abs/2401.17459v1,"Large language models (LLM) are perceived to offer promising potentials for
automating security tasks, such as those found in security operation centers
(SOCs). As a first step towards evaluating this perceived potential, we
investigate the use of LLMs in software pentesting, where the main task is to
automatically identify software security vulnerabilities in source code. We
hypothesize that an LLM-based AI agent can be improved over time for a specific
security task as human operators interact with it. Such improvement can be
made, as a first step, by engineering prompts fed to the LLM based on the
responses produced, to include relevant contexts and structures so that the
model provides more accurate results. Such engineering efforts become
sustainable if the prompts that are engineered to produce better results on
current tasks, also produce better results on future unknown tasks. To examine
this hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains
2,740 hand-crafted source code test cases containing various types of
vulnerabilities. We divide the test cases into training and testing data, where
we engineer the prompts based on the training data (only), and evaluate the
final system on the testing data. We compare the AI agent's performance on the
testing data against the performance of the agent without the prompt
engineering. We also compare the AI agent's results against those from
SonarQube, a widely used static code analyzer for security testing. We built
and tested multiple versions of the AI agent using different off-the-shelf LLMs
-- Google's Gemini-pro, as well as OpenAI's GPT-3.5-Turbo and GPT-4-Turbo (with
both chat completion and assistant APIs). The results show that using LLMs is a
viable approach to build an AI agent for software pentesting that can improve
through repeated use and prompt engineering.",2024-01-30,cs.CR,
http://arxiv.org/abs/2401.17451v1,"Unmanned aerial vehicles (UAVs) are envisioned to provide diverse services
from the air. The service quality may rely on the wireless performance which is
affected by the UAV's position. In this paper, we focus on the UAV placement
problem in the Internet of Vehicles, where the UAV is deployed to monitor the
road traffic and sends the monitored videos to vehicles. The studied problem is
formulated as video resolution maximization by optimizing over the UAV's
position. Moreover, we take into account the maximal transmission delay and
impose a probabilistic constraint. To solve the formulated problem, we first
leverage the techniques in extreme value theory (EVT) and Gaussian process
regression (GPR) to characterize the influence of the UAV's position on the
delay performance. Based on this characterization, we subsequently propose a
proactive resolution selection and UAV placement approach, which adaptively
places the UAV according to the geographic distribution of vehicles. Numerical
results justify the joint usage of EVT and GPR for maximal delay
characterization. Through investigating the maximal transmission delay, the
proposed approach nearly achieves the optimal performance when vehicles are
evenly distributed, and reduces 10% and 19% of the 999-th 1000-quantile over
two baselines when vehicles are biased distributed.",2024-01-30,cs.NI,
http://arxiv.org/abs/2404.16839v1,"Virtual reality and related technologies such as mixed and augmented reality
have received extensive coverage in both mainstream and fringe media outlets.
When the subject goes to a new AR headset, another AR device, or AR glasses,
the talk swiftly shifts to the technical and design details. Unfortunately, no
one seemed to care about security. Data theft and other forms of cyberattack
pose serious threats to virtual reality systems. Virtual reality goggles are
just specialist versions of computers or Internet of Things devices, whereas
virtual reality experiences are software packages. As a result, AR systems are
just as vulnerable as any other Internet of Things (IoT) device we use on a
daily basis, such as computers, tablets, and phones. Preventing and responding
to common cybersecurity threats and assaults is crucial. Cybercriminals can
exploit virtual reality headsets just like any other computer system. This
paper analysis the data breach induced by these assaults could result in a
variety of concerns, including but not limited to identity theft, the
unauthorized acquisition of personal information or network credentials, damage
to hardware and software, and so on. Augmented reality (AR) allows for
real-time monitoring and visualization of network activity, system logs, and
security alerts. This allows security professionals to immediately identify
threats, monitor suspicious activities, and fix any issues that develop. This
data can be displayed in an aesthetically pleasing and intuitively structured
format using augmented reality interfaces, enabling for faster analysis and
decision-making.",2024-01-30,cs.CR,
http://arxiv.org/abs/2401.17444v2,"We present a new language semantics for real-time concurrency. Its
operational models are higher-dimensional timed automata (HDTAs), a
generalization of both higher-dimensional automata and timed automata. We
define languages of HDTAs as sets of interval-timed pomsets with interfaces. As
an application, we show that language inclusion of HDTAs is undecidable. On the
other hand, using a region construction we can show that untimings of HDTA
languages have enough regularity so that untimed language inclusion is
decidable.",2024-01-30,cs.FL,
http://arxiv.org/abs/2401.17443v1,"We argue that there is a strong connection between ensemble learning and a
delegative voting paradigm -- liquid democracy -- that can be leveraged to
reduce ensemble training costs. We present an incremental training procedure
that identifies and removes redundant classifiers from an ensemble via
delegation mechanisms inspired by liquid democracy. Through both analysis and
extensive experiments we show that this process greatly reduces the
computational cost of training compared to training a full ensemble. By
carefully selecting the underlying delegation mechanism, weight centralization
in the classifier population is avoided, leading to higher accuracy than some
boosting methods. Furthermore, this work serves as an exemplar of how
frameworks from computational social choice literature can be applied to
problems in nontraditional domains.",2024-01-30,cs.LG,
http://arxiv.org/abs/2401.17441v1,"Explainable AI has brought transparency into complex ML blackboxes, enabling,
in particular, to identify which features these models use for their
predictions. So far, the question of explaining predictive uncertainty, i.e.
why a model 'doubts', has been scarcely studied. Our investigation reveals that
predictive uncertainty is dominated by second-order effects, involving single
features or product interactions between them. We contribute a new method for
explaining predictive uncertainty based on these second-order effects.
Computationally, our method reduces to a simple covariance computation over a
collection of first-order explanations. Our method is generally applicable,
allowing for turning common attribution techniques (LRP, Gradient x Input,
etc.) into powerful second-order uncertainty explainers, which we call CovLRP,
CovGI, etc. The accuracy of the explanations our method produces is
demonstrated through systematic quantitative evaluations, and the overall
usefulness of our method is demonstrated via two practical showcases.",2024-01-30,cs.LG,
http://arxiv.org/abs/2401.17436v1,"Difficulty is one of the key drivers of player engagement and it is often one
of the aspects that designers tweak most to optimise the player experience;
operationalising it is, therefore, a crucial task for game development studios.
A common practice consists of creating metrics out of data collected by player
interactions with the content; however, this allows for estimation only after
the content is released and does not consider the characteristics of potential
future players.
  In this article, we present a number of potential solutions for the
estimation of difficulty under such conditions, and we showcase the results of
a comparative study intended to understand which method and which types of data
perform better in different scenarios.
  The results reveal that models trained on a combination of cohort statistics
and simulated data produce the most accurate estimations of difficulty in all
scenarios. Furthermore, among these models, artificial neural networks show the
most consistent results.",2024-01-30,cs.AI,
http://arxiv.org/abs/2401.17435v3,"Economic choice prediction is an essential challenging task, often
constrained by the difficulties in acquiring human choice data. Indeed,
experimental economics studies had focused mostly on simple choice settings.
The AI community has recently contributed to that effort in two ways:
considering whether LLMs can substitute for humans in the above-mentioned
simple choice prediction settings, and the study through ML lens of more
elaborated but still rigorous experimental economics settings, employing
incomplete information, repetitive play, and natural language communication,
notably language-based persuasion games. This leaves us with a major
inspiration: can LLMs be used to fully simulate the economic environment and
generate data for efficient human choice prediction, substituting for the
elaborated economic lab studies? We pioneer the study of this subject,
demonstrating its feasibility. In particular, we show that a model trained
solely on LLM-generated data can effectively predict human behavior in a
language-based persuasion game, and can even outperform models trained on
actual human data.",2024-01-30,cs.LG,
http://arxiv.org/abs/2402.00072v1,"With the adoption of machine learning into routine clinical practice comes
the need for Explainable AI methods tailored to medical applications. Shapley
values have sparked wide interest for locally explaining models. Here, we
demonstrate their interpretation strongly depends on both the summary statistic
and the estimator for it, which in turn define what we identify as an 'anchor
point'. We show that the convention of using a mean anchor point may generate
misleading interpretations for survival analysis and introduce median-SHAP, a
method for explaining black-box models predicting individual survival times.",2024-01-30,cs.LG,
http://arxiv.org/abs/2401.17434v2,"Hackathons and software competitions, increasingly pivotal in the software
industry, serve as vital catalysts for innovation and skill development for
both organizations and students. These platforms enable companies to prototype
ideas swiftly, while students gain enriched learning experiences, enhancing
their practical skills. Over the years, hackathons have transitioned from mere
competitive events to significant educational tools, fusing theoretical
knowledge with real-world problem-solving. The integration of hackathons into
computer science and software engineering curricula aims to align educational
proficiencies within a collaborative context, promoting peer connectivity and
enriched learning via industry-academia collaborations. However, the infusion
of advanced technologies, notably artificial intelligence (AI), and machine
learning, into hackathons is revolutionizing their structure and outcomes. This
evolution brings forth both opportunities, like enhanced learning experiences,
and challenges, such as ethical concerns. This study delves into the impact of
generative AI, examining its influence on student's technological choices based
on a case study on the University of Iowa 2023 event. The exploration provides
insights into AI's role in hackathons, and its educational implications, and
offers a roadmap for the integration of such technologies in future events,
ensuring innovation is balanced with ethical and educational considerations.",2024-01-30,cs.CY,
http://arxiv.org/abs/2401.17428v1,"Currently, the development of the metaverse lies in the hands of industry.
Citizens have little influence on this process. Instead, to do justice to the
pluralism of (digital) societies, we should strive for an open discourse
including many different perspectives on the metaverse and its core
technologies such as AI. We utilize a participatory speculative design (PSD)
approach to explore Japanese citizens' perspectives on future metaverse
societies, as well as social and ethical implications. Our contributions are
twofold. Firstly, we demonstrate the effectiveness of PSD in engaging citizens
in critical discourse on emerging technologies like the metaverse, offering our
workshop framework as a methodological contribution. Secondly, we identify key
themes from participants' perspectives, providing insights for culturally
sensitive design and development of virtual environments. Our analysis shows
that participants imagine the metaverse to have the potential to solve a
variety of societal issues; for example, breaking down barriers of physical
environments for communication, social interaction, crisis preparation, and
political participation, or tackling identity-related issues. Regarding future
metaverse societies, participants' imaginations raise critical questions about
human-AI relations, technical solutionism, politics and technology,
globalization and local cultures, and immersive technologies. We discuss
implications and contribute to expanding conversations on metaverse
developments.",2024-01-30,cs.CY,
http://arxiv.org/abs/2401.17426v1,"We present a theoretical analysis of the performance of transformer with
softmax attention in in-context learning with linear regression tasks. While
the existing literature predominantly focuses on the convergence of
transformers with single-/multi-head attention, our research centers on
comparing their performance. We conduct an exact theoretical analysis to
demonstrate that multi-head attention with a substantial embedding dimension
performs better than single-head attention. When the number of in-context
examples D increases, the prediction loss using single-/multi-head attention is
in O(1/D), and the one for multi-head attention has a smaller multiplicative
constant. In addition to the simplest data distribution setting, we consider
more scenarios, e.g., noisy labels, local examples, correlated features, and
prior knowledge. We observe that, in general, multi-head attention is preferred
over single-head attention. Our results verify the effectiveness of the design
of multi-head attention in the transformer architecture.",2024-01-30,cs.LG,
http://arxiv.org/abs/2401.17418v1,"This study explores modeling and control for quadrotor acrobatics, focusing
on executing flip maneuvers. Flips are an elegant way to deliver sensor probes
into no-fly or hazardous zones, like volcanic vents. Successful flips require
feasible trajectories and precise control, influenced by rotor dynamics, thrust
allocation, and control methodologies. The research introduces a novel approach
using Model Predictive Control (MPC) for real-time trajectory planning. The MPC
considers dynamic constraints and environmental variables, ensuring system
stability during maneuvers. The proposed methodology's effectiveness is
examined through simulation studies in ROS and Gazebo, providing insights into
quadrotor behavior, response time, and trajectory accuracy. Real-time flight
experiments on a custom agile quadrotor using PixHawk 4 and Hardkernel Odroid
validate MPC-designed controllers. Experiments confirm successful execution and
adaptability to real-world scenarios. Outcomes contribute to autonomous aerial
robotics, especially aerial acrobatics, enhancing mission capabilities. MPC
controllers find applications in probe throws and optimal image capture views
through efficient flight paths, e.g., full roll maneuvers. This research paves
the way for quadrotors in demanding scenarios, showcasing groundbreaking
applications. Video Link: \url{ https://www.youtube.com/watch?v=UzR0PWjy9W4}",2024-01-30,cs.RO,
http://arxiv.org/abs/2401.17417v1,"This work presents a seminal approach for synthesizing images from WiFi
Channel State Information (CSI) in through-wall scenarios. Leveraging the
strengths of WiFi, such as cost-effectiveness, illumination invariance, and
wall-penetrating capabilities, our approach enables visual monitoring of indoor
environments beyond room boundaries and without the need for cameras. More
generally, it improves the interpretability of WiFi CSI by unlocking the option
to perform image-based downstream tasks, e.g., visual activity recognition. In
order to achieve this crossmodal translation from WiFi CSI to images, we rely
on a multimodal Variational Autoencoder (VAE) adapted to our problem specifics.
We extensively evaluate our proposed methodology through an ablation study on
architecture configuration and a quantitative/qualitative assessment of
reconstructed images. Our results demonstrate the viability of our method and
highlight its potential for practical applications.",2024-01-30,cs.CV,
http://arxiv.org/abs/2402.00071v2,"The current focus in Autonomous Experimentation (AE) is on developing robust
workflows to conduct the AE effectively. This entails the need for well-defined
approaches to guide the AE process, including strategies for hyperparameter
tuning and high-level human interventions within the workflow loop. This paper
presents a comprehensive analysis of the influence of initial experimental
conditions and in-loop interventions on the learning dynamics of Deep Kernel
Learning (DKL) within the realm of AE in Scanning Probe Microscopy. We explore
the concept of 'seed effect', where the initial experiment setup has a
substantial impact on the subsequent learning trajectory. Additionally, we
introduce an approach of the seed point interventions in AE allowing the
operator to influence the exploration process. Using a dataset from
Piezoresponse Force Microscopy (PFM) on PbTiO3 thin films, we illustrate the
impact of the 'seed effect' and in-loop seed interventions on the effectiveness
of DKL in predicting material properties. The study highlights the importance
of initial choices and adaptive interventions in optimizing learning rates and
enhancing the efficiency of automated material characterization. This work
offers valuable insights into designing more robust and effective AE workflows
in microscopy with potential applications across various characterization
techniques. The analysis code that supports the funding is publicly available
at https://github.com/Slautin/2024_Seed_effect_DKL_BO.",2024-01-30,cs.LG,
http://arxiv.org/abs/2401.17409v2,"Our hands serve as a fundamental means of interaction with the world around
us. Therefore, understanding hand poses and interaction context is critical for
human-computer interaction. We present EchoWrist, a low-power wristband that
continuously estimates 3D hand pose and recognizes hand-object interactions
using active acoustic sensing. EchoWrist is equipped with two speakers emitting
inaudible sound waves toward the hand. These sound waves interact with the hand
and its surroundings through reflections and diffractions, carrying rich
information about the hand's shape and the objects it interacts with. The
information captured by the two microphones goes through a deep learning
inference system that recovers hand poses and identifies various everyday hand
activities. Results from the two 12-participant user studies show that
EchoWrist is effective and efficient at tracking 3D hand poses and recognizing
hand-object interactions. Operating at 57.9mW, EchoWrist is able to
continuously reconstruct 20 3D hand joints with MJEDE of 4.81mm and recognize
12 naturalistic hand-object interactions with 97.6% accuracy.",2024-01-30,cs.HC,
http://arxiv.org/abs/2401.17408v1,"Decades of exponential scaling in high performance computing (HPC) efficiency
is coming to an end. Transistor based logic in complementary metal-oxide
semiconductor (CMOS) technology is approaching physical limits beyond which
further miniaturization will be impossible. Future HPC efficiency gains will
necessarily rely on new technologies and paradigms of compute. The Ising model
shows particular promise as a future framework for highly energy efficient
computation. Ising systems are able to operate at energies approaching
thermodynamic limits for energy consumption of computation. Ising systems can
function as both logic and memory. Thus, they have the potential to
significantly reduce energy costs inherent to CMOS computing by eliminating
costly data movement. The challenge in creating Ising-based hardware is in
optimizing useful circuits that produce correct results on fundamentally
nondeterministic hardware. The contribution of this paper is a novel machine
learning approach, a combination of deep neural networks and random forests,
for efficiently solving optimization problems that minimize sources of error in
the Ising model. In addition, we provide a process to express a Boltzmann
probability optimization problem as a supervised machine learning problem.",2024-01-30,cs.LG,
http://arxiv.org/abs/2401.17405v1,"The multi-agent reinforcement learning systems (MARL) based on the Markov
decision process (MDP) have emerged in many critical applications. To improve
the robustness/defense of MARL systems against adversarial attacks, the study
of various adversarial attacks on reinforcement learning systems is very
important. Previous works on adversarial attacks considered some possible
features to attack in MDP, such as the action poisoning attacks, the reward
poisoning attacks, and the state perception attacks. In this paper, we propose
a brand-new form of attack called the camouflage attack in the MARL systems. In
the camouflage attack, the attackers change the appearances of some objects
without changing the actual objects themselves; and the camouflaged appearances
may look the same to all the targeted recipient (victim) agents. The
camouflaged appearances can mislead the recipient agents to misguided actions.
We design algorithms that give the optimal camouflage attacks minimizing the
rewards of recipient agents. Our numerical and theoretical results show that
camouflage attacks can rival the more conventional, but likely more difficult
state perception attacks. We also investigate cost-constrained camouflage
attacks and showed numerically how cost budgets affect the attack performance.",2024-01-30,cs.MA,
http://arxiv.org/abs/2401.17404v1,"Reliable offroad autonomy requires low-latency, high-accuracy state estimates
of pose as well as velocity, which remain viable throughout environments with
sub-optimal operating conditions for the utilized perception modalities. As
state estimation remains a single point of failure system in the majority of
aspiring autonomous systems, failing to address the environmental degradation
the perception sensors could potentially experience given the operating
conditions, can be a mission-critical shortcoming. In this work, a method for
integration of radar velocity information in a LiDAR-inertial odometry solution
is proposed, enabling consistent estimation performance even with degraded
LiDAR-inertial odometry. The proposed method utilizes the direct
velocity-measuring capabilities of an Frequency Modulated Continuous Wave
(FMCW) radar sensor to enhance the LiDAR-inertial smoother solution onboard the
vehicle through integration of the forward velocity measurement into the
graph-based smoother. This leads to increased robustness in the overall
estimation solution, even in the absence of LiDAR data. This method was
validated by hardware experiments conducted onboard an all-terrain vehicle
traveling at high speed, ~12 m/s, in demanding offroad environments.",2024-01-30,cs.RO,
http://arxiv.org/abs/2401.17403v2,"Choreographic programming is a paradigm for writing distributed applications.
It allows programmers to write a single program, called a choreography, that
can be compiled to generate correct implementations of each process in the
application. Although choreographies provide good static guarantees, they can
exhibit high latency when messages or processes are delayed. This is because
processes in a choreography typically execute in a fixed, deterministic order,
and cannot adapt to the order that messages arrive at runtime. In
non-choreographic code, programmers can address this problem by allowing
processes to execute out of order -- for instance by using futures or reactive
programming. However, in choreographic code, out-of-order process execution can
lead to serious and subtle bugs, called communication integrity violations
(CIVs).
  In this paper, we develop a model of choreographic programming for
out-of-order processes that guarantees absence of CIVs and deadlocks. As an
application of our approach, we also introduce an API for safe non-blocking
communication via futures in the choreographic programming language Choral. The
API allows processes to execute out of order, participate in multiple
choreographies concurrently, and to handle unordered data messages. We provide
an illustrative evaluation of our API, showing that out-of-order execution can
reduce latency and increase throughput by overlapping communication with
computation.",2024-01-30,cs.PL,
http://arxiv.org/abs/2402.00070v1,"Extensive fine-tuning on Large Language Models does not always yield better
results. Oftentimes, models tend to get better at imitating one form of data
without gaining greater reasoning ability and may even end up losing some
intelligence. Here I introduce EvoMerge, a systematic approach to large
language model training and merging. Leveraging model merging for weight
crossover and fine-tuning for weight mutation, EvoMerge establishes an
evolutionary process aimed at pushing models beyond the limits of conventional
fine-tuning.",2024-01-30,cs.NE,
http://arxiv.org/abs/2401.17401v1,"In continual learning, a learner has to keep learning from the data over its
whole life time. A key issue is to decide what knowledge to keep and what
knowledge to let go. In a neural network, this can be implemented by using a
step-size vector to scale how much gradient samples change network weights.
Common algorithms, like RMSProp and Adam, use heuristics, specifically
normalization, to adapt this step-size vector. In this paper, we show that
those heuristics ignore the effect of their adaptation on the overall objective
function, for example by moving the step-size vector away from better step-size
vectors. On the other hand, stochastic meta-gradient descent algorithms, like
IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to
the overall objective function. On simple problems, we show that IDBD is able
to consistently improve step-size vectors, where RMSProp and Adam do not. We
explain the differences between the two approaches and their respective
limitations. We conclude by suggesting that combining both approaches could be
a promising future direction to improve the performance of neural networks in
continual learning.",2024-01-30,cs.LG,
http://arxiv.org/abs/2401.17400v1,"In this paper, we showed that the feature map of a convolution layer is
equivalent to the unnormalized log posterior of a special kind of Gaussian
mixture for image modeling. Then we expanded the model to drive diverse
features and proposed a corresponding EM algorithm to learn the model. Learning
convolution weights using this approach is efficient, guaranteed to converge,
and does not need supervised information. Code is available at:
https://github.com/LifanLiang/CALM.",2024-01-30,cs.CV,
http://arxiv.org/abs/2401.17399v1,"Point cloud prediction is an important yet challenging task in the field of
autonomous driving. The goal is to predict future point cloud sequences that
maintain object structures while accurately representing their temporal motion.
These predicted point clouds help in other subsequent tasks like object
trajectory estimation for collision avoidance or estimating locations with the
least odometry drift. In this work, we present ATPPNet, a novel architecture
that predicts future point cloud sequences given a sequence of previous time
step point clouds obtained with LiDAR sensor. ATPPNet leverages Conv-LSTM along
with channel-wise and spatial attention dually complemented by a 3D-CNN branch
for extracting an enhanced spatio-temporal context to recover high quality
fidel predictions of future point clouds. We conduct extensive experiments on
publicly available datasets and report impressive performance outperforming the
existing methods. We also conduct a thorough ablative study of the proposed
architecture and provide an application study that highlights the potential of
our model for tasks like odometry estimation.",2024-01-30,cs.RO,
http://arxiv.org/abs/2402.00069v1,"Artificial Intelligence (AI) has witnessed remarkable growth, particularly
through the proliferation of Deep Neural Networks (DNNs). These powerful models
drive technological advancements across various domains. However, to harness
their potential in real-world applications, specialized hardware accelerators
are essential. This demand has sparked a market for parameterizable AI hardware
accelerators offered by different vendors.
  Manufacturers of AI-integrated products face a critical challenge: selecting
an accelerator that aligns with their product's performance requirements. The
decision involves choosing the right hardware and configuring a suitable set of
parameters. However, comparing different accelerator design alternatives
remains a complex task. Often, engineers rely on data sheets, spreadsheet
calculations, or slow black-box simulators, which only offer a coarse
understanding of the performance characteristics.
  The Abstract Computer Architecture Description Language (ACADL) is a concise
formalization of computer architecture block diagrams, which helps to
communicate computer architecture on different abstraction levels and allows
for inferring performance characteristics. In this paper, we demonstrate how to
use the ACADL to model AI hardware accelerators, use their ACADL description to
map DNNs onto them, and explain the timing simulation semantics to gather
performance results.",2024-01-30,cs.AR,
http://arxiv.org/abs/2401.17396v1,"Deep learning-based and lately Transformer-based language models have been
dominating the studies of natural language processing in the last years. Thanks
to their accurate and fast fine-tuning characteristics, they have outperformed
traditional machine learning-based approaches and achieved state-of-the-art
results for many challenging natural language understanding (NLU) problems.
Recent studies showed that the Transformer-based models such as BERT, which is
Bidirectional Encoder Representations from Transformers, have reached
impressive achievements on many tasks. Moreover, thanks to their transfer
learning capacity, these architectures allow us to transfer pre-built models
and fine-tune them to specific NLU tasks such as question answering. In this
study, we provide a Transformer-based model and a baseline benchmark for the
Turkish Language. We successfully fine-tuned a Turkish BERT model, namely
BERTurk that is trained with base settings, to many downstream tasks and
evaluated with a the Turkish Benchmark dataset. We showed that our studies
significantly outperformed other existing baseline approaches for Named-Entity
Recognition, Sentiment Analysis, Question Answering and Text Classification in
Turkish Language. We publicly released these four fine-tuned models and
resources in reproducibility and with the view of supporting other Turkish
researchers and applications.",2024-01-30,cs.CL,
http://arxiv.org/abs/2401.17390v2,"Large language models (LLMs) are becoming increasingly important for machine
learning applications. However, it can be challenging to align LLMs with our
intent, particularly when we want to generate content that is preferable over
others or when we want the LLM to respond in a certain style or tone that is
hard to describe. To address this challenge, we propose an approach that uses
contrastive examples to better describe our intent. This involves providing
positive examples that illustrate the true intent, along with negative examples
that show what characteristics we want LLMs to avoid. The negative examples can
be retrieved from labeled data, written by a human, or generated by the LLM
itself. Before generating an answer, we ask the model to analyze the examples
to teach itself what to avoid. This reasoning step provides the model with the
appropriate articulation of the user's need and guides it towards generting a
better answer. We tested our approach on both synthesized and real-world
datasets, including StackExchange and Reddit, and found that it significantly
improves performance compared to standard few-shot prompting",2024-01-30,cs.CL,
http://arxiv.org/abs/2401.17373v1,"Speech acts are a speakers actions when performing an utterance within a
conversation, such as asking, recommending, greeting, or thanking someone,
expressing a thought, or making a suggestion. Understanding speech acts helps
interpret the intended meaning and actions behind a speakers or writers words.
This paper proposes a Twitter dialectal Arabic speech act classification
approach based on a transformer deep learning neural network. Twitter and
social media, are becoming more and more integrated into daily life. As a
result, they have evolved into a vital source of information that represents
the views and attitudes of their users. We proposed a BERT based weighted
ensemble learning approach to integrate the advantages of various BERT models
in dialectal Arabic speech acts classification. We compared the proposed model
against several variants of Arabic BERT models and sequence-based models. We
developed a dialectal Arabic tweet act dataset by annotating a subset of a
large existing Arabic sentiment analysis dataset (ASAD) based on six speech act
categories. We also evaluated the models on a previously developed Arabic Tweet
Act dataset (ArSAS). To overcome the class imbalance issue commonly observed in
speech act problems, a transformer-based data augmentation model was
implemented to generate an equal proportion of speech act categories. The
results show that the best BERT model is araBERTv2-Twitter models with a
macro-averaged F1 score and an accuracy of 0.73 and 0.84, respectively. The
performance improved using a BERT-based ensemble method with a 0.74 and 0.85
averaged F1 score and accuracy on our dataset, respectively.",2024-01-30,cs.CL,
http://arxiv.org/abs/2401.17271v1,"We construct a strong baseline method for building damage detection by
starting with the highly-engineered winning solution of the xView2 competition,
and gradually stripping away components. This way, we obtain a much simpler
method, while retaining adequate performance. We expect the simplified solution
to be more widely and easily applicable. This expectation is based on the
reduced complexity, as well as the fact that we choose hyperparameters based on
simple heuristics, that transfer to other datasets. We then re-arrange the
xView2 dataset splits such that the test locations are not seen during
training, contrary to the competition setup. In this setting, we find that both
the complex and the simplified model fail to generalize to unseen locations.
Analyzing the dataset indicates that this failure to generalize is not only a
model-based problem, but that the difficulty might also be influenced by the
unequal class distributions between events.
  Code, including the baseline model, is available under
https://github.com/PaulBorneP/Xview2_Strong_Baseline",2024-01-30,cs.CV,
http://arxiv.org/abs/2401.17270v3,"The You Only Look Once (YOLO) series of detectors have established themselves
as efficient and practical tools. However, their reliance on predefined and
trained object categories limits their applicability in open scenarios.
Addressing this limitation, we introduce YOLO-World, an innovative approach
that enhances YOLO with open-vocabulary detection capabilities through
vision-language modeling and pre-training on large-scale datasets.
Specifically, we propose a new Re-parameterizable Vision-Language Path
Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate
the interaction between visual and linguistic information. Our method excels in
detecting a wide range of objects in a zero-shot manner with high efficiency.
On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on
V100, which outperforms many state-of-the-art methods in terms of both accuracy
and speed. Furthermore, the fine-tuned YOLO-World achieves remarkable
performance on several downstream tasks, including object detection and
open-vocabulary instance segmentation.",2024-01-30,cs.CV,
http://arxiv.org/abs/2403.00196v1,"Advanced Driver Assistance Systems (ADAS) in intelligent vehicles rely on
accurate driver perception within the vehicle cabin, often leveraging a
combination of sensing modalities. However, these modalities operate at varying
rates, posing challenges for real-time, comprehensive driver state monitoring.
This paper addresses the issue of missing data due to sensor frame rate
mismatches, introducing a generative model approach to create synthetic yet
realistic thermal imagery. We propose using conditional generative adversarial
networks (cGANs), specifically comparing the pix2pix and CycleGAN
architectures. Experimental results demonstrate that pix2pix outperforms
CycleGAN, and utilizing multi-view input styles, especially stacked views,
enhances the accuracy of thermal image generation. Moreover, the study
evaluates the model's generalizability across different subjects, revealing the
importance of individualized training for optimal performance. The findings
suggest the potential of generative models in addressing missing frames,
advancing driver state monitoring for intelligent vehicles, and underscoring
the need for continued research in model generalization and customization.",2024-02-29,cs.CV,
http://arxiv.org/abs/2403.00194v1,"Pre-training is a widely used approach to develop models that are robust to
distribution shifts. However, in practice, its effectiveness varies:
fine-tuning a pre-trained model improves robustness significantly in some cases
but not at all in others (compared to training from scratch). In this work, we
seek to characterize the failure modes that pre-training can and cannot
address. In particular, we focus on two possible failure modes of models under
distribution shift: poor extrapolation (e.g., they cannot generalize to a
different domain) and biases in the training data (e.g., they rely on spurious
features). Our study suggests that, as a rule of thumb, pre-training can help
mitigate poor extrapolation but not dataset biases. After providing theoretical
motivation and empirical evidence for this finding, we explore two of its
implications for developing robust models: (1) pre-training and interventions
designed to prevent exploiting biases have complementary robustness benefits,
and (2) fine-tuning on a (very) small, non-diverse but de-biased dataset can
result in significantly more robust models than fine-tuning on a large and
diverse but biased dataset. Code is available at
https://github.com/MadryLab/pretraining-distribution-shift-robustness.",2024-02-29,cs.LG,
http://arxiv.org/abs/2403.00192v1,"Quantum key distribution (QKD) is a popular protocol that provides
information theoretically secure keys to multiple parties. Two important
post-processing steps of QKD are 1) the information reconciliation (IR) step,
where parties reconcile mismatches in generated keys through classical
communication, and 2) the privacy amplification (PA) step, where parties
distill their common key into a new secure key that the adversary has little to
no information about. In general, these two steps have been abstracted as two
distinct problems. In this work, we consider a new technique of performing the
IR and PA steps jointly through sampling that relaxes the requirement on the IR
step, allowing for more success in key creation. We provide a novel LDPC code
construction known as Block-MDS QC-LDPC codes that can utilize the relaxed
requirement by creating LDPC codes with pre-defined sub-matrices of full-rank.
We demonstrate through simulations that our technique of sampling can provide
notable gains in successfully creating secret keys.",2024-02-29,cs.IT,
http://arxiv.org/abs/2403.00193v1,"The study utilizes a comprehensive dataset informed by IPv6 routing
information to provide statistics, degree distribution, joint degree
distribution, and clustering analysis of the IPv6 Internet's structure and
resilience.The dataset includes 17,232 unique ASes and 10,000 unique IPv6
prefixes. Analysis reveals an interconnected network with an average path
length of approximately 3 hops, suggesting a robust and efficient network with
potential redundancy and resilience, despite some isolated components. The
paper outlines the degree distribution, indicating many peripheral nodes in a
sparse network, and a clustering analysis showing a tendency for ASes to form
clusters, which is indicative of redundancy and robustness against failures.
The connectivity analysis, including path redundancy and reachability, supports
the network's resilience.The findings are crucial for network design and
strategic planning, particularly as IPv6 adoption increases. The paper
emphasizes the importance of continuous monitoring and improvement of network
connectivity in the evolving Internet landscape, highlighting the IPv6
Internet's resilience and structured connectivity.",2024-02-29,cs.NI,
http://arxiv.org/abs/2403.00190v1,"This study presents an integrated approach for identifying key nodes in
information propagation networks using advanced artificial intelligence
methods. We introduce a novel technique that combines the Decision-making Trial
and Evaluation Laboratory (DEMATEL) method with the Global Structure Model
(GSM), creating a synergistic model that effectively captures both local and
global influences within a network. This method is applied across various
complex networks, such as social, transportation, and communication systems,
utilizing the Global Network Influence Dataset (GNID). Our analysis highlights
the structural dynamics and resilience of these networks, revealing insights
into node connectivity and community formation. The findings demonstrate the
effectiveness of our AI-based approach in offering a comprehensive
understanding of network behavior, contributing significantly to strategic
network analysis and optimization.",2024-02-29,cs.SI,
http://arxiv.org/abs/2403.00189v1,"The evolution of wireless communications has been significantly influenced by
remarkable advancements in multiple access (MA) technologies over the past five
decades, shaping the landscape of modern connectivity. Within this context, a
comprehensive tutorial review is presented, focusing on representative MA
techniques developed over the past 50 years. The following areas are explored:
i) The foundational principles and information-theoretic capacity limits of
power-domain non-orthogonal multiple access (NOMA) are characterized, along
with its extension to multiple-input multiple-output (MIMO)-NOMA. ii) Several
MA transmission schemes exploiting the spatial domain are investigated,
encompassing both conventional space-division multiple access (SDMA)/MIMO-NOMA
systems and near-field MA systems utilizing spherical-wave propagation models.
iii) The application of NOMA to integrated sensing and communications (ISAC)
systems is studied. This includes an introduction to typical NOMA-based
downlink/uplink ISAC frameworks, followed by an evaluation of their performance
limits using a mutual information (MI)-based analytical framework. iv) Major
issues and research opportunities associated with the integration of MA with
other emerging technologies are identified to facilitate MA in next-generation
networks, i.e., next-generation multiple access (NGMA). Throughout the paper,
promising directions are highlighted to inspire future research endeavors in
the realm of MA and NGMA.",2024-02-29,cs.IT,
http://arxiv.org/abs/2403.00187v1,"Legged robots have the potential to traverse complex terrain and access
confined spaces beyond the reach of traditional platforms thanks to their
ability to carefully select footholds and flexibly adapt their body posture
while walking. However, robust deployment in real-world applications is still
an open challenge. In this paper, we present a method for legged locomotion
control using reinforcement learning and 3D volumetric representations to
enable robust and versatile locomotion in confined and unstructured
environments. By employing a two-layer hierarchical policy structure, we
exploit the capabilities of a highly robust low-level policy to follow 6D
commands and a high-level policy to enable three-dimensional spatial awareness
for navigating under overhanging obstacles. Our study includes the development
of a procedural terrain generator to create diverse training environments. We
present a series of experimental evaluations in both simulation and real-world
settings, demonstrating the effectiveness of our approach in controlling a
quadruped robot in confined, rough terrain. By achieving this, our work extends
the applicability of legged robots to a broader range of scenarios.",2024-02-29,cs.RO,
http://arxiv.org/abs/2403.02347v1,"Data similarity assumptions have traditionally been relied upon to understand
the convergence behaviors of federated learning methods. Unfortunately, this
approach often demands fine-tuning step sizes based on the level of data
similarity. When data similarity is low, these small step sizes result in an
unacceptably slow convergence speed for federated methods. In this paper, we
present a novel and unified framework for analyzing the convergence of
federated learning algorithms without the need for data similarity conditions.
Our analysis centers on an inequality that captures the influence of step sizes
on algorithmic convergence performance. By applying our theorems to well-known
federated algorithms, we derive precise expressions for three widely used step
size schedules: fixed, diminishing, and step-decay step sizes, which are
independent of data similarity conditions. Finally, we conduct comprehensive
evaluations of the performance of these federated learning algorithms,
employing the proposed step size strategies to train deep neural network models
on benchmark datasets under varying data similarity conditions. Our findings
demonstrate significant improvements in convergence speed and overall
performance, marking a substantial advancement in federated learning research.",2024-02-29,cs.LG,
http://arxiv.org/abs/2403.00180v1,"Model editing has emerged as a cost-effective strategy to update knowledge
stored in language models. However, model editing can have unintended
consequences after edits are applied: information unrelated to the edits can
also be changed, and other general behaviors of the model can be wrongly
altered. In this work, we investigate how model editing methods unexpectedly
amplify model biases post-edit. We introduce a novel benchmark dataset,
Seesaw-CF, for measuring bias-related harms of model editing and conduct the
first in-depth investigation of how different weight-editing methods impact
model bias. Specifically, we focus on biases with respect to demographic
attributes such as race, geographic origin, and gender, as well as qualitative
flaws in long-form texts generated by edited language models. We find that
edited models exhibit, to various degrees, more biased behavior as they become
less confident in attributes for Asian, African, and South American subjects.
Furthermore, edited models amplify sexism and xenophobia in text generations
while remaining seemingly coherent and logical. Finally, editing facts about
place of birth, country of citizenship, or gender have particularly negative
effects on the model's knowledge about unrelated features like field of work.",2024-02-29,cs.CL,
http://arxiv.org/abs/2403.00179v1,"Counterspeech, i.e., direct responses against hate speech, has become an
important tool to address the increasing amount of hate online while avoiding
censorship. Although AI has been proposed to help scale up counterspeech
efforts, this raises questions of how exactly AI could assist in this process,
since counterspeech is a deeply empathetic and agentic process for those
involved. In this work, we aim to answer this question, by conducting in-depth
interviews with 10 extensively experienced counterspeakers and a large scale
public survey with 342 everyday social media users. In participant responses,
we identified four main types of barriers and AI needs related to resources,
training, impact, and personal harms. However, our results also revealed
overarching concerns of authenticity, agency, and functionality in using AI
tools for counterspeech. To conclude, we discuss considerations for designing
AI assistants that lower counterspeaking barriers without jeopardizing its
meaning and purpose.",2024-02-29,cs.HC,
http://arxiv.org/abs/2403.00178v1,"Real-world multi-agent systems are often dynamic and continuous, where the
agents co-evolve and undergo changes in their trajectories and interactions
over time. For example, the COVID-19 transmission in the U.S. can be viewed as
a multi-agent system, where states act as agents and daily population movements
between them are interactions. Estimating the counterfactual outcomes in such
systems enables accurate future predictions and effective decision-making, such
as formulating COVID-19 policies. However, existing methods fail to model the
continuous dynamic effects of treatments on the outcome, especially when
multiple treatments (e.g., ""stay-at-home"" and ""get-vaccine"" policies) are
applied simultaneously. To tackle this challenge, we propose Causal Graph
Ordinary Differential Equations (CAG-ODE), a novel model that captures the
continuous interaction among agents using a Graph Neural Network (GNN) as the
ODE function. The key innovation of our model is to learn time-dependent
representations of treatments and incorporate them into the ODE function,
enabling precise predictions of potential outcomes. To mitigate confounding
bias, we further propose two domain adversarial learning-based objectives,
which enable our model to learn balanced continuous representations that are
not affected by treatments or interference. Experiments on two datasets (i.e.,
COVID-19 and tumor growth) demonstrate the superior performance of our proposed
model.",2024-02-29,cs.LG,
http://arxiv.org/abs/2403.00177v2,"A digital twin is a virtual replica of a real-world physical phenomena that
uses mathematical modeling to characterize and simulate its defining features.
By constructing digital twins for disease processes, we can perform in-silico
simulations that mimic patients' health conditions and counterfactual outcomes
under hypothetical interventions in a virtual setting. This eliminates the need
for invasive procedures or uncertain treatment decisions. In this paper, we
propose a method to identify digital twin model parameters using only
noninvasive patient health data. We approach the digital twin modeling as a
composite inverse problem, and observe that its structure resembles pretraining
and finetuning in self-supervised learning (SSL). Leveraging this, we introduce
a physics-informed SSL algorithm that initially pretrains a neural network on
the pretext task of learning a differentiable simulator of a physiological
process. Subsequently, the model is trained to reconstruct physiological
measurements from noninvasive modalities while being constrained by the
physical equations learned in pretraining. We apply our method to identify
digital twins of cardiac hemodynamics using noninvasive echocardiogram videos,
and demonstrate its utility in unsupervised disease detection and in-silico
clinical trials.",2024-02-29,cs.LG,
http://arxiv.org/abs/2403.00863v1,"Product attribute value extraction is a pivotal component in Natural Language
Processing (NLP) and the contemporary e-commerce industry. The provision of
precise product attribute values is fundamental in ensuring high-quality
recommendations and enhancing customer satisfaction. The recently emerging
Large Language Models (LLMs) have demonstrated state-of-the-art performance in
numerous attribute extraction tasks, without the need for domain-specific
training data. Nevertheless, varying strengths and weaknesses are exhibited by
different LLMs due to the diversity in data, architectures, and
hyperparameters. This variation makes them complementary to each other, with no
single LLM dominating all others. Considering the diverse strengths and
weaknesses of LLMs, it becomes necessary to develop an ensemble method that
leverages their complementary potentials. In this paper, we propose a novel
algorithm called LLM-ensemble to ensemble different LLMs' outputs for attribute
value extraction. We iteratively learn the weights for different LLMs to
aggregate the labels with weights to predict the final attribute value. Not
only can our proposed method be proven theoretically optimal, but it also
ensures efficient computation, fast convergence, and safe deployment. We have
also conducted extensive experiments with various state-of-the-art LLMs,
including Llama2-13B, Llama2-70B, PaLM-2, GPT-3.5, and GPT-4, on Walmart's
internal data. Our offline metrics demonstrate that the LLM-ensemble method
outperforms all the state-of-the-art single LLMs on Walmart's internal dataset.
This method has been launched in several production models, leading to improved
Gross Merchandise Volume (GMV), Click-Through Rate (CTR), Conversion Rate
(CVR), and Add-to-Cart Rate (ATC).",2024-02-29,cs.IR,
http://arxiv.org/abs/2403.00175v2,"In the realm of computer vision, the integration of advanced techniques into
the processing of RGB-D camera inputs poses a significant challenge, given the
inherent complexities arising from diverse environmental conditions and varying
object appearances. Therefore, this paper introduces FusionVision, an
exhaustive pipeline adapted for the robust 3D segmentation of objects in RGB-D
imagery. Traditional computer vision systems face limitations in simultaneously
capturing precise object boundaries and achieving high-precision object
detection on depth map as they are mainly proposed for RGB cameras. To address
this challenge, FusionVision adopts an integrated approach by merging
state-of-the-art object detection techniques, with advanced instance
segmentation methods. The integration of these components enables a holistic
(unified analysis of information obtained from both color \textit{RGB} and
depth \textit{D} channels) interpretation of RGB-D data, facilitating the
extraction of comprehensive and accurate object information. The proposed
FusionVision pipeline employs YOLO for identifying objects within the RGB image
domain. Subsequently, FastSAM, an innovative semantic segmentation model, is
applied to delineate object boundaries, yielding refined segmentation masks.
The synergy between these components and their integration into 3D scene
understanding ensures a cohesive fusion of object detection and segmentation,
enhancing overall precision in 3D object segmentation. The code and pre-trained
models are publicly available at https://github.com/safouaneelg/FusionVision/.",2024-02-29,cs.CV,
http://arxiv.org/abs/2403.00174v3,"Street View Imagery (SVI) is a valuable data source for studies (e.g.,
environmental assessments, green space identification or land cover
classification). While commercial SVI is available, such providers commonly
restrict copying or reuse in ways necessary for research. Open SVI datasets are
readily available from less restrictive sources, such as Mapillary, but due to
the heterogeneity of the images, these require substantial preprocessing,
filtering, and careful quality checks. We present an efficient method for
automated downloading, processing, cropping, and filtering open SVI, to be used
in a survey of human perceptions of the streets portrayed in these images. We
demonstrate our open-source reusable SVI preparation and smartphone-friendly
perception-survey software with Amsterdam (Netherlands) as the case study.
Using a citizen science approach, we collected from 331 people 22,637 ratings
about their perceptions for various criteria. We have published our software in
a public repository for future re-use and reproducibility.",2024-02-29,cs.CV,
http://arxiv.org/abs/2403.00170v4,"Writing declarative models has numerous benefits, ranging from automated
reasoning and correction of design-level properties before systems are built to
automated testing and debugging of their implementations after they are built.
Unfortunately, the model itself needs to be correct to gain these benefits.
Alloy is a commonly used modeling language that has several existing efforts to
repair faulty models automatically. Currently, these efforts are search-based
methods that use an Abstract Syntax Tree (AST) representation of the model and
do not scale. One issue is that ASTs themselves suffer from exponential growth
in their data size due to the limitation that ASTs will often have identical
nodes separately listed in the tree. To address this issue, we introduce a
novel code representation schema, Complex Structurally Balanced Abstract
Semantic Graph (CSBASG), which represents code as a complex-weighted directed
graph that lists a semantic element as a node in the graph and ensures its
structural balance for almost finitely enumerable code segments. We evaluate
the efficiency of our CSBASG representation for Alloy models in terms of it's
compactness compared to ASTs, and we explore if a CSBASG can ease the process
of comparing two Alloy predicates. Moreover, with this representation in place,
we identify several future applications of CSBASG, including Alloy code
generation and automated repair.",2024-02-29,cs.SE,
http://arxiv.org/abs/2403.00169v1,"Probabilistic model checking is a widely used formal verification technique
to automatically verify qualitative and quantitative properties for
probabilistic models. However, capturing such systems, writing corresponding
properties, and verifying them require domain knowledge. This makes it not
accessible for researchers and engineers who may not have the required
knowledge. Previous studies have extended UML activity diagrams (ADs),
developed transformations, and implemented accompanying tools for automation.
The research, however, is incomprehensive and not fully open, which makes it
hard to be evaluated, extended, adapted, and accessed. In this paper, we
propose a comprehensive verification framework for ADs, including a new profile
for probability, time, and quality annotations, a semantics interpretation of
ADs in three Markov models, and a set of transformation rules from activity
diagrams to the PRISM language, supported by PRISM and Storm. Most importantly,
we developed algorithms for transformation and implemented them in a tool,
called QASCAD, using model-based techniques, for fully automated verification.
We evaluated one case study where multiple robots are used for delivery in a
hospital and further evaluated six other examples from the literature. With all
these together, this work makes noteworthy contributions to the verification of
ADs by improving evaluation, extensibility, adaptability, and accessibility.",2024-02-29,cs.LO,
http://arxiv.org/abs/2403.00165v1,"Hierarchical text classification aims to categorize each document into a set
of classes in a label taxonomy. Most earlier works focus on fully or
semi-supervised methods that require a large amount of human annotated data
which is costly and time-consuming to acquire. To alleviate human efforts, in
this paper, we work on hierarchical text classification with the minimal amount
of supervision: using the sole class name of each node as the only supervision.
Recently, large language models (LLM) show competitive performance on various
tasks through zero-shot prompting, but this method performs poorly in the
hierarchical setting, because it is ineffective to include the large and
structured label space in a prompt. On the other hand, previous
weakly-supervised hierarchical text classification methods only utilize the raw
taxonomy skeleton and ignore the rich information hidden in the text corpus
that can serve as additional class-indicative features. To tackle the above
challenges, we propose TELEClass, Taxonomy Enrichment and LLM-Enhanced
weakly-supervised hierarchical text classification, which (1) automatically
enriches the label taxonomy with class-indicative topical terms mined from the
corpus to facilitate classifier training and (2) utilizes LLMs for both data
annotation and creation tailored for the hierarchical label space. Experiments
show that TELEClass can outperform previous weakly-supervised hierarchical text
classification methods and LLM-based zero-shot prompting methods on two public
datasets.",2024-02-29,cs.CL,
http://arxiv.org/abs/2403.00157v1,"Distributed optimization and learning has recently garnered great attention
due to its wide applications in sensor networks, smart grids, machine learning,
and so forth. Despite rapid development, existing distributed optimization and
learning algorithms require each agent to exchange messages with its neighbors,
which may expose sensitive information and raise significant privacy concerns.
In this survey paper, we overview privacy-preserving distributed optimization
and learning methods. We first discuss cryptography, differential privacy, and
other techniques that can be used for privacy preservation and indicate their
pros and cons for privacy protection in distributed optimization and learning.
We believe that among these approaches, differential privacy is most promising
due to its low computational and communication complexities, which are
extremely appealing for modern learning based applications with high dimensions
of optimization variables. We then introduce several differential-privacy
algorithms that can simultaneously ensure privacy and optimization accuracy.
Moreover, we provide example applications in several machine learning problems
to confirm the real-world effectiveness of these algorithms. Finally, we
highlight some challenges in this research domain and discuss future
directions.",2024-02-29,cs.LG,
http://arxiv.org/abs/2403.00155v1,"Despite the impressive performance of deep neural networks (DNNs), their
computational complexity and storage space consumption have led to the concept
of network compression. While DNN compression techniques such as pruning and
low-rank decomposition have been extensively studied, there has been
insufficient attention paid to their theoretical explanation. In this paper, we
propose a novel theoretical framework that leverages a probabilistic latent
space of DNN weights and explains the optimal network sparsity by using the
information-theoretic divergence measures. We introduce new analogous projected
patterns (AP2) and analogous-in-probability projected patterns (AP3) notions
for DNNs and prove that there exists a relationship between AP3/AP2 property of
layers in the network and its performance. Further, we provide a theoretical
analysis that explains the training process of the compressed network. The
theoretical results are empirically validated through experiments conducted on
standard pre-trained benchmarks, including AlexNet, ResNet50, and VGG16, using
CIFAR10 and CIFAR100 datasets. Through our experiments, we highlight the
relationship of AP3 and AP2 properties with fine-tuning pruned DNNs and
sparsity levels.",2024-02-29,cs.LG,
http://arxiv.org/abs/2403.00154v2,"Interest is increasing among political scientists in leveraging the extensive
information available in images. However, the challenge of interpreting these
images lies in the need for specialized knowledge in computer vision and access
to specialized hardware. As a result, image analysis has been limited to a
relatively small group within the political science community. This landscape
could potentially change thanks to the rise of large language models (LLMs).
This paper aims to raise awareness of the feasibility of using Gemini for image
content analysis. A retrospective analysis was conducted on a corpus of 688
images. Content reports were elicited from Gemini for each image and then
manually evaluated by the authors. We find that Gemini is highly accurate in
performing object detection, which is arguably the most common and fundamental
task in image analysis for political scientists. Equally important, we show
that it is easy to implement as the entire command consists of a single prompt
in natural language; it is fast to run and should meet the time budget of most
researchers; and it is free to use and does not require any specialized
hardware. In addition, we illustrate how political scientists can leverage
Gemini for other image understanding tasks, including face identification,
sentiment analysis, and caption generation. Our findings suggest that Gemini
and other similar LLMs have the potential to drastically stimulate and
accelerate image research in political science and social sciences more
broadly.",2024-02-29,cs.CV,
http://arxiv.org/abs/2403.00153v1,"A long-standing vision in computer science has been to evolve computing
devices into proactive assistants that enhance our productivity, health and
wellness, and many other facets of our lives. User digitization is crucial in
achieving this vision as it allows computers to intimately understand their
users, capturing activity, pose, routine, and behavior. Today's consumer
devices - like smartphones and smartwatches provide a glimpse of this
potential, offering coarse digital representations of users with metrics such
as step count, heart rate, and a handful of human activities like running and
biking. Even these very low-dimensional representations are already bringing
value to millions of people's lives, but there is significant potential for
improvement. On the other end, professional, high-fidelity comprehensive user
digitization systems exist. For example, motion capture suits and multi-camera
rigs that digitize our full body and appearance, and scanning machines such as
MRI capture our detailed anatomy. However, these carry significant user
practicality burdens, such as financial, privacy, ergonomic, aesthetic, and
instrumentation considerations, that preclude consumer use. In general, the
higher the fidelity of capture, the lower the user's practicality. Most
conventional approaches strike a balance between user practicality and
digitization fidelity.
  My research aims to break this trend, developing sensing systems that
increase user digitization fidelity to create new and powerful computing
experiences while retaining or even improving user practicality and
accessibility, allowing such technologies to have a societal impact. Armed with
such knowledge, our future devices could offer longitudinal health tracking,
more productive work environments, full body avatars in extended reality, and
embodied telepresence experiences, to name just a few domains.",2024-02-29,cs.HC,
http://arxiv.org/abs/2403.00148v1,"With the upcoming AI regulations (e.g., EU AI Act) and rapid advancements in
generative AI, new challenges emerge in the area of Human-Centered Responsible
Artificial Intelligence (HCR-AI). As AI becomes more ubiquitous, questions
around decision-making authority, human oversight, accountability,
sustainability, and the ethical and legal responsibilities of AI and their
creators become paramount. Addressing these questions requires a collaborative
approach. By involving stakeholders from various disciplines in the
2\textsuperscript{nd} edition of the HCR-AI Special Interest Group (SIG) at CHI
2024, we aim to discuss the implications of regulations in HCI research,
develop new theories, evaluation frameworks, and methods to navigate the
complex nature of AI ethics, steering AI development in a direction that is
beneficial and sustainable for all of humanity.",2024-02-29,cs.HC,
http://arxiv.org/abs/2403.00145v1,"Value Sensitive Design (VSD) is a framework for integrating human values
throughout the technology design process. In parallel, Responsible AI (RAI)
advocates for the development of systems aligning with ethical values, such as
fairness and transparency. In this study, we posit that a VSD approach is not
only compatible, but also advantageous to the development of RAI toolkits. To
empirically assess this hypothesis, we conducted four workshops involving 17
early-career AI researchers. Our aim was to establish links between VSD and RAI
values while examining how existing toolkits incorporate VSD principles in
their design. Our findings show that collaborative and educational design
features within these toolkits, including illustrative examples and open-ended
cues, facilitate an understanding of human and ethical values, and empower
researchers to incorporate values into AI systems. Drawing on these insights,
we formulated six design guidelines for integrating VSD values into the
development of RAI toolkits.",2024-02-29,cs.HC,
http://arxiv.org/abs/2403.00143v1,"We address unsupervised discontinuous constituency parsing, where we observe
a high variance in the performance of the only previous model. We propose to
build an ensemble of different runs of the existing discontinuous parser by
averaging the predicted trees, to stabilize and boost performance. To begin
with, we provide comprehensive computational complexity analysis (in terms of P
and NP-complete) for tree averaging under different setups of binarity and
continuity. We then develop an efficient exact algorithm to tackle the task,
which runs in a reasonable time for all samples in our experiments. Results on
three datasets show our method outperforms all baselines in all metrics; we
also provide in-depth analyses of our approach.",2024-02-29,cs.CL,
http://arxiv.org/abs/2403.00144v1,"The ability of zero-shot translation emerges when we train a multilingual
model with certain translation directions; the model can then directly
translate in unseen directions. Alternatively, zero-shot translation can be
accomplished by pivoting through a third language (e.g., English). In our work,
we observe that both direct and pivot translations are noisy and achieve less
satisfactory performance. We propose EBBS, an ensemble method with a novel
bi-level beam search algorithm, where each ensemble component explores its own
prediction step by step at the lower level but they are synchronized by a ""soft
voting"" mechanism at the upper level. Results on two popular multilingual
translation datasets show that EBBS consistently outperforms direct and pivot
translations as well as existing ensemble techniques. Further, we can distill
the ensemble's knowledge back to the multilingual model to improve inference
efficiency; profoundly, our EBBS-based distillation does not sacrifice, or even
improves, the translation quality.",2024-02-29,cs.CL,
http://arxiv.org/abs/2403.00141v1,"Privacy policy documents have a crucial role in educating individuals about
the collection, usage, and protection of users' personal data by organizations.
However, they are notorious for their lengthy, complex, and convoluted language
especially involving privacy-related entities. Hence, they pose a significant
challenge to users who attempt to comprehend organization's data usage policy.
In this paper, we propose to enhance the interpretability and readability of
policy documents by using controlled abstractive summarization -- we enforce
the generated summaries to include critical privacy-related entities (e.g.,
data and medium) and organization's rationale (e.g.,target and reason) in
collecting those entities. To achieve this, we develop PD-Sum, a
policy-document summarization dataset with marked privacy-related entity
labels. Our proposed model, EROS, identifies critical entities through a
span-based entity extraction model and employs them to control the information
content of the summaries using proximal policy optimization (PPO). Comparison
shows encouraging improvement over various baselines. Furthermore, we furnish
qualitative and human evaluations to establish the efficacy of EROS.",2024-02-29,cs.CL,
http://arxiv.org/abs/2403.00137v1,"As Artificial Intelligence (AI) becomes ubiquitous, the need for Explainable
AI (XAI) has become critical for transparency and trust among users. A
significant challenge in XAI is catering to diverse users, such as data
scientists, domain experts, and end-users. Recent research has started to
investigate how users' characteristics impact interactions with and user
experience of explanations, with a view to personalizing XAI. However, are we
heading down a rabbit hole by focusing on unimportant details? Our research
aimed to investigate how user characteristics are related to using,
understanding, and trusting an AI system that provides explanations. Our
empirical study with 149 participants who interacted with an XAI system that
flagged inappropriate comments showed that very few user characteristics
mattered; only age and the personality trait openness influenced actual
understanding. Our work provides evidence to reorient user-focused XAI research
and question the pursuit of personalized XAI based on fine-grained user
characteristics.",2024-02-29,cs.HC,
http://arxiv.org/abs/2403.00136v1,"As highly automated vehicles reach higher deployment rates, they find
themselves in increasingly dangerous situations. Knowing that the consequence
of a crash is significant for the health of occupants, bystanders, and
properties, as well as to the viability of autonomy and adjacent businesses, we
must search for more efficacious ways to comprehensively and reliably train
autonomous vehicles to better navigate the complex scenarios with which they
struggle. We therefore introduce a taxonomy of potentially adversarial elements
that may contribute to poor performance or system failures as a means of
identifying and elucidating lesser-seen risks. This taxonomy may be used to
characterize failures of automation, as well as to support simulation and
real-world training efforts by providing a more comprehensive classification
system for events resulting in disengagement, collision, or other negative
consequences. This taxonomy is created from and tested against real collision
events to ensure comprehensive coverage with minimal class overlap and few
omissions. It is intended to be used both for the identification of
harm-contributing adversarial events and in the generation thereof (to create
extreme edge- and corner-case scenarios) in training procedures.",2024-02-29,cs.RO,
http://arxiv.org/abs/2403.00133v1,"Making ideal decisions as a product leader in a web-facing company is
extremely difficult. In addition to navigating the ambiguity of customer
satisfaction and achieving business goals, one must also pave a path forward
for ones' products and services to remain relevant, desirable, and profitable.
Data and experimentation to test product hypotheses are key to informing
product decisions. Online controlled experiments by A/B testing may provide the
best data to support such decisions with high confidence, but can be
time-consuming and expensive, especially when one wants to understand impact to
key business metrics such as retention or long-term value. Offline
experimentation allows one to rapidly iterate and test, but often cannot
provide the same level of confidence, and cannot easily shine a light on impact
on business metrics. We introduce a novel, lightweight, and flexible approach
to investigating hypotheses, called scenario analysis, that aims to support
product leaders' decisions using data about users and estimates of business
metrics. Its strengths are that it can provide guidance on trade-offs that are
incurred by growing or shifting consumption, estimate trends in long-term
outcomes like retention and other important business metrics, and can generate
hypotheses about relationships between metrics at scale.",2024-02-29,cs.CE,
http://arxiv.org/abs/2403.00131v2,"Advances in time series models are driving a shift from conventional deep
learning methods to pre-trained foundational models. While pre-trained
transformers and reprogrammed text-based LLMs report state-of-the-art results,
the best-performing architectures vary significantly across tasks, and models
often have limited scope, such as focusing only on time series forecasting.
Models that unify predictive and generative time series tasks under a single
framework remain challenging to achieve. We introduce UniTS, a multi-task time
series model that uses task tokenization to express predictive and generative
tasks within a single model. UniTS leverages a modified transformer block
designed to obtain universal time series representations. This design induces
transferability from a heterogeneous, multi-domain pre-training dataset-often
with diverse dynamic patterns, sampling rates, and temporal scales-to many
downstream datasets, which can also be diverse in task specifications and data
domains. Across 38 datasets spanning human activity sensors, healthcare,
engineering, and finance domains, UniTS model performs favorably against 12
forecasting models, 20 classification models, 18 anomaly detection models, and
16 imputation models, including repurposed text-based LLMs. UniTS demonstrates
effective few-shot and prompt learning capabilities when evaluated on new data
domains and tasks. In the conventional single-task setting, UniTS outperforms
strong task-specialized time series models. The source code and datasets are
available at https://github.com/mims-harvard/UniTS.",2024-02-29,cs.LG,
http://arxiv.org/abs/2403.00128v1,"Inverted landing is a routine behavior among a number of animal fliers.
However, mastering this feat poses a considerable challenge for robotic fliers,
especially to perform dynamic perching with rapid body rotations (or flips) and
landing against gravity. Inverted landing in flies have suggested that optical
flow senses are closely linked to the precise triggering and control of body
flips that lead to a variety of successful landing behaviors. Building upon
this knowledge, we aimed to replicate the flies' landing behaviors in small
quadcopters by developing a control policy general to arbitrary
ceiling-approach conditions. First, we employed reinforcement learning in
simulation to optimize discrete sensory-motor pairs across a broad spectrum of
ceiling-approach velocities and directions. Next, we converted the
sensory-motor pairs to a two-stage control policy in a continuous
augmented-optical flow space. The control policy consists of a first-stage
Flip-Trigger Policy, which employs a one-class support vector machine, and a
second-stage Flip-Action Policy, implemented as a feed-forward neural network.
To transfer the inverted-landing policy to physical systems, we utilized domain
randomization and system identification techniques for a zero-shot sim-to-real
transfer. As a result, we successfully achieved a range of robust
inverted-landing behaviors in small quadcopters, emulating those observed in
flies.",2024-02-29,cs.RO,
http://arxiv.org/abs/2403.00127v2,"Prompt engineering has shown potential for improving translation quality in
LLMs. However, the possibility of using translation concepts in prompt design
remains largely underexplored. Against this backdrop, the current paper
discusses the effectiveness of incorporating the conceptual tool of translation
brief and the personas of translator and author into prompt design for
translation tasks in ChatGPT. Findings suggest that, although certain elements
are constructive in facilitating human-to-human communication for translation
tasks, their effectiveness is limited for improving translation quality in
ChatGPT. This accentuates the need for explorative research on how translation
theorists and practitioners can develop the current set of conceptual tools
rooted in the human-to-human communication paradigm for translation purposes in
this emerging workflow involving human-machine interaction, and how translation
concepts developed in translation studies can inform the training of GPT models
for translation tasks.",2024-02-29,cs.CL,
http://arxiv.org/abs/2403.00862v4,"We present NewsBench, a novel evaluation framework to systematically assess
the capabilities of Large Language Models (LLMs) for editorial capabilities in
Chinese journalism. Our constructed benchmark dataset is focused on four facets
of writing proficiency and six facets of safety adherence, and it comprises
manually and carefully designed 1,267 test samples in the types of multiple
choice questions and short answer questions for five editorial tasks in 24 news
domains. To measure performances, we propose different GPT-4 based automatic
evaluation protocols to assess LLM generations for short answer questions in
terms of writing proficiency and safety adherence, and both are validated by
the high correlations with human evaluations. Based on the systematic
evaluation framework, we conduct a comprehensive analysis of ten popular LLMs
which can handle Chinese. The experimental results highlight GPT-4 and ERNIE
Bot as top performers, yet reveal a relative deficiency in journalistic safety
adherence in creative writing tasks. Our findings also underscore the need for
enhanced ethical guidance in machine-generated journalistic content, marking a
step forward in aligning LLMs with journalistic standards and safety
considerations.",2024-02-29,cs.CL,
http://arxiv.org/abs/2403.00861v1,"Generative AI applications, such as ChatGPT or DALL-E, have shown the world
their impressive capabilities in generating human-like text or image. Diving
deeper, the science stakeholder for those AI applications are Deep Generative
Models, a.k.a DGMs, which are designed to learn the underlying distribution of
the data and generate new data points that are statistically similar to the
original dataset. One critical question is raised: how can we leverage DGMs
into morden retail supply chain realm? To address this question, this paper
expects to provide a comprehensive review of DGMs and discuss their existing
and potential usecases in retail supply chain, by (1) providing a taxonomy and
overview of state-of-the-art DGMs and their variants, (2) reviewing existing
DGM applications in retail supply chain from a end-to-end view of point, and
(3) discussing insights and potential directions on how DGMs can be further
utilized on solving retail supply chain problems.",2024-02-29,cs.AI,
http://arxiv.org/abs/2403.00860v1,"A feedforward neural network using rectified linear units constructs a
mapping from inputs to outputs by partitioning its input space into a set of
convex regions where points within a region share a single affine
transformation. In order to understand how neural networks work, when and why
they fail, and how they compare to biological intelligence, we need to
understand the organization and formation of these regions. Step one is to
design and implement algorithms for exact region enumeration in networks beyond
toy examples.
  In this work, we present parallel algorithms for exact enumeration in deep
(and shallow) neural networks. Our work has three main contributions: (1) we
present a novel algorithm framework and parallel algorithms for region
enumeration; (2) we implement one of our algorithms on a variety of network
architectures and experimentally show how the number of regions dictates
runtime; and (3) we show, using our algorithm's output, how the dimension of a
region's affine transformation impacts further partitioning of the region by
deeper layers.
  To our knowledge, we run our implemented algorithm on networks larger than
all of the networks used in the existing region enumeration literature.
Further, we experimentally demonstrate the importance of parallelism for region
enumeration of any reasonably sized network.",2024-02-29,cs.LG,
http://arxiv.org/abs/2403.00116v1,"The demand for collaborative and private bandit learning across multiple
agents is surging due to the growing quantity of data generated from
distributed systems. Federated bandit learning has emerged as a promising
framework for private, efficient, and decentralized online learning. However,
almost all previous works rely on strong assumptions of client homogeneity,
i.e., all participating clients shall share the same bandit model; otherwise,
they all would suffer linear regret. This greatly restricts the application of
federated bandit learning in practice. In this work, we introduce a new
approach for federated bandits for heterogeneous clients, which clusters
clients for collaborative bandit learning under the federated learning setting.
Our proposed algorithm achieves non-trivial sub-linear regret and communication
cost for all clients, subject to the communication protocol under federated
learning that at anytime only one model can be shared by the server.",2024-02-29,cs.LG,
http://arxiv.org/abs/2403.00115v1,"The problem PosSLP is the problem of determining whether a given
straight-line program (SLP) computes a positive integer. PosSLP was introduced
by Allender et al. to study the complexity of numerical analysis (Allender et
al., 2009). PosSLP can also be reformulated as the problem of deciding whether
the integer computed by a given SLP can be expressed as the sum of squares of
four integers, based on the well-known result by Lagrange in 1770, which
demonstrated that every natural number can be represented as the sum of four
non-negative integer squares.
  In this paper, we explore several natural extensions of this problem by
investigating whether the positive integer computed by a given SLP can be
written as the sum of squares of two or three integers. We delve into the
complexity of these variations and demonstrate relations between the complexity
of the original PosSLP problem and the complexity of these related problems.
Additionally, we introduce a new intriguing problem called Div2SLP and
illustrate how Div2SLP is connected to DegSLP and the problem of whether an SLP
computes an integer expressible as the sum of three squares.
  By comprehending the connections between these problems, our results offer a
deeper understanding of decision problems associated with SLPs and open avenues
for further exciting research",2024-02-29,cs.CC,
http://arxiv.org/abs/2403.00111v2,"Introduction: Taxonomies capture knowledge about a particular domain in a
succinct manner and establish a common understanding among peers. Researchers
use taxonomies to convey information about a particular knowledge area or to
support automation tasks, and practitioners use them to enable communication
beyond organizational boundaries. Aims: Despite this important role of
taxonomies in software engineering, their quality is seldom evaluated. Our aim
is to identify and define taxonomy quality attributes that provide practical
measurements, helping researchers and practitioners to compare taxonomies and
choose the one most adequate for the task at hand. Methods: We reviewed 324
publications from software engineering and information systems research and
synthesized, when provided, the definitions of quality attributes and
measurements. We evaluated the usefulness of the measurements on six taxonomies
from three domains. Results: We propose the definition of seven quality
attributes and suggest internal and external measurements that can be used to
assess a taxonomy's quality. For two measurements we provide implementations in
Python. We found the measurements useful for deciding which taxonomy is best
suited for a particular purpose. Conclusion: While there exist several
guidelines for creating taxonomies, there is a lack of actionable criteria to
compare taxonomies. In this paper, we fill this gap by synthesizing from a
wealth of literature seven, non-overlapping taxonomy quality attributes and
corresponding measurements. Future work encompasses their further evaluation of
usefulness and empirical validation.",2024-02-29,cs.SE,
http://arxiv.org/abs/2403.00108v1,"Fine-tuning LLMs is crucial to enhancing their task-specific performance and
ensuring model behaviors are aligned with human preferences. Among various
fine-tuning methods, LoRA is popular for its efficiency and ease to use,
allowing end-users to easily post and adopt lightweight LoRA modules on
open-source platforms to tailor their model for different customization.
However, such a handy share-and-play setting opens up new attack surfaces, that
the attacker can render LoRA as an attacker, such as backdoor injection, and
widely distribute the adversarial LoRA to the community easily. This can result
in detrimental outcomes. Despite the huge potential risks of sharing LoRA
modules, this aspect however has not been fully explored. To fill the gap, in
this study we thoroughly investigate the attack opportunities enabled in the
growing share-and-play scenario. Specifically, we study how to inject backdoor
into the LoRA module and dive deeper into LoRA's infection mechanisms. We found
that training-free mechanism is possible in LoRA backdoor injection. We also
discover the impact of backdoor attacks with the presence of multiple LoRA
adaptions concurrently as well as LoRA based backdoor transferability. Our aim
is to raise awareness of the potential risks under the emerging share-and-play
scenario, so as to proactively prevent potential consequences caused by
LoRA-as-an-Attack. Warning: the paper contains potential offensive content
generated by models.",2024-02-29,cs.CR,
http://arxiv.org/abs/2403.00107v1,"This study aims to investigate the influence of cross-border recruitment
program in China, which confers scientists with a 'talent hat' including a
startup package comprising significant bonuses, pay, and funding, on their
future performance and career development. By curating a unique dataset from
China's 10-year talent recruitment program, we employed multiple matching
designs to quantify the effects of the cross-border recruitment with 'talent
hat' on early career STEM scholars. Our findings indicate that the cross-border
talents perform better than their comparable contenders who move without talent
hats and those who do not move, given equivalent scientific performance before
relocation. Moreover, we observed that scholars in experimental fields derive
greater benefits from the talent program than those in non-experimental fields.
Finally, we investigated how the changes in scientific environment of
scientists affect their future performance. We found that talents who
reassembled their collaboration network with new collaborators in new
institutions after job replacement experienced significant improvements in
their academic performance. However, shifting research directions entails
risks, which results in a subsequent decrease of future productivity and
citation impact following the relocation. This study has significant
implications for young scientists, research institutions, and governments
concerning cultivating cross-border talents.",2024-02-29,cs.DL,
http://arxiv.org/abs/2403.00106v2,"We present Umwelt, an authoring environment for interactive multimodal data
representations. In contrast to prior approaches, which center the visual
modality, Umwelt treats visualization, sonification, and textual description as
coequal representations: they are all derived from a shared abstract data
model, such that no modality is prioritized over the others. To simplify
specification, Umwelt evaluates a set of heuristics to generate default
multimodal representations that express a dataset's functional relationships.
To support smoothly moving between representations, Umwelt maintains a shared
query predicated that is reified across all modalities -- for instance,
navigating the textual description also highlights the visualization and
filters the sonification. In a study with 5 blind / low-vision expert users, we
found that Umwelt's multimodal representations afforded complementary overview
and detailed perspectives on a dataset, allowing participants to fluidly shift
between task- and representation-oriented ways of thinking.",2024-02-29,cs.HC,
http://arxiv.org/abs/2403.00105v1,"Counterfactual explanations are a common approach to providing recourse to
data subjects. However, current methodology can produce counterfactuals that
cannot be achieved by the subject, making the use of counterfactuals for
recourse difficult to justify in practice. Though there is agreement that
plausibility is an important quality when using counterfactuals for algorithmic
recourse, ground truth plausibility continues to be difficult to quantify. In
this paper, we propose using longitudinal data to assess and improve
plausibility in counterfactuals. In particular, we develop a metric that
compares longitudinal differences to counterfactual differences, allowing us to
evaluate how similar a counterfactual is to prior observed changes.
Furthermore, we use this metric to generate plausible counterfactuals. Finally,
we discuss some of the inherent difficulties of using counterfactuals for
recourse.",2024-02-29,cs.LG,
http://arxiv.org/abs/2403.00859v1,"In this work, we formulate the problem of team formation amidst conflicts.
The goal is to assign individuals to tasks, with given capacities, taking into
account individuals' task preferences and the conflicts between them. Using
dependent rounding schemes as our main toolbox, we provide efficient
approximation algorithms. Our framework is extremely versatile and can model
many different real-world scenarios as they arise in educational settings and
human-resource management. We test and deploy our algorithms on real-world
datasets and we show that our algorithms find assignments that are better than
those found by natural baselines. In the educational setting we also show how
our assignments are far better than those done manually by human experts. In
the human resource management application we show how our assignments increase
the diversity of teams. Finally, using a synthetic dataset we demonstrate that
our algorithms scale very well in practice.",2024-02-29,cs.AI,
http://arxiv.org/abs/2403.00103v1,"There is substantial interest in the use of machine learning (ML)-based
techniques throughout the electronic computer-aided design (CAD) flow,
particularly methods based on deep learning. However, while deep learning
methods have achieved state-of-the-art performance in several applications,
recent work has demonstrated that neural networks are generally vulnerable to
small, carefully chosen perturbations of their input (e.g. a single pixel
change in an image). In this work, we investigate robustness in the context of
ML-based EDA tools -- particularly for congestion prediction. As far as we are
aware, we are the first to explore this concept in the context of ML-based EDA.
  We first describe a novel notion of imperceptibility designed specifically
for VLSI layout problems defined on netlists and cell placements. Our
definition of imperceptibility is characterized by a guarantee that a
perturbation to a layout will not alter its global routing. We then demonstrate
that state-of-the-art CNN and GNN-based congestion models exhibit brittleness
to imperceptible perturbations. Namely, we show that when a small number of
cells (e.g. 1%-5% of cells) have their positions shifted such that a measure of
global congestion is guaranteed to remain unaffected (e.g. 1% of the design
adversarially shifted by 0.001% of the layout space results in a predicted
decrease in congestion of up to 90%, while no change in congestion is implied
by the perturbation). In other words, the quality of a predictor can be made
arbitrarily poor (i.e. can be made to predict that a design is
""congestion-free"") for an arbitrary input layout. Next, we describe a simple
technique to train predictors that improves robustness to these perturbations.
Our work indicates that CAD engineers should be cautious when integrating
neural network-based mechanisms in EDA flows to ensure robust and high-quality
results.",2024-02-29,cs.LG,
http://arxiv.org/abs/2403.00099v1,"Model-based testing (MBT) is a method that supports the design and execution
of test cases by models that specify the intended behaviors of a system under
test. While systematic literature reviews on MBT in general exist, the state of
the art on modeling and testing performance requirements has seen much less
attention. Therefore, we conducted a systematic mapping study on model-based
performance testing. Then, we studied natural language software requirements
specifications in order to understand which and how performance requirements
are typically specified. Since none of the identified MBT techniques supported
a major benefit of modeling, namely identifying faults in requirements
specifications, we developed the Performance Requirements verificatiOn and Test
EnvironmentS generaTion approach (PRO-TEST). Finally, we evaluated PRO-TEST on
149 requirements specifications. We found and analyzed 57 primary studies from
the systematic mapping study and extracted 50 performance requirements models.
However, those models don't achieve the goals of MBT, which are validating
requirements, ensuring their testability, and generating the minimum required
test cases. We analyzed 77 Software Requirements Specification (SRS) documents,
extracted 149 performance requirements from those SRS, and illustrate that with
PRO-TEST we can model performance requirements, find issues in those
requirements and detect missing ones. We detected three not-quantifiable
requirements, 43 not-quantified requirements, and 180 underspecified parameters
in the 149 modeled performance requirements. Furthermore, we generated 96 test
environments from those models. By modeling performance requirements with
PRO-TEST, we can identify issues in the requirements related to their
ambiguity, measurability, and completeness. Additionally, it allows to generate
parameters for test environments.",2024-02-29,cs.SE,
http://arxiv.org/abs/2403.00098v1,"The Skolem Problem asks, given an integer linear recurrence sequence (LRS),
to determine whether the sequence contains a zero term or not. Its decidability
is a longstanding open problem in theoretical computer science and automata
theory. Currently, decidability is only known for LRS of order at most 4. On
the other hand, the sole known complexity result is NP-hardness, due to Blondel
and Portier.
  A fundamental result in this area is the celebrated Skolem-Mahler-Lech
theorem, which asserts that the zero set of any LRS is the union of a finite
set and finitely many arithmetic progressions. This paper focuses on a
computational perspective of the Skolem-Mahler-Lech theorem: we show that the
problem of counting the zeros of a given LRS is #P-hard, and in fact
#P-complete for the instances generated in our reduction.",2024-02-29,cs.CC,
http://arxiv.org/abs/2403.00096v1,"This report summarizes the discussions and conclusions of a 2-day
multidisciplinary workshop that brought together researchers and practitioners
in healthcare, computer science, and social sciences to explore what lessons
were learned and what actions, primarily in research, could be taken. One
consistent observation was that there is significant merit in thinking not only
about pandemic situations, but also about peacetime advances, as many
healthcare networks and communities are now in a perpetual state of crisis.
Attendees discussed how the COVID-19 pandemic amplified gaps in our health and
computing systems, and how current and future computing technologies could fill
these gaps and improve the trajectory of the next pandemic.
  Three major computing themes emerged from the workshop: models, data, and
infrastructure. Computational models are extremely important during pandemics,
from anticipating supply needs of hospitals, to determining the care capacity
of hospital and social service providers, to projecting the spread of the
disease. Accurate, reliable models can save lives, and inform community leaders
on policy decisions. Health system users require accurate, reliable data to
achieve success when applying models. This requires data and measurement
standardization across health care organizations, modernizing the data
infrastructure, and methods for ensuring data remains private while shared for
model development, validation, and application. Finally, many health care
systems lack the data, compute, and communication infrastructures required to
build models on their data, use those models in ordinary operations, or even to
reliably access their data. Robust and timely computing research has the
potential to better support healthcare works to save lives in times of crisis
(e.g., pandemics) and today during relative peacetime.",2024-02-29,cs.CY,
http://arxiv.org/abs/2403.00095v1,"Skill mastery is a priority for success in all fields. We present a parallel
between the development of skill mastery and the process of solving jigsaw
puzzles. We show that iterative random sampling solves jigsaw puzzles in two
phases: a lag phase that is characterized by little change and occupies the
majority of the time, and a growth phase that marks rapid and imminent puzzle
completion. Changes in the proportions of the number of single pieces and
larger pieces can be overlaid on the timeline and progression of skill mastery.
An emphasis is placed on the development of connections between pieces, which
serves as an indicator of increasing puzzle completion and increasing skill
mastery. Our manuscript provides a straightforward visual of skill mastery in
the context of a common recreational activity.",2024-02-29,cs.CY,
http://arxiv.org/abs/2403.07927v1,"Cloud service owners need to continuously monitor their services to ensure
high availability and reliability. Gaps in monitoring can lead to delay in
incident detection and significant negative customer impact. Current process of
monitor creation is ad-hoc and reactive in nature. Developers create monitors
using their tribal knowledge and, primarily, a trial and error based process.
As a result, monitors often have incomplete coverage which leads to production
issues, or, redundancy which results in noise and wasted effort.
  In this work, we address this issue by proposing an intelligent monitoring
framework that recommends monitors for cloud services based on their service
properties. We start by mining the attributes of 30,000+ monitors from 791
production services at Microsoft and derive a structured ontology for monitors.
We focus on two crucial dimensions: what to monitor (resources) and which
metrics to monitor. We conduct an extensive empirical study and derive key
insights on the major classes of monitors employed by cloud services at
Microsoft, their associated dimensions, and the interrelationship between
service properties and this ontology. Using these insights, we propose a deep
learning based framework that recommends monitors based on the service
properties. Finally, we conduct a user study with engineers from Microsoft
which demonstrates the usefulness of the proposed framework. The proposed
framework along with the ontology driven projections, succeeded in creating
production quality recommendations for majority of resource classes. This was
also validated by the users from the study who rated the framework's usefulness
as 4.27 out of 5.",2024-02-29,cs.NI,
http://arxiv.org/abs/2404.00498v2,"CIFAR-10 is among the most widely used datasets in machine learning,
facilitating thousands of research projects per year. To accelerate research
and reduce the cost of experiments, we introduce training methods for CIFAR-10
which reach 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3
seconds, when run on a single NVIDIA A100 GPU. As one factor contributing to
these training speeds, we propose a derandomized variant of horizontal flipping
augmentation, which we show improves over the standard method in every case
where flipping is beneficial over no flipping at all. Our code is released at
https://github.com/KellerJordan/cifar10-airbench.",2024-03-30,cs.LG,
http://arxiv.org/abs/2404.00495v1,"State-of-the-art language model fine-tuning techniques, such as Direct
Preference Optimization (DPO), restrict user control by hard-coding predefined
behaviors into the model. To address this, we propose a novel method,
Configurable Safety Tuning (CST), that augments DPO using synthetic preference
data to facilitate flexible safety configuration of LLMs at inference time. CST
overcomes the constraints of vanilla DPO by introducing a system prompt
specifying safety configurations, enabling LLM deployers to disable/enable
safety preferences based on their need, just changing the system prompt. Our
experimental evaluations indicate that CST successfully manages different
safety configurations and retains the original functionality of LLMs, showing
it is a robust method for configurable deployment. Data and models available at
https://github.com/vicgalle/configurable-safety-tuning",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.00494v1,"Perceptions of gender are a significant aspect of human-human interaction,
and gender has wide-reaching social implications for robots deployed in
contexts where they are expected to interact with humans. This work explored
two flexible modalities for communicating gender in robots--voice and
appearance--and we studied their individual and combined influences on a
robot's perceived gender. We evaluated the perception of a robot's gender
through three video-based studies. First, we conducted a study (n=65) on the
gender perception of robot voices by varying speaker identity and pitch.
Second, we conducted a study (n=93) on the gender perception of robot clothing
designed for two different tasks. Finally, building on the results of the first
two studies, we completed a large integrative video-based study (n=273)
involving two human-robot interaction tasks. We found that voice and clothing
can be used to reliably establish a robot's perceived gender, and that
combining these two modalities can have different effects on the robot's
perceived gender. Taken together, these results inform the design of robot
voices and clothing as individual and interacting components in the perceptions
of robot gender.",2024-03-30,cs.RO,
http://arxiv.org/abs/2404.00492v1,"Multi-hop question answering (MQA) under knowledge editing (KE) has garnered
significant attention in the era of large language models. However, existing
models for MQA under KE exhibit poor performance when dealing with questions
containing explicit temporal contexts. To address this limitation, we propose a
novel framework, namely TEMPoral knowLEdge augmented Multi-hop Question
Answering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a
time-aware graph (TAG) to store edit knowledge in a structured manner. Then,
through our proposed inference path, structural retrieval, and joint reasoning
stages, TEMPLE-MQA effectively discerns temporal contexts within the question
query. Experiments on benchmark datasets demonstrate that TEMPLE-MQA
significantly outperforms baseline models. Additionally, we contribute a new
dataset, namely TKEMQA, which serves as the inaugural benchmark tailored
specifically for MQA with temporal scopes.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.00491v1,"Physically-based renderings contain Monte-Carlo noise, with variance that
increases as the number of rays per pixel decreases. This noise, while
zero-mean for good modern renderers, can have heavy tails (most notably, for
scenes containing specular or refractive objects). Learned methods for
restoring low fidelity renders are highly developed, because suppressing render
noise means one can save compute and use fast renders with few rays per pixel.
We demonstrate that a diffusion model can denoise low fidelity renders
successfully. Furthermore, our method can be conditioned on a variety of
natural render information, and this conditioning helps performance.
Quantitative experiments show that our method is competitive with SOTA across a
range of sampling rates, but current metrics slightly favor competitor methods.
Qualitative examination of the reconstructions suggests that the metrics
themselves may not be reliable. The image prior applied by a diffusion method
strongly favors reconstructions that are ""like"" real images -- so have straight
shadow boundaries, curved specularities, no ""fireflies"" and the like -- and
metrics do not account for this. We show numerous examples where methods
preferred by current metrics produce qualitatively weaker reconstructions than
ours.",2024-03-30,cs.CV,
http://arxiv.org/abs/2404.00489v1,"Large language models (LLMs) have shown exceptional abilities for multiple
different natural language processing tasks. While prompting is a crucial tool
for LLM inference, we observe that there is a significant cost associated with
exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead
to sub-standard results in terms of readability and interpretability of the
compressed prompt, with a detrimental impact on prompt utility. To address
this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an
effective strategy for prompt compression over task-agnostic and task-aware
prompts. PROMPT-SAW uses the prompt's textual information to build a graph,
later extracts key information elements in the graph to come up with the
compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the
existing GSM8k benchmark for task-agnostic prompts in order to provide a
comprehensive evaluation platform. Experimental evaluation using benchmark
datasets shows that prompts compressed by PROMPT-SAW are not only better in
terms of readability, but they also outperform the best-performing baseline
models by up to 14.3 and 13.7 respectively for task-aware and task-agnostic
settings while compressing the original prompt text by 33.0 and 56.7.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.00488v1,"A visually rich document (VRD) utilizes visual features along with linguistic
cues to disseminate information. Training a custom extractor that identifies
named entities from a document requires a large number of instances of the
target document type annotated at textual and visual modalities. This is an
expensive bottleneck in enterprise scenarios, where we want to train custom
extractors for thousands of different document types in a scalable way.
Pre-training an extractor model on unlabeled instances of the target document
type, followed by a fine-tuning step on human-labeled instances does not work
in these scenarios, as it surpasses the maximum allowable training time
allocated for the extractor. We address this scenario by proposing a
Noise-Aware Training method or NAT in this paper. Instead of acquiring
expensive human-labeled documents, NAT utilizes weakly labeled documents to
train an extractor in a scalable way. To avoid degradation in the model's
quality due to noisy, weakly labeled samples, NAT estimates the confidence of
each training sample and incorporates it as uncertainty measure during
training. We train multiple state-of-the-art extractor models using NAT.
Experiments on a number of publicly available and in-house datasets show that
NAT-trained models are not only robust in performance -- it outperforms a
transfer-learning baseline by up to 6% in terms of macro-F1 score, but it is
also more label-efficient -- it reduces the amount of human-effort required to
obtain comparable performance by up to 73%.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.00487v1,"MindScape aims to study the benefits of integrating time series behavioral
patterns (e.g., conversational engagement, sleep, location) with Large Language
Models (LLMs) to create a new form of contextual AI journaling, promoting
self-reflection and well-being. We argue that integrating behavioral sensing in
LLMs will likely lead to a new frontier in AI. In this Late-Breaking Work
paper, we discuss the MindScape contextual journal App design that uses LLMs
and behavioral sensing to generate contextual and personalized journaling
prompts crafted to encourage self-reflection and emotional development. We also
discuss the MindScape study of college students based on a preliminary user
study and our upcoming study to assess the effectiveness of contextual AI
journaling in promoting better well-being on college campuses. MindScape
represents a new application class that embeds behavioral intelligence in AI.",2024-03-30,cs.HC,
http://arxiv.org/abs/2404.00486v1,"With the rise of large language models (LLMs), ensuring they embody the
principles of being helpful, honest, and harmless (3H), known as Human
Alignment, becomes crucial. While existing alignment methods like RLHF, DPO,
etc., effectively fine-tune LLMs to match preferences in the preference
dataset, they often lead LLMs to highly receptive human input and external
evidence, even when this information is poisoned. This leads to a tendency for
LLMs to be Adaptive Chameleons when external evidence conflicts with their
parametric memory. This exacerbates the risk of LLM being attacked by external
poisoned data, which poses a significant security risk to LLM system
applications such as Retrieval-augmented generation (RAG). To address the
challenge, we propose a novel framework: Dialectical Alignment (DA), which (1)
utilizes AI feedback to identify optimal strategies for LLMs to navigate
inter-context conflicts and context-memory conflicts with different external
evidence in context window (i.e., different ratios of poisoned factual
contexts); (2) constructs the SFT dataset as well as the preference dataset
based on the AI feedback and strategies above; (3) uses the above datasets for
LLM alignment to defense poisoned context attack while preserving the
effectiveness of in-context knowledge editing. Our experiments show that the
dialectical alignment model improves poisoned data attack defense by 20 and
does not require any additional prompt engineering or prior declaration of
``you may be attacked`` to the LLMs' context window.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.00485v1,"We present DiffHuman, a probabilistic method for photorealistic 3D human
reconstruction from a single RGB image. Despite the ill-posed nature of this
problem, most methods are deterministic and output a single solution, often
resulting in a lack of geometric detail and blurriness in unseen or uncertain
regions. In contrast, DiffHuman predicts a probability distribution over 3D
reconstructions conditioned on an input 2D image, which allows us to sample
multiple detailed 3D avatars that are consistent with the image. DiffHuman is
implemented as a conditional diffusion model that denoises pixel-aligned 2D
observations of an underlying 3D shape representation. During inference, we may
sample 3D avatars by iteratively denoising 2D renders of the predicted 3D
representation. Furthermore, we introduce a generator neural network that
approximates rendering with considerably reduced runtime (55x speed up),
resulting in a novel dual-branch diffusion framework. Our experiments show that
DiffHuman can produce diverse and detailed reconstructions for the parts of the
person that are unseen or uncertain in the input image, while remaining
competitive with the state-of-the-art when reconstructing visible surfaces.",2024-03-30,cs.CV,
http://arxiv.org/abs/2404.00484v1,"The NLI4CT task assesses Natural Language Inference systems in predicting
whether hypotheses entail or contradict evidence from Clinical Trial Reports.
In this study, we evaluate various Large Language Models (LLMs) with multiple
strategies, including Chain-of-Thought, In-Context Learning, and
Parameter-Efficient Fine-Tuning (PEFT). We propose a PEFT method to improve the
consistency of LLMs by merging adapters that were fine-tuned separately using
triplet and language modelling objectives. We found that merging the two PEFT
adapters improves the F1 score (+0.0346) and consistency (+0.152) of the LLMs.
However, our novel methods did not produce more accurate results than GPT-4 in
terms of faithfulness and consistency. Averaging the three metrics, GPT-4 ranks
joint-first in the competition with 0.8328. Finally, our contamination analysis
with GPT-4 indicates that there was no test data leakage.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.00482v2,"This paper presents a corpus manually annotated with named entities for six
Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian.
This work is the result of a series of shared tasks, conducted in 2017-2023 as
a part of the Workshops on Slavic Natural Language Processing. The corpus
consists of 5 017 documents on seven topics. The documents are annotated with
five classes of named entities. Each entity is described by a category, a
lemma, and a unique cross-lingual identifier. We provide two train-tune dataset
splits - single topic out and cross topics. For each split, we set benchmarks
using a transformer-based neural network architecture with the pre-trained
multilingual models - XLM-RoBERTa-large for named entity mention recognition
and categorization, and mT5-large for named entity lemmatization and linking.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.00478v1,"Forensic Scientometrics (FoSci) is emerging as a vital discipline at the
intersection of scientific integrity and security. Scholarship and scholarly
communication are critical for maintaining scientific integrity, influencing
public trust in science, health, technology, policy, and law. Yet, these
foundations are threatened by the misuse of scientific research for personal,
commercial, ideological, and geopolitical gains, including questionable
practices and misconduct. The rise of paper mills and predatory publishers,
along with ideological and geopolitical motivations, undermines academic
integrity. This field pioneers the integration of traditional scientometric
methods with ethics to address pressing challenges in research integrity and
security, crucial in an era of heightened scrutiny over science's reliability.
FoSci's development signifies a collective commitment to maintaining scientific
trust, marked by a call for official recognition and support from stakeholders
across the scientific ecosystem.",2024-03-30,cs.DL,
http://arxiv.org/abs/2404.00477v3,"The run-time for optimization tools used in chip design has grown with the
complexity of designs to the point where it can take several days to go through
one design cycle which has become a bottleneck. Designers want fast tools that
can quickly give feedback on a design. Using the input and output data of the
tools from past designs, one can attempt to build a machine learning model that
predicts the outcome of a design in significantly shorter time than running the
tool. The accuracy of such models is affected by the representation of the
design data, which is usually a netlist that describes the elements of the
digital circuit and how they are connected. Graph representations for the
netlist together with graph neural networks have been investigated for such
models. However, the characteristics of netlists pose several challenges for
existing graph learning frameworks, due to the large number of nodes and the
importance of long-range interactions between nodes. To address these
challenges, we represent the netlist as a directed hypergraph and propose a
Directional Equivariant Hypergraph Neural Network (DE-HNN) for the effective
learning of (directed) hypergraphs. Theoretically, we show that our DE-HNN can
universally approximate any node or hyperedge based function that satisfies
certain permutation equivariant and invariant properties natural for directed
hypergraphs. We compare the proposed DE-HNN with several State-of-the-art
(SOTA) machine learning models for (hyper)graphs and netlists, and show that
the DE-HNN significantly outperforms them in predicting the outcome of
optimized place-and-route tools directly from the input netlists. Our source
code and the netlists data used are publicly available at
https://github.com/YusuLab/chips.git",2024-03-30,cs.LG,
http://arxiv.org/abs/2404.00474v2,"Language models (LMs) may lead their users to make suboptimal downstream
decisions when they confidently hallucinate. This issue can be mitigated by
having the LM verbally convey the probability that its claims are correct, but
existing models cannot produce long-form text with calibrated confidence
statements. Through the lens of decision-making, we define linguistic
calibration for long-form generations: an LM is linguistically calibrated if
its generations enable its users to make calibrated probabilistic predictions.
This definition enables a training framework where a supervised finetuning step
bootstraps an LM to emit long-form generations with confidence statements such
as ""I estimate a 30% chance of..."" or ""I am certain that..."", followed by a
reinforcement learning step which rewards generations that enable a user to
provide calibrated answers to related questions. We linguistically calibrate
Llama 2 7B and find in automated and human evaluations of long-form generations
that it is significantly more calibrated than strong finetuned factuality
baselines with comparable accuracy. These findings generalize under significant
domain shifts to scientific and biomedical questions and to an entirely
held-out person biography generation task. Our results demonstrate that
long-form generations may be calibrated end-to-end by constructing an objective
in the space of the predictions that users make in downstream decision-making.",2024-03-30,cs.LG,
http://arxiv.org/abs/2404.00473v1,"Practitioners commonly download pretrained machine learning models from open
repositories and finetune them to fit specific applications. We show that this
practice introduces a new risk of privacy backdoors. By tampering with a
pretrained model's weights, an attacker can fully compromise the privacy of the
finetuning data. We show how to build privacy backdoors for a variety of
models, including transformers, which enable an attacker to reconstruct
individual finetuning samples, with a guaranteed success! We further show that
backdoored models allow for tight privacy attacks on models trained with
differential privacy (DP). The common optimistic practice of training DP models
with loose privacy guarantees is thus insecure if the model is not trusted.
Overall, our work highlights a crucial and overlooked supply chain attack on
machine learning privacy.",2024-03-30,cs.CR,
http://arxiv.org/abs/2404.00470v1,"Congenital anomalies arising as a result of a defect in the structure of the
heart and great vessels are known as congenital heart diseases or CHDs. A PCG
can provide essential details about the mechanical conduction system of the
heart and point out specific patterns linked to different kinds of CHD. This
study aims to investigate the minimum signal duration required for the
automatic classification of heart sounds. This study also investigated the
optimum signal quality assessment indicator (Root Mean Square of Successive
Differences) RMSSD and (Zero Crossings Rate) ZCR value. Mel-frequency cepstral
coefficients (MFCCs) based feature is used as an input to build a
Transformer-Based residual one-dimensional convolutional neural network, which
is then used for classifying the heart sound. The study showed that 0.4 is the
ideal threshold for getting suitable signals for the RMSSD and ZCR indicators.
Moreover, a minimum signal length of 5s is required for effective heart sound
classification. It also shows that a shorter signal (3 s heart sound) does not
have enough information to categorize heart sounds accurately, and the longer
signal (15 s heart sound) may contain more noise. The best accuracy, 93.69%, is
obtained for the 5s signal to distinguish the heart sound.",2024-03-30,cs.SD,
http://arxiv.org/abs/2404.00469v1,"We introduce a novel problem, i.e., the localization of an input image within
a multi-modal reference map represented by a database of 3D scene graphs. These
graphs comprise multiple modalities, including object-level point clouds,
images, attributes, and relationships between objects, offering a lightweight
and efficient alternative to conventional methods that rely on extensive image
databases. Given the available modalities, the proposed method SceneGraphLoc
learns a fixed-sized embedding for each node (i.e., representing an object
instance) in the scene graph, enabling effective matching with the objects
visible in the input query image. This strategy significantly outperforms other
cross-modal methods, even without incorporating images into the map embeddings.
When images are leveraged, SceneGraphLoc achieves performance close to that of
state-of-the-art techniques depending on large image databases, while requiring
three orders-of-magnitude less storage and operating orders-of-magnitude
faster. The code will be made public.",2024-03-30,cs.CV,
http://arxiv.org/abs/2404.00466v1,"The exploration of computational and communication efficiency within
Federated Learning (FL) has emerged as a prominent and crucial field of study.
While most existing efforts to enhance these efficiencies have focused on
Horizontal FL, the distinct processes and model structures of Vertical FL
preclude the direct application of Horizontal FL-based techniques. In response,
we introduce the concept of Lightweight Vertical Federated Learning (LVFL),
targeting both computational and communication efficiencies. This approach
involves separate lightweighting strategies for the feature model, to improve
computational efficiency, and for feature embedding, to enhance communication
efficiency. Moreover, we establish a convergence bound for our LVFL algorithm,
which accounts for both communication and computational lightweighting ratios.
Our evaluation of the algorithm on a image classification dataset reveals that
LVFL significantly alleviates computational and communication demands while
preserving robust learning performance. This work effectively addresses the
gaps in communication and computational efficiency within Vertical FL.",2024-03-30,cs.LG,
http://arxiv.org/abs/2404.00464v1,"Alzheimer's disease is a progressive, debilitating neurodegenerative disease
that affects 50 million people globally. Despite this substantial health
burden, available treatments for the disease are limited and its fundamental
causes remain poorly understood. Previous work has suggested the existence of
clinically-meaningful sub-types, which it is suggested may correspond to
distinct etiologies, disease courses, and ultimately appropriate treatments.
Here, we use unsupervised learning techniques on electronic health records
(EHRs) from a cohort of memory disorder patients to characterise heterogeneity
in this disease population. Pre-trained embeddings for medical codes as well as
transformer-derived Clinical BERT embeddings of free text are used to encode
patient EHRs. We identify the existence of sub-populations on the basis of
comorbidities and shared textual features, and discuss their clinical
significance.",2024-03-30,cs.LG,
http://arxiv.org/abs/2404.00463v1,"Statistical fairness stipulates equivalent outcomes for every protected
group, whereas causal fairness prescribes that a model makes the same
prediction for an individual regardless of their protected characteristics.
Counterfactual data augmentation (CDA) is effective for reducing bias in NLP
models, yet models trained with CDA are often evaluated only on metrics that
are closely tied to the causal fairness notion; similarly, sampling-based
methods designed to promote statistical fairness are rarely evaluated for
causal fairness. In this work, we evaluate both statistical and causal
debiasing methods for gender bias in NLP models, and find that while such
methods are effective at reducing bias as measured by the targeted metric, they
do not necessarily improve results on other bias metrics. We demonstrate that
combinations of statistical and causal debiasing techniques are able to reduce
bias measured through both types of metrics.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.00462v3,"A world model creates a surrogate world to train a controller and predict
safety violations by learning the internal dynamic model of systems. However,
the existing world models rely solely on statistical learning of how
observations change in response to actions, lacking precise quantification of
how accurate the surrogate dynamics are, which poses a significant challenge in
safety-critical systems. To address this challenge, we propose foundation world
models that embed observations into meaningful and causally latent
representations. This enables the surrogate dynamics to directly predict causal
future states by leveraging a training-free large language model. In two common
benchmarks, this novel model outperforms standard world models in the safety
prediction task and has a performance comparable to supervised learning despite
not using any data. We evaluate its performance with a more specialized and
system-relevant metric by comparing estimated states instead of aggregating
observation-wide error.",2024-03-30,cs.LG,
http://arxiv.org/abs/2404.00461v1,"Prompt-based learning paradigm has demonstrated remarkable efficacy in
enhancing the adaptability of pretrained language models (PLMs), particularly
in few-shot scenarios. However, this learning paradigm has been shown to be
vulnerable to backdoor attacks. The current clean-label attack, employing a
specific prompt as a trigger, can achieve success without the need for external
triggers and ensure correct labeling of poisoned samples, which is more
stealthy compared to the poisoned-label attack, but on the other hand, it faces
significant issues with false activations and poses greater challenges,
necessitating a higher rate of poisoning. Using conventional negative data
augmentation methods, we discovered that it is challenging to trade off between
effectiveness and stealthiness in a clean-label setting. In addressing this
issue, we are inspired by the notion that a backdoor acts as a shortcut and
posit that this shortcut stems from the contrast between the trigger and the
data utilized for poisoning. In this study, we propose a method named
Contrastive Shortcut Injection (CSI), by leveraging activation values,
integrates trigger design and data selection strategies to craft stronger
shortcut features. With extensive experiments on full-shot and few-shot text
classification tasks, we empirically validate CSI's high effectiveness and high
stealthiness at low poisoning rates. Notably, we found that the two approaches
play leading roles in full-shot and few-shot settings, respectively.",2024-03-30,cs.LG,
http://arxiv.org/abs/2404.00459v1,"Language models struggle with handling numerical data and performing
arithmetic operations. We hypothesize that this limitation can be partially
attributed to non-intuitive textual numbers representation. When a digit is
read or generated by a causal language model it does not know its place value
(e.g. thousands vs. hundreds) until the entire number is processed. To address
this issue, we propose a simple adjustment to how numbers are represented by
including the count of digits before each number. For instance, instead of
""42"", we suggest using ""{2:42}"" as the new format. This approach, which we term
NumeroLogic, offers an added advantage in number generation by serving as a
Chain of Thought (CoT). By requiring the model to consider the number of digits
first, it enhances the reasoning process before generating the actual number.
We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic
formatting. We further demonstrate NumeroLogic applicability to general natural
language modeling, improving language understanding performance in the MMLU
benchmark.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.00458v1,"This position paper proposes a systematic approach towards developing a
framework to help select the most effective embedding models for natural
language processing (NLP) tasks, addressing the challenge posed by the
proliferation of both proprietary and open-source encoder models.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.00457v1,"Information extraction (IE) is a fundamental area in natural language
processing where prompting large language models (LLMs), even with in-context
examples, cannot defeat small LMs tuned on very small IE datasets. We observe
that IE tasks, such as named entity recognition and relation extraction, all
focus on extracting important information, which can be formalized as a
label-to-span matching. In this paper, we propose a novel framework MetaIE to
build a small LM as meta-model by learning to extract ""important information"",
i.e., the meta-understanding of IE, so that this meta-model can be adapted to
all kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains
the small LM via a symbolic distillation from an LLM following the
label-to-span scheme. We construct the distillation dataset via sampling
sentences from language model pre-training datasets (e.g., OpenWebText in our
implementation) and prompting an LLM to identify the typed spans of ""important
information"". We evaluate the meta-model under the few-shot adaptation setting.
Extensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer
a better starting point for few-shot tuning on IE datasets and outperform other
meta-models from (1) vanilla language model pre-training, (2) multi-IE-task
pre-training with human annotations, and (3) single-IE-task symbolic
distillation from LLM. Moreover, we provide comprehensive analyses of MetaIE,
such as the size of the distillation dataset, the meta-model architecture, and
the size of the meta-model.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.00456v1,"We introduce QuaRot, a new Quantization scheme based on Rotations, which is
able to quantize LLMs end-to-end, including all weights, activations, and KV
cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the
hidden state without changing the output, making quantization easier. This
computational invariance is applied to the hidden state (residual) of the LLM,
as well as to the activations of the feed-forward components, aspects of the
attention mechanism and to the KV cache. The result is a quantized model where
all matrix multiplications are performed in 4-bits, without any channels
identified for retention in higher precision. Our quantized LLaMa2-70B model
has losses of at most 0.29 WikiText-2 perplexity and retains 99% of the
zero-shot performance. Code is available at: https://github.com/spcl/QuaRot.",2024-03-30,cs.LG,
http://arxiv.org/abs/2404.00451v1,"In this work, we aim to teach robots to manipulate various thin-shell
materials. Prior works studying thin-shell object manipulation mostly rely on
heuristic policies or learn policies from real-world video demonstrations, and
only focus on limited material types and tasks (e.g., cloth unfolding).
However, these approaches face significant challenges when extended to a wider
variety of thin-shell materials and a diverse range of tasks. While virtual
simulations are shown to be effective in diverse robot skill learning and
evaluation, prior thin-shell simulation environments only support a subset of
thin-shell materials, which also limits their supported range of tasks. We
introduce ThinShellLab - a fully differentiable simulation platform tailored
for robotic interactions with diverse thin-shell materials possessing varying
material properties, enabling flexible thin-shell manipulation skill learning
and evaluation. Our experiments suggest that manipulating thin-shell objects
presents several unique challenges: 1) thin-shell manipulation relies heavily
on frictional forces due to the objects' co-dimensional nature, 2) the
materials being manipulated are highly sensitive to minimal variations in
interaction actions, and 3) the constant and frequent alteration in contact
pairs makes trajectory optimization methods susceptible to local optima, and
neither standard reinforcement learning algorithms nor trajectory optimization
methods (either gradient-based or gradient-free) are able to solve the tasks
alone. To overcome these challenges, we present an optimization scheme that
couples sampling-based trajectory optimization and gradient-based optimization,
boosting both learning efficiency and converged performance across various
proposed tasks. In addition, the differentiable nature of our platform
facilitates a smooth sim-to-real transition.",2024-03-30,cs.RO,
http://arxiv.org/abs/2404.00450v2,"Recent advancements in integrating external tools with Large Language Models
(LLMs) have opened new frontiers, with applications in mathematical reasoning,
code generators, and smart assistants. However, existing methods, relying on
simple one-time retrieval strategies, fall short on effectively and accurately
shortlisting relevant tools. This paper introduces a novel PLUTO (Planning,
Learning, and Understanding for TOols) approach, encompassing
`Plan-and-Retrieve (P&R)` and `Edit-and-Ground (E&G)` paradigms. The P&R
paradigm consists of a neural retrieval module for shortlisting relevant tools
and an LLM-based query planner that decomposes complex queries into actionable
tasks, enhancing the effectiveness of tool utilization. The E&G paradigm
utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the
gap between user queries and tool functionalities. Experiment results
demonstrate that these paradigms significantly improve the recall and NDCG in
tool retrieval tasks, significantly surpassing current state-of-the-art models.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.00447v1,"The aim of this study is to investigate an automated industrial manipulation
pipeline, where assembly tasks can be flexibly adapted to production without
the need for a robotic expert, both for the vision system and the robot
program. The objective of this study is first, to develop a
synthetic-dataset-generation pipeline with a special focus on industrial parts,
and second, to use Learning-from-Demonstration (LfD) methods to replace manual
robot programming, so that a non-robotic expert/process engineer can introduce
a new manipulation task by teaching it to the robot.",2024-03-30,cs.RO,
http://arxiv.org/abs/2404.00446v2,"We address the question of characterising the well-formedness properties of
multiparty session types semantically, i.e., as properties of the semantic
model used to interpret types. Choosing Prime Event Structures (PESs) as our
semantic model, we present semantic counterparts for the two properties that
underpin global type well-formedness, namely projectability and boundedness, in
this model. As a first step towards a characterisation of the class of PESs
corresponding to well-formed global types, we identify some simple structural
properties satisfied by such PESs.",2024-03-30,cs.LO,
http://arxiv.org/abs/2404.00443v1,"Mobile manipulators are known for their superior mobility over manipulators
on fixed bases, offering promising applications in smart industry and
housekeeping scenarios. However, the dynamic coupling nature between the mobile
base and the manipulator presents challenges for the physical interactive tasks
of the mobile manipulator. Current methods suffer from complex modeling
processes and poor transferability. To address this, this article presents a
novel dynamic model of the manipulator on the mobile base that requires only
the manipulator dynamics and the kinematic information of the mobile base. In
addition, embedding the dynamic model, an uncertainty and disturbance
estimator-based (UDE-based) dynamic motion/force control scheme is proposed for
the mobile manipulator, which compensates for the dynamic coupling and other
unmodeled uncertainties. Passivity and stability analyses justify the proposed
control law. Simulation and experimental results on our mobile manipulator
platform demonstrate the feasibility and effectiveness of our proposed
methodology.",2024-03-30,cs.RO,
http://arxiv.org/abs/2404.00442v1,"For decades, robotics researchers have pursued various tasks for multi-robot
systems, from cooperative manipulation to search and rescue. These tasks are
multi-robot extensions of classical robotic tasks and often optimized on
dimensions such as speed or efficiency. As robots transition from commercial
and research settings into everyday environments, social task aims such as
engagement or entertainment become increasingly relevant. This work presents a
compelling multi-robot task, in which the main aim is to enthrall and interest.
In this task, the goal is for a human to be drawn to move alongside and
participate in a dynamic, expressive robot flock. Towards this aim, the
research team created algorithms for robot movements and engaging interaction
modes such as gestures and sound. The contributions are as follows: (1) a novel
group navigation algorithm involving human and robot agents, (2) a gesture
responsive algorithm for real-time, human-robot flocking interaction, (3) a
weight mode characterization system for modifying flocking behavior, and (4) a
method of encoding a choreographer's preferences inside a dynamic, adaptive,
learned system. An experiment was performed to understand individual human
behavior while interacting with the flock under three conditions: weight modes
selected by a human choreographer, a learned model, or subset list. Results
from the experiment showed that the perception of the experience was not
influenced by the weight mode selection. This work elucidates how differing
task aims such as engagement manifest in multi-robot system design and
execution, and broadens the domain of multi-robot tasks.",2024-03-30,cs.RO,
http://arxiv.org/abs/2404.00441v1,"Over the last couple of decades, there has been a surge in various approaches
to multiple-point statistics simulation, commonly referred to as MPS. These
methods have aimed to improve several critical aspects of realism in the
results, including spatial continuity, conditioning, stochasticity, and
computational efficiency. Nevertheless, achieving a simultaneous enhancement of
these crucial factors has presented challenges to researchers. In the approach
that we propose, CCSIM is combined with the Discrete Wavelet Transform (DWT) to
address some of these concerns. The primary step in the method involves the
computation of the DWT for both the Training Image (TI) and a region shared
with previously simulated grids at a specific level of wavelet decomposition.
Then, the degree of similarity between the wavelet approximation coefficients
is measured using a Cross-Correlation Function (CCF). These approximation
coefficients offer a compressed representation of the pattern while capturing
its primary variations and essential characteristics, thereby expediting the
search for the best-matched pattern. Once the best-matched pattern in the
wavelet approximation coefficients is identified, the original pattern can be
perfectly reconstructed by integrating the DWT detail coefficients through an
Inverse-DWT transformation. Experiments conducted across diverse categorical
TIs demonstrate simulations comparable to multi-scale CCSIM (MS-CCSIM),
accompanied by an enhancement in facies connectivity and pattern reproduction.
The source code implementations are available at
https://github.com/MBS1984/CCWSIM.",2024-03-30,cs.GR,
http://arxiv.org/abs/2404.00439v1,"The application of natural language processing models to PDF documents is
pivotal for various business applications yet the challenge of training models
for this purpose persists in businesses due to specific hurdles. These include
the complexity of working with PDF formats that necessitate parsing text and
layout information for curating training data and the lack of
privacy-preserving annotation tools. This paper introduces DOCMASTER, a unified
platform designed for annotating PDF documents, model training, and inference,
tailored to document question-answering. The annotation interface enables users
to input questions and highlight text spans within the PDF file as answers,
saving layout information and text spans accordingly. Furthermore, DOCMASTER
supports both state-of-the-art layout-aware and text models for comprehensive
training purposes. Importantly, as annotations, training, and inference occur
on-device, it also safeguards privacy. The platform has been instrumental in
driving several research prototypes concerning document analysis such as the AI
assistant utilized by University of California San Diego's (UCSD) International
Services and Engagement Office (ISEO) for processing a substantial volume of
PDF documents.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.00438v1,"The Lion optimizer has been a promising competitor with the AdamW for
training large AI models, with advantages on memory, computation, and sample
efficiency. In this paper, we introduce Distributed Lion, an innovative
adaptation of Lion for distributed training environments. Leveraging the sign
operator in Lion, our Distributed Lion only requires communicating binary or
lower-precision vectors between workers to the center server, significantly
reducing the communication cost. Our theoretical analysis confirms Distributed
Lion's convergence properties. Empirical results demonstrate its robustness
across a range of tasks, worker counts, and batch sizes, on both vision and
language problems. Notably, Distributed Lion attains comparable performance to
standard Lion or AdamW optimizers applied on aggregated gradients, but with
significantly reduced communication bandwidth. This feature is particularly
advantageous for training large models. In addition, we also demonstrate that
Distributed Lion presents a more favorable performance-bandwidth balance
compared to existing efficient distributed methods such as deep gradient
compression and ternary gradients.",2024-03-30,cs.DC,
http://arxiv.org/abs/2404.00437v1,"Automatic legal text classification systems have been proposed in the
literature to address knowledge extraction from judgments and detect their
aspects. However, most of these systems are black boxes even when their models
are interpretable. This may raise concerns about their trustworthiness.
Accordingly, this work contributes with a system combining Natural Language
Processing (NLP) with Machine Learning (ML) to classify legal texts in an
explainable manner. We analyze the features involved in the decision and the
threshold bifurcation values of the decision paths of tree structures and
present this information to the users in natural language. This is the first
work on automatic analysis of legal texts combining NLP and ML along with
Explainable Artificial Intelligence techniques to automatically make the
models' decisions understandable to end users. Furthermore, legal experts have
validated our solution, and this knowledge has also been incorporated into the
explanation process as ""expert-in-the-loop"" dictionaries. Experimental results
on an annotated data set in law categories by jurisdiction demonstrate that our
system yields competitive classification performance, with accuracy values well
above 90%, and that its automatic explanations are easily understandable even
to non-expert users.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.01338v1,"Financial news items are unstructured sources of information that can be
mined to extract knowledge for market screening applications. Manual extraction
of relevant information from the continuous stream of finance-related news is
cumbersome and beyond the skills of many investors, who, at most, can follow a
few sources and authors. Accordingly, we focus on the analysis of financial
news to identify relevant text and, within that text, forecasts and
predictions. We propose a novel Natural Language Processing (NLP) system to
assist investors in the detection of relevant financial events in unstructured
textual sources by considering both relevance and temporality at the discursive
level. Firstly, we segment the text to group together closely related text.
Secondly, we apply co-reference resolution to discover internal dependencies
within segments. Finally, we perform relevant topic modelling with Latent
Dirichlet Allocation (LDA) to separate relevant from less relevant text and
then analyse the relevant text using a Machine Learning-oriented temporal
approach to identify predictions and speculative statements. We created an
experimental data set composed of 2,158 financial news items that were manually
labelled by NLP researchers to evaluate our solution. The ROUGE-L values for
the identification of relevant text and predictions/forecasts were 0.662 and
0.982, respectively. To our knowledge, this is the first work to jointly
consider relevance and temporality at the discursive level. It contributes to
the transfer of human associative discourse capabilities to expert systems
through the combination of multi-paragraph topic segmentation and co-reference
resolution to separate author expression patterns, topic modelling with LDA to
detect relevant text, and discursive temporality analysis to identify forecasts
and predictions within this text.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.00431v1,"Street-level visual appearances play an important role in studying social
systems, such as understanding the built environment, driving routes, and
associated social and economic factors. It has not been integrated into a
typical geographical visualization interface (e.g., map services) for planning
driving routes. In this paper, we study this new visualization task with
several new contributions. First, we experiment with a set of AI techniques and
propose a solution of using semantic latent vectors for quantifying visual
appearance features. Second, we calculate image similarities among a large set
of street-view images and then discover spatial imagery patterns. Third, we
integrate these discovered patterns into driving route planners with new
visualization techniques. Finally, we present VivaRoutes, an interactive
visualization prototype, to show how visualizations leveraged with these
discovered patterns can help users effectively and interactively explore
multiple routes. Furthermore, we conducted a user study to assess the
usefulness and utility of VivaRoutes.",2024-03-30,cs.HC,
http://arxiv.org/abs/2404.00429v1,"We introduce a novel framework for multiway point cloud mosaicking (named
Wednesday), designed to co-align sets of partially overlapping point clouds --
typically obtained from 3D scanners or moving RGB-D cameras -- into a unified
coordinate system. At the core of our approach is ODIN, a learned pairwise
registration algorithm that iteratively identifies overlaps and refines
attention scores, employing a diffusion-based process for denoising pairwise
correlation matrices to enhance matching accuracy. Further steps include
constructing a pose graph from all point clouds, performing rotation averaging,
a novel robust algorithm for re-estimating translations optimally in terms of
consensus maximization and translation optimization. Finally, the point cloud
rotations and positions are optimized jointly by a diffusion-based approach.
Tested on four diverse, large-scale datasets, our method achieves
state-of-the-art pairwise and multiway registration results by a large margin
on all benchmarks. Our code and models are available at
https://github.com/jinsz/Multiway-Point-Cloud-Mosaicking-with-Diffusion-and-Global-Optimization.",2024-03-30,cs.CV,
http://arxiv.org/abs/2404.00426v1,"Drones may be more advantageous than fixed cameras for quality control
applications in industrial facilities, since they can be redeployed dynamically
and adjusted to production planning. The practical scenario that has motivated
this paper, image acquisition with drones in a car manufacturing plant,
requires drone positioning accuracy in the order of 5 cm. During repetitive
manufacturing processes, it is assumed that quality control imaging drones will
follow highly deterministic periodic paths, stop at predefined points to take
images and send them to image recognition servers. Therefore, by relying on
prior knowledge about production chain schedules, it is possible to optimize
the positioning technologies for the drones to stay at all times within the
boundaries of their flight plans, which will be composed of stopping points and
the paths in between. This involves mitigating issues such as temporary
blocking of line-of-sight between the drone and any existing radio beacons;
sensor data noise; and the loss of visual references. We present a
self-corrective solution for this purpose. It corrects visual odometer readings
based on filtered and clustered Ultra-Wide Band (UWB) data, as an alternative
to direct Kalman fusion. The approach combines the advantages of these
technologies when at least one of them works properly at any measurement spot.
It has three method components: independent Kalman filtering, data association
by means of stream clustering and mutual correction of sensor readings based on
the generation of cumulative correction vectors. The approach is inspired by
the observation that UWB positioning works reasonably well at static spots
whereas visual odometer measurements reflect straight displacements correctly
but can underestimate their length. Our experimental results demonstrate the
advantages of the approach in the application scenario over Kalman fusion.",2024-03-30,cs.RO,
http://arxiv.org/abs/2404.00423v1,"Password management has long been a persistently challenging task. This led
to the introduction of password management software, which has been around for
at least 25 years in various forms, including desktop and browser-based
applications. This work assesses the ability of two dozen password managers, 12
desktop applications, and 12 browser-plugins, to effectively protect the
confidentiality of secret credentials in six representative scenarios. Our
analysis focuses on the period during which a Password Manager (PM) resides in
the RAM. Despite the sensitive nature of these applications, our results show
that across all scenarios, only three desktop PM applications and two browser
plugins do not store plaintext passwords in the system memory. Oddly enough, at
the time of writing, only two vendors recognized the exploit as a
vulnerability, reserving CVE-2023-23349, while the rest chose to disregard or
underrate the issue.",2024-03-30,cs.CR,
http://arxiv.org/abs/2404.00420v1,"Increasingly, more software services have been published onto the Internet,
making it a big challenge to recommend services in the process of a scientific
workflow composition. In this paper, a novel context-aware approach is proposed
to recommending next services in a workflow development process, through
learning service representation and service selection decision making behaviors
from workflow provenance. Inspired by natural language sentence generation, the
composition process of a scientific workflow is formalized as a step-wise
procedure within the context of the goal of workflow, and the problem of next
service recommendation is mapped to next word prediction. Historical service
dependencies are first extracted from scientific workflow provenance to build a
knowledge graph. Service sequences are then generated based on diverse
composition path generation strategies. Afterwards, the generated corpus of
composition paths are leveraged to study previous decision making strategies.
Such a trained goal-oriented next service prediction model will be used to
recommend top K candidate services during workflow composition process.
Extensive experiments on a real-word repository have demonstrated the
effectiveness of this approach.",2024-03-30,cs.SE,
http://arxiv.org/abs/2404.00419v1,"Open-vocabulary vision-language models (VLMs) like CLIP, trained using
contrastive loss, have emerged as a promising new paradigm for text-to-image
retrieval. However, do VLMs understand compound nouns (CNs) (e.g., lab coat) as
well as they understand nouns (e.g., lab)? We curate Compun, a novel benchmark
with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in
interpreting CNs. The Compun benchmark challenges a VLM for text-to-image
retrieval where, given a text prompt with a CN, the task is to select the
correct image that shows the CN among a pair of distractor images that show the
constituent nouns that make up the CN. Next, we perform an in-depth analysis to
highlight CLIPs' limited understanding of certain types of CNs. Finally, we
present an alternative framework that moves beyond hand-written templates for
text prompts widely used by CLIP-like models. We employ a Large Language Model
to generate multiple diverse captions that include the CN as an object in the
scene described by the caption. Our proposed method improves CN understanding
of CLIP by 8.25% on Compun. Code and benchmark are available at:
https://github.com/sonalkum/Compun",2024-03-30,cs.CV,
http://arxiv.org/abs/2404.00418v1,"Humans and animals learn throughout their lives from limited amounts of
sensed data, both with and without supervision. Autonomous, intelligent robots
of the future are often expected to do the same. The existing continual
learning (CL) methods are usually not directly applicable to robotic settings:
they typically require buffering and a balanced replay of training data. A
few-shot online continual learning (FS-OCL) setting has been proposed to
address more realistic scenarios where robots must learn from a non-repeated
sparse data stream. To enable truly autonomous life-long learning, an
additional challenge of detecting novelties and learning new items without
supervision needs to be addressed. We address this challenge with our new
prototype-based approach called Continually Learning Prototypes (CLP). In
addition to being capable of FS-OCL learning, CLP also detects novel objects
and learns them without supervision. To mitigate forgetting, CLP utilizes a
novel metaplasticity mechanism that adapts the learning rate individually per
prototype. CLP is rehearsal-free, hence does not require a memory buffer, and
is compatible with neuromorphic hardware, characterized by ultra-low power
consumption, real-time processing abilities, and on-chip learning. Indeed, we
have open-sourced a simple version of CLP in the neuromorphic software
framework Lava, targetting Intel's neuromorphic chip Loihi 2. We evaluate CLP
on a robotic vision dataset, OpenLORIS. In a low-instance FS-OCL scenario, CLP
shows state-of-the-art results. In the open world, CLP detects novelties with
superior precision and recall and learns features of the detected novel classes
without supervision, achieving a strong baseline of 99% base class and 65%/76%
(5-shot/10-shot) novel class accuracy.",2024-03-30,cs.LG,
http://arxiv.org/abs/2404.00417v1,"To accommodate real-world dynamics, artificial intelligence systems need to
cope with sequentially arriving content in an online manner. Beyond regular
Continual Learning (CL) attempting to address catastrophic forgetting with
offline training of each task, Online Continual Learning (OCL) is a more
challenging yet realistic setting that performs CL in a one-pass data stream.
Current OCL methods primarily rely on memory replay of old training samples.
However, a notable gap from CL to OCL stems from the additional
overfitting-underfitting dilemma associated with the use of rehearsal buffers:
the inadequate learning of new training samples (underfitting) and the repeated
learning of a few old training samples (overfitting). To this end, we introduce
a novel approach, Multi-level Online Sequential Experts (MOSE), which
cultivates the model as stacked sub-experts, integrating multi-level
supervision and reverse self-distillation. Supervision signals across multiple
stages facilitate appropriate convergence of the new task while gathering
various strengths from experts by knowledge distillation mitigates the
performance decline of old tasks. MOSE demonstrates remarkable efficacy in
learning new samples and preserving past knowledge through multi-level experts,
thereby significantly advancing OCL performance over state-of-the-art baselines
(e.g., up to 7.3% on Split CIFAR-100 and 6.1% on Split Tiny-ImageNet).",2024-03-30,cs.LG,
http://arxiv.org/abs/2404.00415v1,"We present CoDa (Constrained Generation based Data Augmentation), a
controllable, effective, and training-free data augmentation technique for
low-resource (data-scarce) NLP. Our approach is based on prompting
off-the-shelf instruction-following Large Language Models (LLMs) for generating
text that satisfies a set of constraints. Precisely, we extract a set of simple
constraints from every instance in the low-resource dataset and verbalize them
to prompt an LLM to generate novel and diverse training instances. Our findings
reveal that synthetic data that follows simple constraints in the downstream
dataset act as highly effective augmentations, and CoDa can achieve this
without intricate decoding-time constrained generation techniques or
fine-tuning with complex algorithms that eventually make the model biased
toward the small number of training instances. Additionally, CoDa is the first
framework that provides users explicit control over the augmentation generation
process, thereby also allowing easy adaptation to several domains. We
demonstrate the effectiveness of CoDa across 11 datasets spanning 3 tasks and 3
low-resource settings. CoDa outperforms all our baselines, qualitatively and
quantitatively, with improvements of 0.12%-7.19%. Code is available here:
https://github.com/Sreyan88/CoDa",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.08665v1,"Microblogging platforms, of which Twitter is a representative example, are
valuable information sources for market screening and financial models. In
them, users voluntarily provide relevant information, including educated
knowledge on investments, reacting to the state of the stock markets in
real-time and, often, influencing this state. We are interested in the user
forecasts in financial, social media messages expressing opportunities and
precautions about assets. We propose a novel Targeted Aspect-Based Emotion
Analysis (TABEA) system that can individually discern the financial emotions
(positive and negative forecasts) on the different stock market assets in the
same tweet (instead of making an overall guess about that whole tweet). It is
based on Natural Language Processing (NLP) techniques and Machine Learning
streaming algorithms. The system comprises a constituency parsing module for
parsing the tweets and splitting them into simpler declarative clauses; an
offline data processing module to engineer textual, numerical and categorical
features and analyse and select them based on their relevance; and a stream
classification module to continuously process tweets on-the-fly. Experimental
results on a labelled data set endorse our solution. It achieves over 90%
precision for the target emotions, financial opportunity, and precaution on
Twitter. To the best of our knowledge, no prior work in the literature has
addressed this problem despite its practical interest in decision-making, and
we are not aware of any previous NLP nor online Machine Learning approaches to
TABEA.",2024-03-30,cs.IR,
http://arxiv.org/abs/2404.00412v1,"Generating VectorArt from text prompts is a challenging vision task,
requiring diverse yet realistic depictions of the seen as well as unseen
entities. However, existing research has been mostly limited to the generation
of single objects, rather than comprehensive scenes comprising multiple
elements. In response, this work introduces SVGCraft, a novel end-to-end
framework for the creation of vector graphics depicting entire scenes from
textual descriptions. Utilizing a pre-trained LLM for layout generation from
text prompts, this framework introduces a technique for producing masked
latents in specified bounding boxes for accurate object placement. It
introduces a fusion mechanism for integrating attention maps and employs a
diffusion U-Net for coherent composition, speeding up the drawing process. The
resulting SVG is optimized using a pre-trained encoder and LPIPS loss with
opacity modulation to maximize similarity. Additionally, this work explores the
potential of primitive shapes in facilitating canvas completion in constrained
environments. Through both qualitative and quantitative assessments, SVGCraft
is demonstrated to surpass prior works in abstraction, recognizability, and
detail, as evidenced by its performance metrics (CLIP-T: 0.4563, Cosine
Similarity: 0.6342, Confusion: 0.66, Aesthetic: 6.7832). The code will be
available at https://github.com/ayanban011/SVGCraft.",2024-03-30,cs.CV,
http://arxiv.org/abs/2404.01337v1,"Finance-related news such as Bloomberg News, CNN Business and Forbes are
valuable sources of real data for market screening systems. In news, an expert
shares opinions beyond plain technical analyses that include context such as
political, sociological and cultural factors. In the same text, the expert
often discusses the performance of different assets. Some key statements are
mere descriptions of past events while others are predictions. Therefore,
understanding the temporality of the key statements in a text is essential to
separate context information from valuable predictions. We propose a novel
system to detect the temporality of finance-related news at discourse level
that combines Natural Language Processing and Machine Learning techniques, and
exploits sophisticated features such as syntactic and semantic dependencies.
More specifically, we seek to extract the dominant tenses of the main
statements, which may be either explicit or implicit. We have tested our system
on a labelled dataset of finance-related news annotated by researchers with
knowledge in the field. Experimental results reveal a high detection precision
compared to an alternative rule-based baseline approach. Ultimately, this
research contributes to the state-of-the-art of market screening by identifying
predictive knowledge for financial decision making.",2024-03-30,cs.CL,
http://arxiv.org/abs/2404.19159v1,"This study investigates the factors influencing the performance of
multilingual large language models (MLLMs) across diverse languages. We study 6
MLLMs, including masked language models, autoregressive models, and
instruction-tuned LLMs, on the SIB-200 dataset, a topic classification dataset
encompassing 204 languages. Our analysis considers three scenarios: ALL
languages, SEEN languages (present in the model's pretraining data), and UNSEEN
languages (not present or documented in the model's pretraining data in any
meaningful way). We examine the impact of factors such as pretraining data
size, general resource availability, language family, and script type on model
performance. Decision tree analysis reveals that pretraining data size is the
most influential factor for SEEN languages. However, interestingly, script type
and language family are crucial for UNSEEN languages, highlighting the
importance of cross-lingual transfer learning. Notably, model size and
architecture do not significantly alter the most important features identified.
Our findings provide valuable insights into the strengths and limitations of
current MLLMs and hope to guide the development of more effective and equitable
multilingual NLP systems.",2024-04-29,cs.CL,
http://arxiv.org/abs/2404.19154v1,"Relational triple extraction is crucial work for the automatic construction
of knowledge graphs. Existing methods only construct shallow representations
from a token or token pair-level. However, previous works ignore local spatial
dependencies of relational triples, resulting in a weakness of entity pair
boundary detection. To tackle this problem, we propose a novel Region-based
Table Filling method (RTF). We devise a novel region-based tagging scheme and
bi-directional decoding strategy, which regard each relational triple as a
region on the relation-specific table, and identifies triples by determining
two endpoints of each region. We also introduce convolution to construct
region-level table representations from a spatial perspective which makes
triples easier to be captured. In addition, we share partial tagging scores
among different relations to improve learning efficiency of relation
classifier. Experimental results show that our method achieves state-of-the-art
with better generalization capability on three variants of two widely used
benchmark datasets.",2024-04-29,cs.CL,
http://arxiv.org/abs/2404.19148v1,"Effective communication is paramount for the inclusion of deaf individuals in
society. However, persistent communication barriers due to limited Sign
Language (SL) knowledge hinder their full participation. In this context, Sign
Language Recognition (SLR) systems have been developed to improve communication
between signing and non-signing individuals. In particular, there is the
problem of recognizing isolated signs (Isolated Sign Language Recognition,
ISLR) of great relevance in the development of vision-based SL search engines,
learning tools, and translation systems. This work proposes an ISLR approach
where body, hands, and facial landmarks are extracted throughout time and
encoded as 2-D images. These images are processed by a convolutional neural
network, which maps the visual-temporal information into a sign label.
Experimental results demonstrate that our method surpassed the state-of-the-art
in terms of performance metrics on two widely recognized datasets in Brazilian
Sign Language (LIBRAS), the primary focus of this study. In addition to being
more accurate, our method is more time-efficient and easier to train due to its
reliance on a simpler network architecture and solely RGB data as input.",2024-04-29,cs.CV,
http://arxiv.org/abs/2404.19146v1,"Despite widespread applications of knowledge graphs (KGs) in various tasks
such as question answering and intelligent conversational systems, existing KGs
face two major challenges: information granularity and deficiency in
timeliness. These hinder considerably the retrieval and analysis of in-context,
fine-grained, and up-to-date knowledge from KGs, particularly in highly
specialized themes (e.g., specialized scientific research) and rapidly evolving
contexts (e.g., breaking news or disaster tracking). To tackle such challenges,
we propose a theme-specific knowledge graph (i.e., ThemeKG), a KG constructed
from a theme-specific corpus, and design an unsupervised framework for ThemeKG
construction (named TKGCon). The framework takes raw theme-specific corpus and
generates a high-quality KG that includes salient entities and relations under
the theme. Specifically, we start with an entity ontology of the theme from
Wikipedia, based on which we then generate candidate relations by Large
Language Models (LLMs) to construct a relation ontology. To parse the documents
from the theme corpus, we first map the extracted entity pairs to the ontology
and retrieve the candidate relations. Finally, we incorporate the context and
ontology to consolidate the relations for entity pairs. We observe that
directly prompting GPT-4 for theme-specific KG leads to inaccurate entities
(such as ""two main types"" as one entity in the query result) and unclear (such
as ""is"", ""has"") or wrong relations (such as ""have due to"", ""to start""). In
contrast, by constructing the theme-specific KG step by step, our model
outperforms GPT-4 and could consistently identify accurate entities and
relations. Experimental results also show that our framework excels in
evaluations compared with various KG construction baselines.",2024-04-29,cs.AI,
http://arxiv.org/abs/2404.19143v1,"Today, cloud workloads are essentially opaque to the cloud platform.
Typically, the only information the platform receives is the virtual machine
(VM) type and possibly a decoration to the type (e.g., the VM is evictable).
Similarly, workloads receive little to no information from the platform;
generally, workloads might receive telemetry from their VMs or exceptional
signals (e.g., shortly before a VM is evicted). The narrow interface between
workloads and platforms has several drawbacks: (1) a surge in VM types and
decorations in public cloud platforms complicates customer selection; (2)
essential workload characteristics (e.g., low availability requirements, high
latency tolerance) are often unspecified, hindering platform customization for
optimized resource usage and cost savings; and (3) workloads may be unaware of
potential optimizations or lack sufficient time to react to platform events.
  In this paper, we propose a framework, called Workload Intelligence (WI), for
dynamic bi-directional communication between cloud workloads and cloud
platform. Via WI, workloads can programmatically adjust their key
characteristics, requirements, and even dynamically adapt behaviors like VM
priorities. In the other direction, WI allows the platform to programmatically
inform workloads about upcoming events, opportunities for optimization, among
other scenarios. Because of WI, the cloud platform can drastically simplify its
offerings, reduce its costs without fear of violating any workload
requirements, and reduce prices to its customers on average by 48.8%.",2024-04-29,cs.DC,
http://arxiv.org/abs/2404.19141v1,"Recovering intermediate missing GPS points in a sparse trajectory, while
adhering to the constraints of the road network, could offer deep insights into
users' moving behaviors in intelligent transportation systems. Although recent
studies have demonstrated the advantages of achieving map-constrained
trajectory recovery via an end-to-end manner, they still face two significant
challenges. Firstly, existing methods are mostly sequence-based models. It is
extremely hard for them to comprehensively capture the micro-semantics of
individual trajectory, including the information of each GPS point and the
movement between two GPS points. Secondly, existing approaches ignore the
impact of the macro-semantics, i.e., the road conditions and the people's
shared travel preferences reflected by a group of trajectories. To address the
above challenges, we propose a Micro-Macro Spatial-Temporal Graph-based
Encoder-Decoder (MM-STGED). Specifically, we model each trajectory as a graph
to efficiently describe the micro-semantics of trajectory and design a novel
message-passing mechanism to learn trajectory representations. Additionally, we
extract the macro-semantics of trajectories and further incorporate them into a
well-designed graph-based decoder to guide trajectory recovery. Extensive
experiments conducted on sparse trajectories with three different sampling
intervals that are respectively constructed from two real-world trajectory
datasets demonstrate the superiority of our proposed model.",2024-04-29,cs.LG,
http://arxiv.org/abs/2404.19139v1,"Data race, a category of insidious software concurrency bugs, is often
challenging and resource-intensive to detect and debug. Existing dynamic race
detection tools incur significant execution time and memory overhead while
exhibiting high false positives. This paper proposes HMTRace, a novel Armv8.5-A
memory tag extension (MTE) based dynamic data race detection framework,
emphasizing low compute and memory requirements while maintaining high accuracy
and precision. HMTRace supports race detection in userspace OpenMP- and
Pthread-based multi-threaded C applications. HMTRace showcases a combined
f1-score of 0.86 while incurring a mean execution time overhead of 4.01% and
peak memory (RSS) overhead of 54.31%. HMTRace also does not report false
positives, asserting all reported races.",2024-04-29,cs.DC,
http://arxiv.org/abs/2404.19138v1,"We present a decentralized control algorithm for a minimalist robotic swarm
lacking memory, explicit communication, or relative position information, to
encapsulate multiple diffusive target sources in a bounded environment. The
state-of-the-art approaches generally require either local communication or
relative localization to provide guarantees of convergence and safety. We
quantify trade-offs between task, control, and robot parameters for guaranteed
safe convergence to all the sources. Furthermore, our algorithm is robust to
occlusions and noise in the sensor measurements as we demonstrate in
simulation.",2024-04-29,cs.RO,
http://arxiv.org/abs/2404.19136v1,"It was recently conjectured that every component of a discrete rational
dynamical system is a solution to an algebraic difference equation that is
linear in its highest-shift term (a quasi-linear equation). Holonomic sequences
are trivially seen as solutions to such dynamical systems. We prove that the
conjecture holds for holonomic sequences and propose two algorithms for
converting holonomic recurrence equations into such quasi-linear equations. The
two algorithms differ in their efficiency and the minimality of orders in their
outputs.",2024-04-29,cs.SC,
http://arxiv.org/abs/2404.19134v1,"We introduce the first work on benchmarking and evaluating deep clustering
algorithms on large-scale non-categorical 3D CAD models. We first propose a
workflow to allow expert mechanical engineers to efficiently annotate 252,648
carefully sampled pairwise CAD model similarities, from a subset of the ABC
dataset with 22,968 shapes. Using seven baseline deep clustering methods, we
then investigate the fundamental challenges of evaluating clustering methods
for non-categorical data. Based on these challenges, we propose a novel and
viable ensemble-based clustering comparison approach. This work is the first to
directly target the underexplored area of deep clustering algorithms for 3D
shapes, and we believe it will be an important building block to analyze and
utilize the massive 3D shape collections that are starting to appear in deep
geometric computing.",2024-04-29,cs.CV,
http://arxiv.org/abs/2404.19132v1,"We formulate a unifying framework for unsupervised continual learning (UCL),
which disentangles learning objectives that are specific to the present and the
past data, encompassing stability, plasticity, and cross-task consolidation.
The framework reveals that many existing UCL approaches overlook cross-task
consolidation and try to balance plasticity and stability in a shared embedding
space. This results in worse performance due to a lack of within-task data
diversity and reduced effectiveness in learning the current task. Our method,
Osiris, which explicitly optimizes all three objectives on separate embedding
spaces, achieves state-of-the-art performance on all benchmarks, including two
novel benchmarks proposed in this paper featuring semantically structured task
sequences. Compared to standard benchmarks, these two structured benchmarks
more closely resemble visual signals received by humans and animals when
navigating real-world environments. Finally, we show some preliminary evidence
that continual models can benefit from such realistic learning scenarios.",2024-04-29,cs.LG,
http://arxiv.org/abs/2404.19130v1,"Knowledge graphs (KGs), which store an extensive number of relational facts
(head, relation, tail), serve various applications. While many downstream tasks
highly rely on the expressive modeling and predictive embedding of KGs, most of
the current KG representation learning methods, where each entity is embedded
as a vector in the Euclidean space and each relation is embedded as a
transformation, follow an entity ranking protocol. On one hand, such an
embedding design cannot capture many-to-many relations. On the other hand, in
many retrieval cases, the users wish to get an exact set of answers without any
ranking, especially when the results are expected to be precise, e.g., which
genes cause an illness. Such scenarios are commonly referred to as ""set
retrieval"". This work presents a pioneering study on the KG set retrieval
problem. We show that the set retrieval highly depends on expressive modeling
of many-to-many relations, and propose a new KG embedding model SpherE to
address this problem. SpherE is based on rotational embedding methods, but each
entity is embedded as a sphere instead of a vector. While inheriting the high
interpretability of rotational-based models, our SpherE can more expressively
model one-to-many, many-to-one, and many-to-many relations. Through extensive
experiments, we show that our SpherE can well address the set retrieval problem
while still having a good predictive ability to infer missing facts. The code
is available at https://github.com/Violet24K/SpherE.",2024-04-29,cs.IR,
http://arxiv.org/abs/2404.19128v1,"Vision and Language Models (VLMs) continue to demonstrate remarkable
zero-shot (ZS) performance across various tasks. However, many probing studies
have revealed that even the best-performing VLMs struggle to capture aspects of
compositional scene understanding, lacking the ability to properly ground and
localize linguistic phrases in images. Recent VLM advancements include scaling
up both model and dataset sizes, additional training objectives and levels of
supervision, and variations in the model architectures. To characterize the
grounding ability of VLMs, such as phrase grounding, referring expressions
comprehension, and relationship understanding, Pointing Game has been used as
an evaluation metric for datasets with bounding box annotations. In this paper,
we introduce a novel suite of quantitative metrics that utilize GradCAM
activations to rigorously evaluate the grounding capabilities of pre-trained
VLMs like CLIP, BLIP, and ALBEF. These metrics offer an explainable and
quantifiable approach for a more detailed comparison of the zero-shot
capabilities of VLMs and enable measuring models' grounding uncertainty. This
characterization reveals interesting tradeoffs between the size of the model,
the dataset size, and their performance.",2024-04-29,cs.CV,
http://arxiv.org/abs/2404.19126v1,"We propose a system for visual scene analysis and recognition based on
encoding the sparse, latent feature-representation of an image into a
high-dimensional vector that is subsequently factorized to parse scene content.
The sparse feature representation is learned from image statistics via
convolutional sparse coding, while scene parsing is performed by a resonator
network. The integration of sparse coding with the resonator network increases
the capacity of distributed representations and reduces collisions in the
combinatorial search space during factorization. We find that for this problem
the resonator network is capable of fast and accurate vector factorization, and
we develop a confidence-based metric that assists in tracking the convergence
of the resonator network.",2024-04-29,cs.CV,
http://arxiv.org/abs/2404.19124v2,"This technical report describes the design and training of novel speculative
decoding draft models, for accelerating the inference speeds of large language
models in a production environment. By conditioning draft predictions on both
context vectors and sampled tokens, we can train our speculators to efficiently
predict high-quality n-grams, which the base model then accepts or rejects.
This allows us to effectively predict multiple tokens per inference forward
pass, accelerating wall-clock inference speeds of highly optimized base model
implementations by a factor of 2-3x. We explore these initial results and
describe next steps for further improvements.",2024-04-29,cs.CL,
http://arxiv.org/abs/2404.19121v1,"Accurate and timely detection of cyber threats is critical to keeping our
online economy and data safe. A key technique in early detection is the
classification of unusual patterns of network behaviour, often hidden as
low-frequency events within complex time-series packet flows. One of the ways
in which such anomalies can be detected is to analyse the information entropy
of the payload within individual packets, since changes in entropy can often
indicate suspicious activity - such as whether session encryption has been
compromised, or whether a plaintext channel has been co-opted as a covert
channel. To decide whether activity is anomalous we need to compare real-time
entropy values with baseline values, and while the analysis of entropy in
packet data is not particularly new, to the best of our knowledge there are no
published baselines for payload entropy across common network services. We
offer two contributions: 1) We analyse several large packet datasets to
establish baseline payload information entropy values for common network
services, 2) We describe an efficient method for engineering entropy metrics
when performing flow recovery from live or offline packet data, which can be
expressed within feature subsets for subsequent analysis and machine learning
applications.",2024-04-29,cs.CR,
http://arxiv.org/abs/2404.19119v1,"Health literacy is crucial to supporting good health and is a major national
goal. Audio delivery of information is becoming more popular for informing
oneself. In this study, we evaluate the effect of audio enhancements in the
form of information emphasis and pauses with health texts of varying difficulty
and we measure health information comprehension and retention. We produced
audio snippets from difficult and easy text and conducted the study on Amazon
Mechanical Turk (AMT). Our findings suggest that emphasis matters for both
information comprehension and retention. When there is no added pause,
emphasizing significant information can lower the perceived difficulty for
difficult and easy texts. Comprehension is higher (54%) with correctly placed
emphasis for the difficult texts compared to not adding emphasis (50%). Adding
a pause lowers perceived difficulty and can improve retention but adversely
affects information comprehension.",2024-04-29,cs.CL,
http://arxiv.org/abs/2404.19117v1,"This paper tackles the problem of designing proper uplink multiple access
(MA) schemes for coexistence between enhanced mobile broadband+ (eMBB+) users
and massive machine-type communications+ (mMTC+) devices in a terminal-centric
cell-free massive MIMO system. Specifically, the use of a time-frequency
spreading technique for the mMTC+ devices has been proposed. Coupled with the
assumption of imperfect channel knowledge, closed-form bounds of the achievable
(ergodic) rate for the two types of data services are derived. Using suitable
power control mechanisms, we show it is possible to efficiently multiplex eMBB+
and mMTC+ traffic in the same time-frequency resource grid. Numerical
experiments reveal interesting trade-offs in the selection of the spreading
gain and the number of serving access points within the system. Results also
demonstrate that the performance of the mMTC+ devices is slightly affected by
the presence of the eMBB+ users. Overall, our approach can endow good quality
of service to both 6G cornerstones at once.",2024-04-29,cs.IT,
http://arxiv.org/abs/2404.19114v1,"The integration of Internet of Things (IoT) applications in our daily lives
has led to a surge in data traffic, posing significant security challenges. IoT
applications using cloud and edge computing are at higher risk of cyberattacks
because of the expanded attack surface from distributed edge and cloud
services, the vulnerability of IoT devices, and challenges in managing security
across interconnected systems leading to oversights. This led to the rise of
ML-based solutions for intrusion detection systems (IDSs), which have proven
effective in enhancing network security and defending against diverse threats.
However, ML-based IDS in IoT systems encounters challenges, particularly from
noisy, redundant, and irrelevant features in varied IoT datasets, potentially
impacting its performance. Therefore, reducing such features becomes crucial to
enhance system performance and minimize computational costs. This paper focuses
on improving the effectiveness of ML-based IDS at the edge level by introducing
a novel method to find a balanced trade-off between cost and accuracy through
the creation of informative features in a two-tier edge-user IoT environment. A
hybrid Binary Quantum-inspired Artificial Bee Colony and Genetic Programming
algorithm is utilized for this purpose. Three IoT intrusion detection datasets,
namely NSL-KDD, UNSW-NB15, and BoT-IoT, are used for the evaluation of the
proposed approach.",2024-04-29,cs.CR,
http://arxiv.org/abs/2405.00738v1,"Graphics Processing Units (GPUs) have become the leading hardware accelerator
for deep learning applications and are used widely in training and inference of
transformers; transformers have achieved state-of-the-art performance in many
areas of machine learning and are especially used in most modern Large Language
Models (LLMs). However, GPUs require large amounts of energy, which poses
environmental concerns, demands high operational costs, and causes GPUs to be
unsuitable for edge computing. We develop an accelerator for transformers,
namely, Llama 2, an open-source state-of-the-art LLM, using high level
synthesis (HLS) on Field Programmable Gate Arrays (FPGAs). HLS allows us to
rapidly prototype FPGA designs without writing code at the register-transfer
level (RTL). We name our method HLSTransform, and the FPGA designs we
synthesize with HLS achieve up to a 12.75x reduction and 8.25x reduction in
energy used per token on the Xilinx Virtex UltraScale+ VU9P FPGA compared to an
Intel Xeon Broadwell E5-2686 v4 CPU and NVIDIA RTX 3090 GPU respectively, while
increasing inference speeds by up to 2.46x compared to CPU and maintaining
0.53x the speed of an RTX 3090 GPU despite the GPU's 4 times higher base clock
rate. With the lack of existing open-source FPGA accelerators for transformers,
we open-source our code and document our steps for synthesis. We hope this work
will serve as a step in democratizing the use of FPGAs in transformer inference
and inspire research into energy-efficient inference methods as a whole. The
code can be found on https://github.com/HLSTransform/submission.",2024-04-29,cs.AR,
http://arxiv.org/abs/2404.19113v2,"Given the emergence of deep learning, digital pathology has gained popularity
for cancer diagnosis based on histology images. Deep weakly supervised object
localization (WSOL) models can be trained to classify histology images
according to cancer grade and identify regions of interest (ROIs) for
interpretation, using inexpensive global image-class annotations. A WSOL model
initially trained on some labeled source image data can be adapted using
unlabeled target data in cases of significant domain shifts caused by
variations in staining, scanners, and cancer type. In this paper, we focus on
source-free (unsupervised) domain adaptation (SFDA), a challenging problem
where a pre-trained source model is adapted to a new target domain without
using any source domain data for privacy and efficiency reasons. SFDA of WSOL
models raises several challenges in histology, most notably because they are
not intended to adapt for both classification and localization tasks. In this
paper, 4 state-of-the-art SFDA methods, each one representative of a main SFDA
family, are compared for WSOL in terms of classification and localization
accuracy. They are the SFDA-Distribution Estimation, Source HypOthesis
Transfer, Cross-Domain Contrastive Learning, and Adaptively Domain Statistics
Alignment. Experimental results on the challenging Glas (smaller, breast
cancer) and Camelyon16 (larger, colon cancer) histology datasets indicate that
these SFDA methods typically perform poorly for localization after adaptation
when optimized for classification.",2024-04-29,cs.CV,
http://arxiv.org/abs/2404.19110v1,"Head avatars animated by visual signals have gained popularity, particularly
in cross-driving synthesis where the driver differs from the animated
character, a challenging but highly practical approach. The recently presented
MegaPortraits model has demonstrated state-of-the-art results in this domain.
We conduct a deep examination and evaluation of this model, with a particular
focus on its latent space for facial expression descriptors, and uncover
several limitations with its ability to express intense face motions. To
address these limitations, we propose substantial changes in both training
pipeline and model architecture, to introduce our EMOPortraits model, where we:
  Enhance the model's capability to faithfully support intense, asymmetric face
expressions, setting a new state-of-the-art result in the emotion transfer
task, surpassing previous methods in both metrics and quality.
  Incorporate speech-driven mode to our model, achieving top-tier performance
in audio-driven facial animation, making it possible to drive source identity
through diverse modalities, including visual signal, audio, or a blend of both.
  We propose a novel multi-view video dataset featuring a wide range of intense
and asymmetric facial expressions, filling the gap with absence of such data in
existing datasets.",2024-04-29,cs.CV,
http://arxiv.org/abs/2404.19109v2,"Subgraph representation learning is a technique for analyzing local
structures (or shapes) within complex networks. Enabled by recent developments
in scalable Graph Neural Networks (GNNs), this approach encodes relational
information at a subgroup level (multiple connected nodes) rather than at a
node level of abstraction. We posit that certain domain applications, such as
anti-money laundering (AML), are inherently subgraph problems and mainstream
graph techniques have been operating at a suboptimal level of abstraction. This
is due in part to the scarcity of annotated datasets of real-world size and
complexity, as well as the lack of software tools for managing subgraph GNN
workflows at scale. To enable work in fundamental algorithms as well as domain
applications in AML and beyond, we introduce Elliptic2, a large graph dataset
containing 122K labeled subgraphs of Bitcoin clusters within a background graph
consisting of 49M node clusters and 196M edge transactions. The dataset
provides subgraphs known to be linked to illicit activity for learning the set
of ""shapes"" that money laundering exhibits in cryptocurrency and accurately
classifying new criminal activity. Along with the dataset we share our graph
techniques, software tooling, promising early experimental results, and new
domain insights already gleaned from this approach. Taken together, we find
immediate practical value in this approach and the potential for a new standard
in anti-money laundering and forensic analytics in cryptocurrencies and other
financial networks.",2024-04-29,cs.LG,
http://arxiv.org/abs/2404.19108v1,"Star trackers are one of the most accurate celestial sensors used for
absolute attitude determination. The devices detect stars in captured images
and accurately compute their projected centroids on an imaging focal plane with
subpixel precision. Traditional algorithms for star detection and centroiding
often rely on threshold adjustments for star pixel detection and pixel
brightness weighting for centroid computation. However, challenges like high
sensor noise and stray light can compromise algorithm performance. This article
introduces a Convolutional Neural Network (CNN)-based approach for star
detection and centroiding, tailored to address the issues posed by noisy star
tracker images in the presence of stray light and other artifacts. Trained
using simulated star images overlayed with real sensor noise and stray light,
the CNN produces both a binary segmentation map distinguishing star pixels from
the background and a distance map indicating each pixel's proximity to the
nearest star centroid. Leveraging this distance information alongside pixel
coordinates transforms centroid calculations into a set of trilateration
problems solvable via the least squares method. Our method employs efficient
UNet variants for the underlying CNN architectures, and the variants'
performances are evaluated. Comprehensive testing has been undertaken with
synthetic image evaluations, hardware-in-the-loop assessments, and night sky
tests. The tests consistently demonstrated that our method outperforms several
existing algorithms in centroiding accuracy and exhibits superior resilience to
high sensor noise and stray light interference. An additional benefit of our
algorithms is that they can be executed in real-time on low-power edge AI
processors.",2024-04-29,cs.CV,
http://arxiv.org/abs/2405.01592v1,"Text and audio simplification to increase information comprehension are
important in healthcare. With the introduction of ChatGPT, an evaluation of its
simplification performance is needed. We provide a systematic comparison of
human and ChatGPT simplified texts using fourteen metrics indicative of text
difficulty. We briefly introduce our online editor where these simplification
tools, including ChatGPT, are available. We scored twelve corpora using our
metrics: six text, one audio, and five ChatGPT simplified corpora. We then
compare these corpora with texts simplified and verified in a prior user study.
Finally, a medical domain expert evaluated these texts and five, new ChatGPT
simplified versions. We found that simple corpora show higher similarity with
the human simplified texts. ChatGPT simplification moves metrics in the right
direction. The medical domain expert evaluation showed a preference for the
ChatGPT style, but the text itself was rated lower for content retention.",2024-04-29,cs.CL,
http://arxiv.org/abs/2404.19100v1,"This paper investigates the relationships between hyperparameters of machine
learning and fairness. Data-driven solutions are increasingly used in critical
socio-technical applications where ensuring fairness is important. Rather than
explicitly encoding decision logic via control and data structures, the ML
developers provide input data, perform some pre-processing, choose ML
algorithms, and tune hyperparameters (HPs) to infer a program that encodes the
decision logic. Prior works report that the selection of HPs can significantly
influence fairness. However, tuning HPs to find an ideal trade-off between
accuracy, precision, and fairness has remained an expensive and tedious task.
Can we predict fairness of HP configuration for a given dataset? Are the
predictions robust to distribution shifts?
  We focus on group fairness notions and investigate the HP space of 5 training
algorithms. We first find that tree regressors and XGBoots significantly
outperformed deep neural networks and support vector machines in accurately
predicting the fairness of HPs. When predicting the fairness of ML
hyperparameters under temporal distribution shift, the tree regressors
outperforms the other algorithms with reasonable accuracy. However, the
precision depends on the ML training algorithm, dataset, and protected
attributes. For example, the tree regressor model was robust for training data
shift from 2014 to 2018 on logistic regression and discriminant analysis HPs
with sex as the protected attribute; but not for race and other training
algorithms. Our method provides a sound framework to efficiently perform
fine-tuning of ML training algorithms and understand the relationships between
HPs and fairness.",2024-04-29,cs.SE,
http://arxiv.org/abs/2404.19097v2,"Data visualizations help extract insights from datasets, but reaching these
insights requires decomposing high level goals into low-level analytic tasks
that can be complex due to varying degrees of data literacy and visualization
experience. Recent advancements in large language models (LLMs) have shown
promise for lowering barriers for users to achieve tasks such as writing code
and may likewise facilitate visualization insight. Scalable Vector Graphics
(SVG), a text-based image format common in data visualizations, matches well
with the text sequence processing of transformer-based LLMs. In this paper, we
explore the capability of LLMs to perform 10 low-level visual analytic tasks
defined by Amar, Eagan, and Stasko directly on SVG-based visualizations. Using
zero-shot prompts, we instruct the models to provide responses or modify the
SVG code based on given visualizations. Our findings demonstrate that LLMs can
effectively modify existing SVG visualizations for some tasks like Cluster but
perform poorly on tasks requiring mathematical operations like Compute Derived
Value. We also discovered that LLM performance can vary based on factors such
as the number of data points, the presence of value labels, and the chart type.
Our findings contribute to gauging the general capabilities of LLMs and
highlight the need for further exploration and development to fully harness
their potential in supporting visual analytic tasks.",2024-04-29,cs.HC,
http://arxiv.org/abs/2404.19095v1,"We create an innovative mixed reality-first social recommendation model,
utilizing features uniquely collected through mixed reality (MR) systems to
promote social interaction, such as gaze recognition, proximity, noise level,
congestion level, and conversational intensity. We further extend these models
to include right-time features to deliver timely notifications. We measure
performance metrics across various models by creating a new intersection of
user features, MR features, and right-time features. We create four model types
trained on different combinations of the feature classes, where we compare the
baseline model trained on the class of user features against the models trained
on MR features, right-time features, and a combination of all of the feature
classes. Due to limitations in data collection and cost, we observe performance
degradation in the right-time, mixed reality, and combination models. Despite
these challenges, we introduce optimizations to improve accuracy across all
models by over 14 percentage points, where the best performing model achieved
24% greater accuracy.",2024-04-29,cs.HC,
http://arxiv.org/abs/2404.19094v1,"Symbolic Regression (SR) is a task which aims to extract the mathematical
expression underlying a set of empirical observations. Transformer-based
methods trained on SR datasets detain the current state-of-the-art in this
task, while the application of Large Language Models (LLMs) to SR remains
unexplored. This work investigates the integration of pre-trained LLMs into the
SR pipeline, utilizing an approach that iteratively refines a functional form
based on the prediction error it achieves on the observation set, until it
reaches convergence. Our method leverages LLMs to propose an initial set of
possible functions based on the observations, exploiting their strong
pre-training prior. These functions are then iteratively refined by the model
itself and by an external optimizer for their coefficients. The process is
repeated until the results are satisfactory. We then analyze Vision-Language
Models in this context, exploring the inclusion of plots as visual inputs to
aid the optimization process. Our findings reveal that LLMs are able to
successfully recover good symbolic equations that fit the given data,
outperforming SR baselines based on Genetic Programming, with the addition of
images in the input showing promising results for the most complex benchmarks.",2024-04-29,cs.CL,
http://arxiv.org/abs/2404.19093v1,"This paper explores the effectiveness of using large language models (LLMs)
for personalized movie recommendations from users' perspectives in an online
field experiment. Our study involves a combination of between-subject prompt
and historic consumption assessments, along with within-subject recommendation
scenario evaluations. By examining conversation and survey response data from
160 active users, we find that LLMs offer strong recommendation explainability
but lack overall personalization, diversity, and user trust. Our results also
indicate that different personalized prompting techniques do not significantly
affect user-perceived recommendation quality, but the number of movies a user
has watched plays a more significant role. Furthermore, LLMs show a greater
ability to recommend lesser-known or niche movies. Through qualitative
analysis, we identify key conversational patterns linked to positive and
negative user interaction experiences and conclude that providing personal
context and examples is crucial for obtaining high-quality recommendations from
LLMs.",2024-04-29,cs.IR,
http://arxiv.org/abs/2404.19090v1,"Ambient Internet of Things networks use low-cost, low-power backscatter tags
in various industry applications. By exploiting those tags, we introduce the
integrated sensing and backscatter communication (ISABC) system, featuring
multiple backscatter tags, a user (reader), and a full-duplex base station (BS)
that integrates sensing and (backscatter) communications. The BS undertakes
dual roles of detecting backscatter tags and communicating with the user,
leveraging the same temporal and frequency resources. The tag-reflected BS
signals offer data to the user and enable the BS to sense the environment
simultaneously. We derive both user and tag communication rates and the sensing
rate of the BS. We jointly optimize the transmit/received beamformers and tag
reflection coefficients to minimize the total BS power. To solve this problem,
we employ the alternating optimization technique. We offer a closed-form
solution for the received beamformers while utilizing semi-definite relaxation
and slack-optimization for transmit beamformers and power reflection
coefficients, respectively. For example, with ten transmit/reception antennas
at the BS, ISABC delivers a 75% sum communication and sensing rates gain over a
traditional backscatter while requiring a 3.4% increase in transmit power.
Furthermore, ISABC with active tags only requires a 0.24% increase in transmit
power over conventional integrated sensing and communication.",2024-04-29,cs.IT,
http://arxiv.org/abs/2404.19087v1,"Existing Advanced Driver Assistance Systems primarily focus on the vehicle
directly ahead, often overlooking potential risks from following vehicles. This
oversight can lead to ineffective handling of high risk situations, such as
high speed, closely spaced, multi vehicle scenarios where emergency braking by
one vehicle might trigger a pile up collision. To overcome these limitations,
this study introduces a novel deep reinforcement learning based algorithm for
longitudinal control and collision avoidance. This proposed algorithm
effectively considers the behavior of both leading and following vehicles. Its
implementation in simulated high risk scenarios, which involve emergency
braking in dense traffic where traditional systems typically fail, has
demonstrated the algorithm ability to prevent potential pile up collisions,
including those involving heavy duty vehicles.",2024-04-29,cs.RO,
http://arxiv.org/abs/2404.19077v1,"The functional replication and actuation of complex structures inspired by
nature is a longstanding goal for humanity. Creating such complex structures
combining soft and rigid features and actuating them with artificial muscles
would further our understanding of natural kinematic structures. We printed a
biomimetic hand in a single print process comprised of a rigid skeleton, soft
joint capsules, tendons, and printed touch sensors. We showed it's actuation
using electric motors. In this work, we expand on this work by adding a forearm
that is also closely modeled after the human anatomy and replacing the hand's
motors with 22 independently controlled pneumatic artificial muscles (PAMs).
Our thin, high-strain (up to 30.1%) PAMs match the performance of
state-of-the-art artificial muscles at a lower cost. The system showcases
human-like dexterity with independent finger movements, demonstrating
successful grasping of various objects, ranging from a small, lightweight coin
to a large can of 272g in weight. The performance evaluation, based on
fingertip and grasping forces along with finger joint range of motion,
highlights the system's potential.",2024-04-29,cs.RO,
http://arxiv.org/abs/2404.19076v1,"This study examines the extent to which U.S. federal agencies responded to
and implemented the principles outlined in the White House's October 2022
""Blueprint for an AI Bill of Rights."" The Blueprint provided a framework for
the ethical governance of artificial intelligence systems, organized around
five core principles: safety and effectiveness, protection against algorithmic
discrimination, data privacy, notice and explanation about AI systems, and
human alternatives and fallback.
  Through an analysis of publicly available records across 15 federal
departments, the authors found limited evidence that the Blueprint directly
influenced agency actions after its release. Only five departments explicitly
mentioned the Blueprint, while 12 took steps aligned with one or more of its
principles. However, much of this work appeared to have precedents predating
the Blueprint or motivations disconnected from it, such as compliance with
prior executive orders on trustworthy AI. Departments' activities often
emphasized priorities like safety, accountability and transparency that
overlapped with Blueprint principles, but did not necessarily stem from it.
  The authors conclude that the non-binding Blueprint seems to have had minimal
impact on shaping the U.S. government's approach to ethical AI governance in
its first year. Factors like public concerns after high-profile AI releases and
obligations to follow direct executive orders likely carried more influence
over federal agencies. More rigorous study would be needed to definitively
assess the Blueprint's effects within the federal bureaucracy and broader
society.",2024-04-29,cs.CY,
http://arxiv.org/abs/2404.19071v1,"With the rapid proliferation of artificial intelligence, there is growing
concern over its potential to exacerbate existing biases and societal
disparities and introduce novel ones. This issue has prompted widespread
attention from academia, policymakers, industry, and civil society. While
evidence suggests that integrating human perspectives can mitigate bias-related
issues in AI systems, it also introduces challenges associated with cognitive
biases inherent in human decision-making. Our research focuses on reviewing
existing methodologies and ongoing investigations aimed at understanding
annotation attributes that contribute to bias.",2024-04-29,cs.HC,
http://arxiv.org/abs/2404.19070v1,"Multi-drone cooperative transport (CT) problem has been widely studied in the
literature. However, limited work exists on control of such systems in the
presence of time-varying uncertainties, such as the time-varying center of
gravity (CG). This paper presents a leader-follower approach for the control of
a multi-drone CT system with time-varying CG. The leader uses a traditional
Proportional-Integral-Derivative (PID) controller, and in contrast, the
follower uses a deep reinforcement learning (RL) controller using only local
information and minimal leader information. Extensive simulation results are
presented, showing the effectiveness of the proposed method over a previously
developed adaptive controller and for variations in the mass of the objects
being transported and CG speeds. Preliminary experimental work also
demonstrates ball balance (depicting moving CG) on a stick/rod lifted by two
Crazyflie drones cooperatively.",2024-04-29,cs.RO,
http://arxiv.org/abs/2404.19066v1,"This research introduces an innovative method for Traffic Sign Recognition
(TSR) by leveraging deep learning techniques, with a particular emphasis on
Vision Transformers. TSR holds a vital role in advancing driver assistance
systems and autonomous vehicles. Traditional TSR approaches, reliant on manual
feature extraction, have proven to be labor-intensive and costly. Moreover,
methods based on shape and color have inherent limitations, including
susceptibility to various factors and changes in lighting conditions. This
study explores three variants of Vision Transformers (PVT, TNT, LNL) and six
convolutional neural networks (AlexNet, ResNet, VGG16, MobileNet, EfficientNet,
GoogleNet) as baseline models. To address the shortcomings of traditional
methods, a novel pyramid EATFormer backbone is proposed, amalgamating
Evolutionary Algorithms (EAs) with the Transformer architecture. The introduced
EA-based Transformer block captures multi-scale, interactive, and individual
information through its components: Feed-Forward Network, Global and Local
Interaction, and Multi-Scale Region Aggregation modules. Furthermore, a
Modulated Deformable MSA module is introduced to dynamically model irregular
locations. Experimental evaluations on the GTSRB and BelgiumTS datasets
demonstrate the efficacy of the proposed approach in enhancing both prediction
speed and accuracy. This study concludes that Vision Transformers hold
significant promise in traffic sign classification and contributes a fresh
algorithmic framework for TSR. These findings set the stage for the development
of precise and dependable TSR algorithms, benefiting driver assistance systems
and autonomous vehicles.",2024-04-29,cs.CV,
http://arxiv.org/abs/2404.19065v1,"Recent research on instructable agents has used memory-augmented Large
Language Models (LLMs) as task planners, a technique that retrieves
language-program examples relevant to the input instruction and uses them as
in-context examples in the LLM prompt to improve the performance of the LLM in
inferring the correct action and task plans. In this technical report, we
extend the capabilities of HELPER, by expanding its memory with a wider array
of examples and prompts, and by integrating additional APIs for asking
questions. This simple expansion of HELPER into a shared memory enables the
agent to work across the domains of executing plans from dialogue, natural
language instruction following, active question asking, and commonsense room
reorganization. We evaluate the agent on four diverse interactive
visual-language embodied agent benchmarks: ALFRED, TEACh, DialFRED, and the
Tidy Task. HELPER-X achieves few-shot, state-of-the-art performance across
these benchmarks using a single agent, without requiring in-domain training,
and remains competitive with agents that have undergone in-domain training.",2024-04-29,cs.AI,
http://arxiv.org/abs/2404.19064v1,"Multiple sequence alignment (MSA) is a fundamental algorithm in
bioinformatics. In a situation when the alignment might need to be protected
while revealing the other information such the input sequences and the
alignment score, zero knowledge proof can be used. In this paper, a validator
checks the consistency between the input sequence and the alignment, and
between the alignment and the alignment score. The validator is written in
Circom language which will be compile into a circuit. Using a zero knowledge
prove system called zkSNARK, a cryptographic proof is generates for the circuit
and its input. This proof demonstrates that all inputs are consistent without
revealing the actual alignment.",2024-04-29,cs.CR,
http://arxiv.org/abs/2404.19063v1,"The SuperCLUE-Fin (SC-Fin) benchmark is a pioneering evaluation framework
tailored for Chinese-native financial large language models (FLMs). It assesses
FLMs across six financial application domains and twenty-five specialized
tasks, encompassing theoretical knowledge and practical applications such as
compliance, risk management, and investment analysis. Using multi-turn,
open-ended conversations that mimic real-life scenarios, SC-Fin measures models
on a range of criteria, including accurate financial understanding, logical
reasoning, clarity, computational efficiency, business acumen, risk perception,
and compliance with Chinese regulations.
  In a rigorous evaluation involving over a thousand questions, SC-Fin
identifies a performance hierarchy where domestic models like GLM-4 and
MoonShot-v1-128k outperform others with an A-grade, highlighting the potential
for further development in transforming theoretical knowledge into pragmatic
financial solutions. This benchmark serves as a critical tool for refining FLMs
in the Chinese context, directing improvements in financial knowledge
databases, standardizing financial interpretations, and promoting models that
prioritize compliance, risk management, and secure practices.
  We create a contextually relevant and comprehensive benchmark that drives the
development of AI in the Chinese financial sector. SC-Fin facilitates the
advancement and responsible deployment of FLMs, offering valuable insights for
enhancing model performance and usability for both individual and institutional
users in the Chinese market..~\footnote{Our benchmark can be found at
\url{https://www.CLUEbenchmarks.com}}.",2024-04-29,cs.CL,
http://arxiv.org/abs/2404.19060v1,"Given a maze populated with different objects, one may task a robot with a
sequential goal completion task, e.g. 1) pick up a key then 2) unlock the door
then 3) unlock the treasure chest. A typical machine learning (ML) solution
would involve a monolithically trained artificial neural network (ANN).
However, if the sequence of goals or the goals themselves change, then the ANN
must be significantly (or, at worst, completely) retrained. Instead of a
monolithic ANN, a modular ML component would be 1) independently optimizable
(task-agnostic) and 2) arbitrarily reconfigurable with other ML modules. This
work describes a modular, hierarchical ML framework by integrating two emerging
ML techniques: 1) cognitive map learners (CML) and 2) hyperdimensional
computing (HDC). A CML is a collection of three single layer ANNs (matrices)
collaboratively trained to learn the topology of an abstract graph. Here, two
CMLs were constructed, one describing locations on in 2D physical space and the
other the relative distribution of objects found in this space. Each CML node
states was encoded as a high-dimensional vector to utilize HDC, an ML algebra,
for symbolic reasoning over these high-dimensional symbol vectors. In this way,
each sub-goal above was described by algebraic equations of CML node states.
Multiple, independently trained CMLs were subsequently assembled together to
navigate a maze to solve a sequential goal task. Critically, changes to these
goals required only localized changes in the CML-HDC architecture, as opposed
to a global ANN retraining scheme. This framework therefore enabled a more
traditional engineering approach to ML, akin to digital logic design.",2024-04-29,cs.NE,
http://arxiv.org/abs/2404.19055v1,"While language models (LMs) offer significant capability in zero-shot
reasoning tasks across a wide range of domains, they do not perform
satisfactorily in problems which requires multi-step reasoning. Previous
approaches to mitigate this involves breaking a larger, multi-step task into
sub-tasks and asking the language model to generate proposals (""thoughts"") for
each sub-task and using exhaustive planning approaches such as DFS to compose a
solution. In this work, we leverage this idea to introduce two new
contributions: first, we formalize a planning-based approach to perform
multi-step problem solving with LMs via Partially Observable Markov Decision
Processes (POMDPs), with the LM's own reflections about the value of a state
used as a search heuristic; second, leveraging the online POMDP solver POMCP,
we demonstrate a superior success rate of 89.4% on the Game of 24 task as
compared to existing approaches while also offering better anytime performance
characteristics than fixed tree-search which is used previously. Taken
together, these contributions allow modern LMs to decompose and solve
larger-scale reasoning tasks more effectively.",2024-04-29,cs.CL,
http://arxiv.org/abs/2404.19052v1,"Measuring similarity between RDF graphs is essential for various
applications, including knowledge discovery, semantic web analysis, and
recommender systems. However, traditional similarity measures often treat all
properties equally, potentially overlooking the varying importance of different
properties in different contexts. Consequently, exploring weighted property
approaches for RDF graph similarity measure presents an intriguing avenue for
investigation. Therefore, in this paper, we propose a weighted property
approach for RDF graph similarity measure to address this limitation. Our
approach incorporates the relative importance of properties into the similarity
calculation, enabling a more nuanced and context-aware measures of similarity.
We evaluate our approach through a comprehensive experimental study on an RDF
graph dataset in the vehicle domain. Our results demonstrate that the proposed
approach achieves promising accuracy and effectively reflects the perceived
similarity between RDF graphs.",2024-04-29,cs.DB,
http://arxiv.org/abs/2404.19051v1,"Cognitive map learners (CML) are a collection of separate yet collaboratively
trained single-layer artificial neural networks (matrices), which navigate an
abstract graph by learning internal representations of the node states, edge
actions, and edge action availabilities. A consequence of this atypical
segregation of information is that the CML performs near-optimal path planning
between any two graph node states. However, the CML does not learn when or why
to transition from one node to another. This work created CMLs with node states
expressed as high dimensional vectors consistent with hyperdimensional
computing (HDC), a form of symbolic machine learning (ML). This work evaluated
HDC-based CMLs as ML modules, capable of receiving external inputs and
computing output responses which are semantically meaningful for other
HDC-based modules. Several CMLs were prepared independently then repurposed to
solve the Tower of Hanoi puzzle without retraining these CMLs and without
explicit reference to their respective graph topologies. This work suggests a
template for building levels of biologically plausible cognitive abstraction
and orchestration.",2024-04-29,cs.NE,
http://arxiv.org/abs/2404.19048v2,"Large Language Models (LLMs) have significantly advanced natural language
processing (NLP) tasks but also pose ethical and societal risks due to their
propensity to generate harmful content. To address this, various approaches
have been developed to safeguard LLMs from producing unsafe content. However,
existing methods have limitations, including the need for training specific
control models and proactive intervention during text generation, that lead to
quality degradation and increased computational overhead. To mitigate those
limitations, we propose LLMSafeGuard, a lightweight framework to safeguard LLM
text generation in real-time. LLMSafeGuard integrates an external validator
into the beam search algorithm during decoding, rejecting candidates that
violate safety constraints while allowing valid ones to proceed. We introduce a
similarity based validation approach, simplifying constraint introduction and
eliminating the need for control model training. Additionally, LLMSafeGuard
employs a context-wise timing selection strategy, intervening LLMs only when
necessary. We evaluate LLMSafeGuard on two tasks, detoxification and copyright
safeguarding, and demonstrate its superior performance over SOTA baselines. For
instance, LLMSafeGuard reduces the average toxic score of. LLM output by 29.7%
compared to the best baseline meanwhile preserving similar linguistic quality
as natural output in detoxification task. Similarly, in the copyright task,
LLMSafeGuard decreases the Longest Common Subsequence (LCS) by 56.2% compared
to baselines. Moreover, our context-wise timing selection strategy reduces
inference time by at least 24% meanwhile maintaining comparable effectiveness
as validating each time step. LLMSafeGuard also offers tunable parameters to
balance its effectiveness and efficiency.",2024-04-29,cs.CL,
http://arxiv.org/abs/2404.19045v1,"This paper presents field results and lessons learned from the deployment of
aerial robots inside ship ballast tanks. Vessel tanks including ballast tanks
and cargo holds present dark, dusty environments having simultaneously very
narrow openings and wide open spaces that create several challenges for
autonomous navigation and inspection operations. We present a system for vessel
tank inspection using an aerial robot along with its autonomy modules. We show
the results of autonomous exploration and visual inspection in 3 ships spanning
across 7 distinct types of sections of the ballast tanks. Additionally, we
comment on the lessons learned from the field and possible directions for
future work. Finally, we release a dataset consisting of the data from these
missions along with data collected with a handheld sensor stick.",2024-04-29,cs.RO,
http://arxiv.org/abs/2404.19043v1,"Flood inundation mapping is a critical task for responding to the increasing
risk of flooding linked to global warming. Significant advancements of deep
learning in recent years have triggered its extensive applications, including
flood inundation mapping. To cope with the time-consuming and labor-intensive
data labeling process in supervised learning, deep active learning strategies
are one of the feasible approaches. However, there remains limited exploration
into the interpretability of how deep active learning strategies operate, with
a specific focus on flood inundation mapping in the field of remote sensing. In
this study, we introduce a novel framework of Interpretable Deep Active
Learning for Flood inundation Mapping (IDAL-FIM), specifically in terms of
class ambiguity of multi-spectral satellite images. In the experiments, we
utilize Sen1Floods11 dataset, and adopt U-Net with MC-dropout. In addition, we
employ five acquisition functions, which are the random, K-means, BALD,
entropy, and margin acquisition functions. Based on the experimental results,
we demonstrate that two proposed class ambiguity indices are effective
variables to interpret the deep active learning by establishing statistically
significant correlation with the predictive uncertainty of the deep learning
model at the tile level. Then, we illustrate the behaviors of deep active
learning through visualizing two-dimensional density plots and providing
interpretations regarding the operation of deep active learning, in flood
inundation mapping.",2024-04-29,cs.CV,
http://arxiv.org/abs/2404.19038v1,"The talking head generation recently attracted considerable attention due to
its widespread application prospects, especially for digital avatars and 3D
animation design. Inspired by this practical demand, several works explored
Neural Radiance Fields (NeRF) to synthesize the talking heads. However, these
methods based on NeRF face two challenges: (1) Difficulty in generating
style-controllable talking heads. (2) Displacement artifacts around the neck in
rendered images. To overcome these two challenges, we propose a novel
generative paradigm \textit{Embedded Representation Learning Network} (ERLNet)
with two learning stages. First, the \textit{ audio-driven FLAME} (ADF) module
is constructed to produce facial expression and head pose sequences
synchronized with content audio and style video. Second, given the sequence
deduced by the ADF, one novel \textit{dual-branch fusion NeRF} (DBF-NeRF)
explores these contents to render the final images. Extensive empirical studies
demonstrate that the collaboration of these two stages effectively facilitates
our method to render a more realistic talking head than the existing
algorithms.",2024-04-29,cs.CV,
http://arxiv.org/abs/2404.19031v1,"Document understanding models have recently demonstrated remarkable
performance by leveraging extensive collections of user documents. However,
since documents often contain large amounts of personal data, their usage can
pose a threat to user privacy and weaken the bonds of trust between humans and
AI services. In response to these concerns, legislation advocating ``the right
to be forgotten"" has recently been proposed, allowing users to request the
removal of private information from computer systems and neural network models.
A novel approach, known as machine unlearning, has emerged to make AI models
forget about a particular class of data. In our research, we explore machine
unlearning for document classification problems, representing, to the best of
our knowledge, the first investigation into this area. Specifically, we
consider a realistic scenario where a remote server houses a well-trained model
and possesses only a small portion of training data. This setup is designed for
efficient forgetting manipulation. This work represents a pioneering step
towards the development of machine unlearning methods aimed at addressing
privacy concerns in document analysis applications. Our code is publicly
available at
\url{https://github.com/leitro/MachineUnlearning-DocClassification}.",2024-04-29,cs.CV,
http://arxiv.org/abs/2404.19026v1,"Creating high-fidelity head avatars from multi-view videos is a core issue
for many AR/VR applications. However, existing methods usually struggle to
obtain high-quality renderings for all different head components simultaneously
since they use one single representation to model components with drastically
different characteristics (e.g., skin vs. hair). In this paper, we propose a
Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components
with more suitable representations. Specifically, we select an enhanced FLAME
mesh as our facial representation and predict a UV displacement map to provide
per-vertex offsets for improved personalized geometric details. To achieve
photorealistic renderings, we obtain facial colors using deferred neural
rendering and disentangle neural textures into three meaningful parts. For hair
modeling, we first build a static canonical hair using 3D Gaussian Splatting. A
rigid transformation and an MLP-based deformation field are further applied to
handle complex dynamic expressions. Combined with our occlusion-aware blending,
MeGA generates higher-fidelity renderings for the whole head and naturally
supports more downstream tasks. Experiments on the NeRSemble dataset
demonstrate the effectiveness of our designs, outperforming previous
state-of-the-art methods and supporting various editing functionalities,
including hairstyle alteration and texture editing.",2024-04-29,cs.CV,
http://arxiv.org/abs/2405.19576v1,"Digital engineering practices offer significant yet underutilized potential
for improving information assurance and system lifecycle management. This paper
examines how capabilities like model-based engineering, digital threads, and
integrated product lifecycles can address gaps in prevailing frameworks. A
reference model demonstrates applying digital engineering techniques to a
reference information system, exhibiting enhanced traceability, risk
visibility, accuracy, and integration. The model links strategic needs to
requirements and architecture while reusing authoritative elements across
views. Analysis of the model shows digital engineering closes gaps in
compliance, monitoring, change management, and risk assessment. Findings
indicate purposeful digital engineering adoption could transform cybersecurity,
operations, service delivery, and system governance through comprehensive
digital system representations. This research provides a foundation for
maturing application of digital engineering for information systems as
organizations modernize infrastructure and pursue digital transformation.",2024-05-29,cs.CR,
http://arxiv.org/abs/2405.19575v1,"Aspect-based Sentiment Analysis (ABSA) is crucial for understanding sentiment
nuances in text, especially across diverse languages and cultures. This paper
introduces a novel Deep Convolutional Neural Network (CNN)-based model tailored
for aspect and polarity classification in Hausa movie reviews, an
underrepresented language in sentiment analysis research. A comprehensive Hausa
ABSA dataset is created, filling a significant gap in resource availability.
The dataset, preprocessed using sci-kit-learn for TF-IDF transformation,
includes manually annotated aspect-level feature ontology words and sentiment
polarity assignments. The proposed model combines CNNs with attention
mechanisms for aspect-word prediction, leveraging contextual information and
sentiment polarities. With 91% accuracy on aspect term extraction and 92% on
sentiment polarity classification, the model outperforms traditional machine
models, offering insights into specific aspects and sentiments. This study
advances ABSA research, particularly in underrepresented languages, with
implications for cross-cultural linguistic research.",2024-05-29,cs.CL,
http://arxiv.org/abs/2405.19572v1,"Recently, various methods have been proposed to solve Image Restoration (IR)
tasks using a pre-trained diffusion model leading to state-of-the-art
performance. However, most of these methods assume that the degradation
operator in the IR task is completely known. Furthermore, a common
characteristic among these approaches is that they alter the diffusion sampling
process in order to satisfy the consistency with the degraded input image. This
choice has recently been shown to be sub-optimal and to cause the restored
image to deviate from the data manifold. To address these issues, we propose
Blind Image Restoration via fast Diffusion inversion (BIRD) a blind IR method
that jointly optimizes for the degradation model parameters and the restored
image. To ensure that the restored images lie onto the data manifold, we
propose a novel sampling technique on a pre-trained diffusion model. A key idea
in our method is not to modify the reverse sampling, i.e., not to alter all the
intermediate latents, once an initial noise is sampled. This is ultimately
equivalent to casting the IR task as an optimization problem in the space of
the input noise. Moreover, to mitigate the computational cost associated with
inverting a fully unrolled diffusion model, we leverage the inherent capability
of these models to skip ahead in the forward diffusion process using large time
steps. We experimentally validate BIRD on several image restoration tasks and
show that it achieves state of the art performance on all of them. Our code is
available at https://github.com/hamadichihaoui/BIRD.",2024-05-29,cs.CV,
http://arxiv.org/abs/2405.19570v1,"Min-max problems are important in multi-agent sequential decision-making
because they improve the performance of the worst-performing agent in the
network. However, solving the multi-agent min-max problem is challenging. We
propose a modular, distributed, online planning-based algorithm that is able to
approximate the solution of the min-max objective in networked Markov games,
assuming that the agents communicate within a network topology and the
transition and reward functions are neighborhood-dependent. This set-up is
encountered in the multi-robot setting. Our method consists of two phases at
every planning step. In the first phase, each agent obtains sample returns
based on its local reward function, by performing online planning. Using the
samples from online planning, each agent constructs a concave approximation of
its underlying local return as a function of only the action of its
neighborhood at the next planning step. In the second phase, the agents deploy
a distributed optimization framework that converges to the optimal immediate
next action for each agent, based on the function approximations of the first
phase. We demonstrate our algorithm's performance through formation control
simulations.",2024-05-29,cs.MA,
http://arxiv.org/abs/2405.19569v1,"Describing a scene in terms of primitives -- geometrically simple shapes that
offer a parsimonious but accurate abstraction of structure -- is an established
vision problem. This is a good model of a difficult fitting problem: different
scenes require different numbers of primitives and primitives interact
strongly, but any proposed solution can be evaluated at inference time. The
state of the art method involves a learned regression procedure to predict a
start point consisting of a fixed number of primitives, followed by a descent
method to refine the geometry and remove redundant primitives. Methods are
evaluated by accuracy in depth and normal prediction and in scene segmentation.
This paper shows that very significant improvements in accuracy can be obtained
by (a) incorporating a small number of negative primitives and (b) ensembling
over a number of different regression procedures. Ensembling is by refining
each predicted start point, then choosing the best by fitting loss. Extensive
experiments on a standard dataset confirm that negative primitives are useful
in a large fraction of images, and that our refine-then-choose strategy
outperforms choose-then-refine, confirming that the fitting problem is very
difficult.",2024-05-29,cs.CV,
http://arxiv.org/abs/2405.19568v1,"The goal of incremental Few-shot Semantic Segmentation (iFSS) is to extend
pre-trained segmentation models to new classes via few annotated images without
access to old training data. During incrementally learning novel classes, the
data distribution of old classes will be destroyed, leading to catastrophic
forgetting. Meanwhile, the novel classes have only few samples, making models
impossible to learn the satisfying representations of novel classes. For the
iFSS problem, we propose a network called OINet, i.e., the background embedding
space \textbf{O}rganization and prototype \textbf{I}nherit Network.
Specifically, when training base classes, OINet uses multiple classification
heads for the background and sets multiple sub-class prototypes to reserve
embedding space for the latent novel classes. During incrementally learning
novel classes, we propose a strategy to select the sub-class prototypes that
best match the current learning novel classes and make the novel classes
inherit the selected prototypes' embedding space. This operation allows the
novel classes to be registered in the embedding space using few samples without
affecting the distribution of the base classes. Results on Pascal-VOC and COCO
show that OINet achieves a new state of the art.",2024-05-29,cs.CV,
http://arxiv.org/abs/2405.19567v1,"Vision-Language Models (VLM) can support clinicians by analyzing medical
images and engaging in natural language interactions to assist in diagnostic
and treatment tasks. However, VLMs often exhibit ""hallucinogenic"" behavior,
generating textual outputs not grounded in contextual multimodal information.
This challenge is particularly pronounced in the medical domain, where we do
not only require VLM outputs to be accurate in single interactions but also to
be consistent with clinical reasoning and diagnostic pathways throughout
multi-turn conversations. For this purpose, we propose a new alignment
algorithm that uses symbolic representations of clinical reasoning to ground
VLMs in medical knowledge. These representations are utilized to (i) generate
GPT-4-guided visual instruction tuning data at scale, simulating clinician-VLM
conversations with demonstrations of clinical reasoning, and (ii) create an
automatic reward function that evaluates the clinical validity of VLM
generations throughout clinician-VLM interactions. Our algorithm eliminates the
need for human involvement in training data generation or reward model
construction, reducing costs compared to standard reinforcement learning with
human feedback (RLHF). We apply our alignment algorithm to develop Dr-LLaVA, a
conversational VLM finetuned for analyzing bone marrow pathology slides,
demonstrating strong performance in multi-turn medical conversations.",2024-05-29,cs.AI,
http://arxiv.org/abs/2405.19563v1,"Misinformation regarding climate change is a key roadblock in addressing one
of the most serious threats to humanity. This paper investigates factual
accuracy in large language models (LLMs) regarding climate information. Using
true/false labeled Q&A data for fine-tuning and evaluating LLMs on
climate-related claims, we compare open-source models, assessing their ability
to generate truthful responses to climate change questions. We investigate the
detectability of models intentionally poisoned with false climate information,
finding that such poisoning may not affect the accuracy of a model's responses
in other domains. Furthermore, we compare the effectiveness of unlearning
algorithms, fine-tuning, and Retrieval-Augmented Generation (RAG) for factually
grounding LLMs on climate change topics. Our evaluation reveals that unlearning
algorithms can be effective for nuanced conceptual claims, despite previous
findings suggesting their inefficacy in privacy contexts. These insights aim to
guide the development of more factually reliable LLMs and highlight the need
for additional work to secure LLMs against misinformation attacks.",2024-05-29,cs.CL,
http://arxiv.org/abs/2405.19562v1,"Feature attribution methods explain black-box machine learning (ML) models by
assigning importance scores to input features. These methods can be
computationally expensive for large ML models. To address this challenge, there
has been increasing efforts to develop amortized explainers, where a machine
learning model is trained to predict feature attribution scores with only one
inference. Despite their efficiency, amortized explainers can produce
inaccurate predictions and misleading explanations. In this paper, we propose
selective explanations, a novel feature attribution method that (i) detects
when amortized explainers generate low-quality explanations and (ii) improves
these explanations using a technique called explanations with initial guess.
Our selective explanation method allows practitioners to specify the fraction
of samples that receive explanations with initial guess, offering a principled
way to bridge the gap between amortized explainers and their high-quality
counterparts.",2024-05-29,cs.CY,
http://arxiv.org/abs/2406.00062v1,"Automated clinical text anonymization has the potential to unlock the
widespread sharing of textual health data for secondary usage while assuring
patient privacy and safety. Despite the proposal of many complex and
theoretically successful anonymization solutions in literature, these
techniques remain flawed. As such, clinical institutions are still reluctant to
apply them for open access to their data. Recent advances in developing Large
Language Models (LLMs) pose a promising opportunity to further the field, given
their capability to perform various tasks. This paper proposes six new
evaluation metrics tailored to the challenges of generative anonymization with
LLMs. Moreover, we present a comparative study of LLM-based methods, testing
them against two baseline techniques. Our results establish LLM-based models as
a reliable alternative to common approaches, paving the way toward trustworthy
anonymization of clinical text.",2024-05-29,cs.CL,
http://arxiv.org/abs/2405.19561v1,"The startling success of ChatGPT and other large language models (LLMs) using
transformer-based generative neural network architecture in applications such
as natural language processing and image synthesis has many researchers excited
about potential opportunities in process systems engineering (PSE). The almost
human-like performance of LLMs in these areas is indeed very impressive,
surprising, and a major breakthrough. Their capabilities are very useful in
certain tasks, such as writing first drafts of documents, code writing
assistance, text summarization, etc. However, their success is limited in
highly scientific domains as they cannot yet reason, plan, or explain due to
their lack of in-depth domain knowledge. This is a problem in domains such as
chemical engineering as they are governed by fundamental laws of physics and
chemistry (and biology), constitutive relations, and highly technical knowledge
about materials, processes, and systems. Although purely data-driven machine
learning has its immediate uses, the long-term success of AI in scientific and
engineering domains would depend on developing hybrid AI systems that use first
principles and technical knowledge effectively. We call these hybrid AI systems
Large Knowledge Models (LKMs), as they will not be limited to only NLP-based
techniques or NLP-like applications. In this paper, we discuss the challenges
and opportunities in developing such systems in chemical engineering.",2024-05-29,cs.AI,
http://arxiv.org/abs/2406.00061v1,"We present STAT: a simple algorithm to prune transformer models without any
fine-tuning. STAT eliminates both attention heads and neurons from the network,
while preserving accuracy by calculating a correction to the weights of the
next layer. Each layer block in the network is compressed using a series of
principled matrix factorizations that preserve the network structure. Our
entire algorithm takes minutes to compress BERT, and less than three hours to
compress models with 7B parameters using a single GPU. Using only several
hundred data examples, STAT preserves the output of the network and improves
upon existing gradient-free pruning methods. It is even competitive with
methods that include significant fine-tuning. We demonstrate our method on both
encoder and decoder architectures, including BERT, DistilBERT, and Llama-2
using benchmarks such as GLUE, Squad, WikiText2.",2024-05-29,cs.LG,
http://arxiv.org/abs/2405.19559v1,"In this note, we provide a refined analysis of Mitra's algorithm
\cite{mitra2008clustering} for classifying general discrete mixture
distribution models. Built upon spectral clustering
\cite{mcsherry2001spectral}, this algorithm offers compelling conditions for
probability distributions. We enhance this analysis by tailoring the model to
bipartite stochastic block models, resulting in more refined conditions.
Compared to those derived in \cite{mitra2008clustering}, our improved
separation conditions are obtained.",2024-05-29,cs.LG,
http://arxiv.org/abs/2406.00060v1,"Reducing serving cost and latency is a fundamental concern for the deployment
of language models (LMs) in business applications. To address this, cascades of
LMs offer an effective solution that conditionally employ smaller models for
simpler queries. Cascaded systems are typically built with independently
trained models, neglecting the advantages of considering inference-time
interactions of the cascaded LMs during training. In this paper, we present
cascade-aware training(CAT), an approach to optimizing the overall quality-cost
performance tradeoff of a cascade of LMs. We achieve inference-time benefits by
training the small LM with awareness of its place in a cascade and downstream
capabilities. We demonstrate the value of the proposed method with over 60 LM
tasks of the SuperGLUE, WMT22, and FLAN2021 datasets.",2024-05-29,cs.CL,
http://arxiv.org/abs/2405.19550v1,"To determine the safety of large language models (LLMs), AI developers must
be able to assess their dangerous capabilities. But simple prompting strategies
often fail to elicit an LLM's full capabilities. One way to elicit capabilities
more robustly is to fine-tune the LLM to complete the task. In this paper, we
investigate the conditions under which fine-tuning-based elicitation suffices
to elicit capabilities. To do this, we introduce password-locked models, LLMs
fine-tuned such that some of their capabilities are deliberately hidden.
Specifically, these LLMs are trained to exhibit these capabilities only when a
password is present in the prompt, and to imitate a much weaker LLM otherwise.
Password-locked models enable a novel method of evaluating capabilities
elicitation methods, by testing whether these password-locked capabilities can
be elicited without using the password. We find that a few high-quality
demonstrations are often sufficient to fully elicit password-locked
capabilities. More surprisingly, fine-tuning can elicit other capabilities that
have been locked using the same password, or even different passwords.
Furthermore, when only evaluations, and not demonstrations, are available,
approaches like reinforcement learning are still often able to elicit
capabilities. Overall, our findings suggest that fine-tuning is an effective
method of eliciting hidden capabilities of current models, but may be
unreliable when high-quality demonstrations are not available, e.g. as may be
the case when models' (hidden) capabilities exceed those of human
demonstrators.",2024-05-29,cs.LG,
http://arxiv.org/abs/2405.19548v1,"Extrinsic rewards can effectively guide reinforcement learning (RL) agents in
specific tasks. However, extrinsic rewards frequently fall short in complex
environments due to the significant human effort needed for their design and
annotation. This limitation underscores the necessity for intrinsic rewards,
which offer auxiliary and dense signals and can enable agents to learn in an
unsupervised manner. Although various intrinsic reward formulations have been
proposed, their implementation and optimization details are insufficiently
explored and lack standardization, thereby hindering research progress. To
address this gap, we introduce RLeXplore, a unified, highly modularized, and
plug-and-play framework offering reliable implementations of eight
state-of-the-art intrinsic reward algorithms. Furthermore, we conduct an
in-depth study that identifies critical implementation details and establishes
well-justified standard practices in intrinsically-motivated RL. The source
code for RLeXplore is available at https://github.com/RLE-Foundation/RLeXplore.",2024-05-29,cs.LG,
http://arxiv.org/abs/2405.19547v1,"Data selection has emerged as a core issue for large-scale visual-language
model pretaining (e.g., CLIP), particularly with noisy web-curated datasets.
Three main data selection approaches are: (1) leveraging external non-CLIP
models to aid data selection, (2) training new CLIP-style embedding models that
are more effective at selecting high-quality data than the original OpenAI CLIP
model, and (3) designing better metrics or strategies universally applicable to
any CLIP embedding without requiring specific model properties (e.g., CLIPScore
is one popular metric). While the first two approaches have been extensively
studied, the third remains under-explored. In this paper, we advance the third
approach by proposing two new methods. Firstly, instead of classical CLIP
scores that only consider the alignment between two modalities from a single
sample, we introduce negCLIPLoss, a CLIP loss-inspired method that adds the
alignment between one sample and its contrastive pairs as an extra
normalization term for better quality measurement. Secondly, when downstream
tasks are known, we propose a new norm-based metric, NormSim, to measure the
similarity between pretraining data and target data. We test our methods on the
data selection benchmark, DataComp~\cite{gadre2023datacomp}. Compared to the
best baseline using only OpenAI's CLIP-L/14, our methods achieve a 5.3\%
improvement on ImageNet-1k and a 2.8\% improvement on 38 downstream evaluation
tasks. Moreover, both negCLIPLoss and NormSim are compatible with existing
techniques. By combining our methods with the current best methods
DFN~\cite{fang2023data} and HYPE~\cite{kim2024hype}, we can boost average
performance on downstream tasks by 0.9\%, achieving a new state-of-the-art.",2024-05-29,cs.LG,
http://arxiv.org/abs/2405.19544v1,"The growing safety concerns surrounding Large Language Models (LLMs) raise an
urgent need to align them with diverse human preferences to simultaneously
enhance their helpfulness and safety. A promising approach is to enforce safety
constraints through Reinforcement Learning from Human Feedback (RLHF). For such
constrained RLHF, common Lagrangian-based primal-dual policy optimization
methods are computationally expensive and often unstable. This paper presents a
dualization perspective that reduces constrained alignment to an equivalent
unconstrained alignment problem. We do so by pre-optimizing a smooth and convex
dual function that has a closed form. This shortcut eliminates the need for
cumbersome primal-dual policy iterations, thus greatly reducing the
computational burden and improving training stability. Our strategy leads to
two practical algorithms in model-based and preference-based scenarios (MoCAN
and PeCAN, respectively). A broad range of experiments demonstrate the
effectiveness of our methods.",2024-05-29,cs.AI,
http://arxiv.org/abs/2406.02580v1,"Chaos presents complex dynamics arising from nonlinearity and a sensitivity
to initial states. These characteristics suggest a depth of expressivity that
underscores their potential for advanced computational applications. However,
strategies to effectively exploit chaotic dynamics for information processing
have largely remained elusive. In this study, we reveal that the essence of
chaos can be found in various state-of-the-art deep neural networks. Drawing
inspiration from this revelation, we propose a novel method that directly
leverages chaotic dynamics for deep learning architectures. Our approach is
systematically evaluated across distinct chaotic systems. In all instances, our
framework presents superior results to conventional deep neural networks in
terms of accuracy, convergence speed, and efficiency. Furthermore, we found an
active role of transient chaos formation in our scheme. Collectively, this
study offers a new path for the integration of chaos, which has long been
overlooked in information processing, and provides insights into the
prospective fusion of chaotic dynamics within the domains of machine learning
and neuromorphic computation.",2024-05-29,cs.NE,
http://arxiv.org/abs/2405.19540v1,"Minimum-entropy coupling (MEC) -- the process of finding a joint distribution
with minimum entropy for given marginals -- has applications in areas such as
causality and steganography. However, existing algorithms are either
computationally intractable for large-support distributions or limited to
specific distribution types and sensitive to hyperparameter choices. This work
addresses these limitations by unifying a prior family of iterative MEC (IMEC)
approaches into a generalized partition-based formalism. From this framework,
we derive a novel IMEC algorithm called ARIMEC, capable of handling arbitrary
discrete distributions, and introduce a method to make IMEC robust to
suboptimal hyperparameter settings. These innovations facilitate the
application of IMEC to high-throughput steganography with language models,
among other settings. Our codebase is available at
https://github.com/ssokota/mec .",2024-05-29,cs.IT,
http://arxiv.org/abs/2405.19538v2,"Since the release of the original CheXpert paper five years ago, CheXpert has
become one of the most widely used and cited clinical AI datasets. The
emergence of vision language models has sparked an increase in demands for
sharing reports linked to CheXpert images, along with a growing interest among
AI fairness researchers in obtaining demographic data. To address this,
CheXpert Plus serves as a new collection of radiology data sources, made
publicly available to enhance the scaling, performance, robustness, and
fairness of models for all subsequent machine learning tasks in the field of
radiology. CheXpert Plus is the largest text dataset publicly released in
radiology, with a total of 36 million text tokens, including 13 million
impression tokens. To the best of our knowledge, it represents the largest text
de-identification effort in radiology, with almost 1 million PHI spans
anonymized. It is only the second time that a large-scale English paired
dataset has been released in radiology, thereby enabling, for the first time,
cross-institution training at scale. All reports are paired with high-quality
images in DICOM format, along with numerous image and patient metadata covering
various clinical and socio-economic groups, as well as many pathology labels
and RadGraph annotations. We hope this dataset will boost research for AI
models that can further assist radiologists and help improve medical care. Data
is available at the following URL:
https://stanfordaimi.azurewebsites.net/datasets/5158c524-d3ab-4e02-96e9-6ee9efc110a1
Models are available at the following URL:
https://github.com/Stanford-AIMI/chexpert-plus",2024-05-29,cs.CL,
http://arxiv.org/abs/2406.00059v2,"The complexity of large language model (LLM) serving workloads has
substantially increased due to the integration with external tool invocations,
such as ChatGPT plugins. In this paper, we identify a new opportunity for
efficient LLM serving for requests that trigger tools: tool partial execution
alongside LLM decoding. To this end, we design Conveyor, an efficient LLM
serving system optimized for handling requests involving external tools. We
introduce a novel interface for tool developers to expose partial execution
opportunities to the LLM serving system and a request scheduler that
facilitates partial tool execution. Our results demonstrate that tool partial
execution can improve request completion latency by up to 38.8%.",2024-05-29,cs.CL,
http://arxiv.org/abs/2405.19531v1,"Advances in artificial intelligence (AI) have been propelling the evolution
of human-robot interaction (HRI) technologies. However, significant challenges
remain in achieving seamless interactions, particularly in tasks requiring
physical contact with humans. These challenges arise from the need for accurate
real-time perception of human actions, adaptive control algorithms for robots,
and the effective coordination between human and robotic movements. In this
paper, we propose an approach to enhancing physical HRI with a focus on dynamic
robot-assisted hand-object interaction (HOI). Our methodology integrates hand
pose estimation, adaptive robot control, and motion primitives to facilitate
human-robot collaboration. Specifically, we employ a transformer-based
algorithm to perform real-time 3D modeling of human hands from single RGB
images, based on which a motion primitives model (MPM) is designed to translate
human hand motions into robotic actions. The robot's action implementation is
dynamically fine-tuned using the continuously updated 3D hand models.
Experimental validations, including a ring-wearing task, demonstrate the
system's effectiveness in adapting to real-time movements and assisting in
precise task executions.",2024-05-29,cs.RO,
http://arxiv.org/abs/2405.19528v1,"Long-term human trajectory prediction is a challenging yet critical task in
robotics and autonomous systems. Prior work that studied how to predict
accurate short-term human trajectories with only unimodal features often failed
in long-term prediction. Reinforcement learning provides a good solution for
learning human long-term behaviors but can suffer from challenges in data
efficiency and optimization. In this work, we propose a long-term human
trajectory forecasting framework that leverages a guided diffusion model to
generate diverse long-term human behaviors in a high-level latent action space,
obtained via a hierarchical action quantization scheme using a VQ-VAE to
discretize continuous trajectories and the available context. The latent
actions are predicted by our guided diffusion model, which uses
physics-inspired guidance at test time to constrain generated multimodal action
distributions. Specifically, we use reachability analysis during the reverse
denoising process to guide the diffusion steps toward physically feasible
latent actions. We evaluate our framework on two publicly available human
trajectory forecasting datasets: SFU-Store-Nav and JRDB, and extensive
experimental results show that our framework achieves superior performance in
long-term human trajectory forecasting.",2024-05-29,cs.RO,
http://arxiv.org/abs/2405.19525v1,"Current state-of-the-art video object segmentation models have achieved great
success using supervised learning with massive labeled training datasets.
However, these models are trained using a single source domain and evaluated
using videos sampled from the same source domain. When these models are
evaluated using videos sampled from a different target domain, their
performance degrades significantly due to poor domain generalization, i.e.,
their inability to learn from multi-domain sources simultaneously using
traditional supervised learning. In this paper, We propose a dynamically
growing tree of sub-networks (DGT) to learn effectively from multi-domain
sources. DGT uses a novel lifelong learning technique that allows the model to
continuously and effectively learn from new domains without forgetting the
previously learned domains. Hence, the model can generalize to out-of-domain
videos. The proposed work is evaluated using single-source in-domain
(traditional video object segmentation), multi-source in-domain, and
multi-source out-of-domain video object segmentation. The results of DGT show a
single source in-domain performance gain of 0.2% and 3.5% on the DAVIS16 and
DAVIS17 datasets, respectively. However, when DGT is evaluated using in-domain
multi-sources, the results show superior performance compared to
state-of-the-art video object segmentation and other lifelong learning
techniques with an average performance increase in the F-score of 6.9% with
minimal catastrophic forgetting. Finally, in the out-of-domain experiment, the
performance of DGT is 2.7% and 4% better than state-of-the-art in 1 and
5-shots, respectively.",2024-05-29,cs.CV,
http://arxiv.org/abs/2405.19524v1,"The exposure of security vulnerabilities in safety-aligned language models,
e.g., susceptibility to adversarial attacks, has shed light on the intricate
interplay between AI safety and AI security. Although the two disciplines now
come together under the overarching goal of AI risk management, they have
historically evolved separately, giving rise to differing perspectives.
Therefore, in this paper, we advocate that stakeholders in AI risk management
should be aware of the nuances, synergies, and interplay between safety and
security, and unambiguously take into account the perspectives of both
disciplines in order to devise mostly effective and holistic risk mitigation
approaches. Unfortunately, this vision is often obfuscated, as the definitions
of the basic concepts of ""safety"" and ""security"" themselves are often
inconsistent and lack consensus across communities. With AI risk management
being increasingly cross-disciplinary, this issue is particularly salient. In
light of this conceptual challenge, we introduce a unified reference framework
to clarify the differences and interplay between AI safety and AI security,
aiming to facilitate a shared understanding and effective collaboration across
communities.",2024-05-29,cs.CR,
http://arxiv.org/abs/2405.19522v1,"The 2024 Index is our most comprehensive to date and arrives at an important
moment when AI's influence on society has never been more pronounced. This
year, we have broadened our scope to more extensively cover essential trends
such as technical advancements in AI, public perceptions of the technology, and
the geopolitical dynamics surrounding its development. Featuring more original
data than ever before, this edition introduces new estimates on AI training
costs, detailed analyses of the responsible AI landscape, and an entirely new
chapter dedicated to AI's impact on science and medicine. The AI Index report
tracks, collates, distills, and visualizes data related to artificial
intelligence (AI). Our mission is to provide unbiased, rigorously vetted,
broadly sourced data in order for policymakers, researchers, executives,
journalists, and the general public to develop a more thorough and nuanced
understanding of the complex field of AI. The AI Index is recognized globally
as one of the most credible and authoritative sources for data and insights on
artificial intelligence. Previous editions have been cited in major newspapers,
including the The New York Times, Bloomberg, and The Guardian, have amassed
hundreds of academic citations, and been referenced by high-level policymakers
in the United States, the United Kingdom, and the European Union, among other
places. This year's edition surpasses all previous ones in size, scale, and
scope, reflecting the growing significance that AI is coming to hold in all of
our lives.",2024-05-29,cs.AI,
http://arxiv.org/abs/2405.19519v1,"Retrieval augmented generation (RAG) provides the capability to constrain
generative model outputs, and mitigate the possibility of hallucination, by
providing relevant in-context text. The number of tokens a generative large
language model (LLM) can incorporate as context is finite, thus limiting the
volume of knowledge from which to generate an answer. We propose a two-layer
RAG framework for query-focused answer generation and evaluate a
proof-of-concept for this framework in the context of query-focused summary
generation from social media forums, focusing on emerging drug-related
information. The evaluations demonstrate the effectiveness of the two-layer
framework in resource constrained settings to enable researchers in obtaining
near real-time data from users.",2024-05-29,cs.CL,
http://arxiv.org/abs/2405.19514v1,"Digital systems are growing in importance and computing hardware is growing
more heterogeneous. Hardware design, however, remains laborious and expensive,
in part due to the limitations of conventional hardware description languages
(HDLs) like VHDL and Verilog. A longstanding research goal has been programming
hardware like software, with high-level languages that can generate efficient
hardware designs. This paper describes Kanagawa, a language that takes a new
approach to combine the programmer productivity benefits of traditional
High-Level Synthesis (HLS) approaches with the expressibility and hardware
efficiency of Register-Transfer Level (RTL) design. The language's concise
syntax, matched with a hardware design-friendly execution model, permits a
relatively simple toolchain to map high-level code into efficient hardware
implementations.",2024-05-29,cs.PL,
http://arxiv.org/abs/2405.19513v1,"We consider a decentralized optimization problem for networks affected by
communication delays. Examples of such networks include collaborative machine
learning, sensor networks, and multi-agent systems. To mimic communication
delays, we add virtual non-computing nodes to the network, resulting in
directed graphs. This motivates investigating decentralized optimization
solutions on directed graphs. Existing solutions assume nodes know their
out-degrees, resulting in limited applicability. To overcome this limitation,
we introduce a novel gossip-based algorithm, called DT-GO, that does not need
to know the out-degrees. The algorithm is applicable in general directed
networks, for example networks with delays or limited acknowledgment
capabilities. We derive convergence rates for both convex and non-convex
objectives, showing that our algorithm achieves the same complexity order as
centralized Stochastic Gradient Descent. In other words, the effects of the
graph topology and delays are confined to higher-order terms. Additionally, we
extend our analysis to accommodate time-varying network topologies. Numerical
simulations are provided to support our theoretical findings.",2024-05-29,cs.LG,
http://arxiv.org/abs/2405.19507v1,"Triply periodic minimal surfaces (TPMS) are a class of metamaterials with a
variety of applications and well-known primitives. We present a new method for
discovering novel microscale TPMS structures with exceptional
energy-dissipation capabilities, achieving double the energy absorption of the
best existing TPMS primitive structure. Our approach employs a parametric
representation, allowing seamless interpolation between structures and
representing a rich TPMS design space. We show that simulations are intractable
for optimizing microscale hyperelastic structures, and instead propose a
sample-efficient computational strategy for rapidly discovering structures with
extreme energy dissipation using limited amounts of empirical data from
3D-printed and tested microscale metamaterials. This strategy ensures
high-fidelity results but involves time-consuming 3D printing and testing. To
address this, we leverage an uncertainty-aware Deep Ensembles model to predict
microstructure behaviors and identify which structures to 3D-print and test
next. We iteratively refine our model through batch Bayesian optimization,
selecting structures for fabrication that maximize exploration of the
performance space and exploitation of our energy-dissipation objective. Using
our method, we produce the first open-source dataset of hyperelastic microscale
TPMS structures, including a set of novel structures that demonstrate extreme
energy dissipation capabilities. We show several potential applications of
these structures in protective equipment and bone implants.",2024-05-29,cs.GR,
http://arxiv.org/abs/2405.19501v1,"In this paper, we present a novel methodology we call MDS-ViTNet (Multi
Decoder Saliency by Vision Transformer Network) for enhancing visual saliency
prediction or eye-tracking. This approach holds significant potential for
diverse fields, including marketing, medicine, robotics, and retail. We propose
a network architecture that leverages the Vision Transformer, moving beyond the
conventional ImageNet backbone. The framework adopts an encoder-decoder
structure, with the encoder utilizing a Swin transformer to efficiently embed
most important features. This process involves a Transfer Learning method,
wherein layers from the Vision Transformer are converted by the Encoder
Transformer and seamlessly integrated into a CNN Decoder. This methodology
ensures minimal information loss from the original input image. The decoder
employs a multi-decoding technique, utilizing dual decoders to generate two
distinct attention maps. These maps are subsequently combined into a singular
output via an additional CNN model. Our trained model MDS-ViTNet achieves
state-of-the-art results across several benchmarks. Committed to fostering
further collaboration, we intend to make our code, models, and datasets
accessible to the public.",2024-05-29,cs.CV,
http://arxiv.org/abs/2405.19498v1,"This paper introduces an interdisciplinary framework called Machine
Psychology, which merges principles from operant learning psychology with a
specific Artificial Intelligence model, the Non-Axiomatic Reasoning System
(NARS), to enhance Artificial General Intelligence (AGI) research. The core
premise of this framework is that adaptation is crucial to both biological and
artificial intelligence and can be understood through operant conditioning
principles. The study assesses this approach via three operant learning tasks
using OpenNARS for Applications (ONA): simple discrimination, changing
contingencies, and conditional discrimination tasks.
  In the simple discrimination task, NARS demonstrated rapid learning,
achieving perfect accuracy during both training and testing phases. The
changing contingencies task showcased NARS's adaptability, as it successfully
adjusted its behavior when task conditions were reversed. In the conditional
discrimination task, NARS handled complex learning scenarios effectively,
achieving high accuracy by forming and utilizing intricate hypotheses based on
conditional cues.
  These findings support the application of operant conditioning as a framework
for creating adaptive AGI systems. NARS's ability to operate under conditions
of insufficient knowledge and resources, coupled with its sensorimotor
reasoning capabilities, establishes it as a robust model for AGI. The Machine
Psychology framework, by incorporating elements of natural intelligence such as
continuous learning and goal-driven behavior, offers a scalable and flexible
approach for real-world applications. Future research should investigate using
enhanced NARS systems, more advanced tasks, and applying this framework to
diverse, complex challenges to further progress the development of human-level
AI.",2024-05-29,cs.AI,
http://arxiv.org/abs/2405.19493v1,"We report on experiments with the ziggurat algorithm for generating Gaussian
distributed random numbers. The study utilizes our open source Java
implementation that was introduced originally for Java 11 at a time when the
Java API only provided the much slower polar method. Our Java implementation of
the ziggurat algorithm is a port of the GNU Scientific Library's C
implementation. Java 17 introduced a significant overhaul of pseudorandom
number generation, including several modern pseudorandom number generators
(PRNGs) as well as additional functionality, among which includes switching
from the polar method to a modified ziggurat algorithm. In the experiments of
this paper, we explore whether there is still a need for our implementation for
Java 17+ applications. Our results show that Java 17's modified ziggurat is
faster than our implementation for the PRNGs that support it. However, Java 17+
continues to use the polar method for the legacy PRNGs Random, SecureRandom,
and ThreadLocalRandom. The linear congruential method of Java's Random class
lacks the statistical properties required by Java's modified ziggurat
implementation; and SecureRandom and ThreadLocalRandom unfortunately use the
polar method as a side-effect of extending Random. Our implementation of the
original ziggurat algorithm does not require the same statistical properties of
the underlying PRNG as Java 17's optimized version, and can be used with any of
these PRNGs, and is especially relevant where pre-Java 17 support is required.",2024-05-29,cs.DS,
http://arxiv.org/abs/2405.19491v1,"In the context of the Damage Mechanics Challenge, we adopt a phase-field
model of brittle fracture to blindly predict the behavior up to failure of a
notched three-point-bending specimen loaded under mixed-mode conditions. The
beam is additively manufactured using a geo-architected gypsum based on the
combination of bassanite and a water-based binder. The calibration of the
material parameters involved in the model is based on a set of available
independent experimental tests and on a two-stage procedure. In the first stage
an estimate of most of the elastic parameters is obtained, whereas the
remaining parameters are optimized in the second stage so as to minimize the
discrepancy between the numerical predictions and a set of experimental results
on notched three-point-bending beams. The good agreement between numerical
predictions and experimental results in terms of load-displacement curves and
crack paths demonstrates the predictive ability of the model and the
reliability of the calibration procedure.",2024-05-29,cs.CE,
http://arxiv.org/abs/2405.19487v1,"We present a generative dialogue system capable of operating in a full-duplex
manner, allowing for seamless interaction. It is based on a large language
model (LLM) carefully aligned to be aware of a perception module, a motor
function module, and the concept of a simple finite state machine (called
neural FSM) with two states. The perception and motor function modules operate
simultaneously, allowing the system to simultaneously speak and listen to the
user. The LLM generates textual tokens for inquiry responses and makes
autonomous decisions to start responding to, wait for, or interrupt the user by
emitting control tokens to the neural FSM. All these tasks of the LLM are
carried out as next token prediction on a serialized view of the dialogue in
real-time. In automatic quality evaluations simulating real-life interaction,
the proposed system reduces the average conversation response latency by more
than 3 folds compared with LLM-based half-duplex dialogue systems while
responding within less than 500 milliseconds in more than 50% of evaluated
interactions. Running a LLM with only 8 billion parameters, our system exhibits
a 8% higher interruption precision rate than the best available commercial LLM
for voice-based dialogue.",2024-05-29,cs.CL,
http://arxiv.org/abs/2405.19480v1,"The rapid advancement of 5G networks and the upcoming transition to 6G
necessitate the use of the Open Radio Access Network (O-RAN) architecture to
enable greater flexibility, interoperability, and innovation. This shift
towards 6G and O-RAN requires the development of advanced simulation tools for
testing, analyzing, and optimizing Radio Access Network (RAN) operations. This
need becomes critical due to the complex dynamics of mobility management
inherent in the 6G vision and next-generation networks. These networks
anticipate advanced handover methods for mobile users, UAVs, IoT devices, and
beyond. Addressing this gap, this paper introduces RANFusion: a robust RAN
simulator specifically created to explore a variety of handover scenarios and
to test and balance resources between users. This tool enables precise
simulations for refining handover strategies within RAN and O-RAN environments,
thereby ensuring optimal performance and reliability in these advanced network
infrastructures.",2024-05-29,cs.NI,
http://arxiv.org/abs/2405.19479v1,"Growing interest and investment in the capabilities of foundation models has
positioned such systems to impact a wide array of public services. Alongside
these opportunities is the risk that these systems reify existing power
imbalances and cause disproportionate harm to marginalized communities.
Participatory approaches hold promise to instead lend agency and
decision-making power to marginalized stakeholders. But existing approaches in
participatory AI/ML are typically deeply grounded in context - how do we apply
these approaches to foundation models, which are, by design, disconnected from
context? Our paper interrogates this question.
  First, we examine existing attempts at incorporating participation into
foundation models. We highlight the tension between participation and scale,
demonstrating that it is intractable for impacted communities to meaningfully
shape a foundation model that is intended to be universally applicable. In
response, we develop a blueprint for participatory foundation models that
identifies more local, application-oriented opportunities for meaningful
participation. In addition to the ""foundation"" layer, our framework proposes
the ""subfloor'' layer, in which stakeholders develop shared technical
infrastructure, norms and governance for a grounded domain, and the ""surface''
layer, in which affected communities shape the use of a foundation model for a
specific downstream task. The intermediate ""subfloor'' layer scopes the range
of potential harms to consider, and affords communities more concrete avenues
for deliberation and intervention. At the same time, it avoids duplicative
effort by scaling input across relevant use cases. Through three case studies
in clinical care, financial services, and journalism, we illustrate how this
multi-layer model can create more meaningful opportunities for participation
than solely intervening at the foundation layer.",2024-05-29,cs.CY,
http://arxiv.org/abs/2405.19471v1,"The principle of data minimization aims to reduce the amount of data
collected, processed or retained to minimize the potential for misuse,
unauthorized access, or data breaches. Rooted in privacy-by-design principles,
data minimization has been endorsed by various global data protection
regulations. However, its practical implementation remains a challenge due to
the lack of a rigorous formulation. This paper addresses this gap and
introduces an optimization framework for data minimization based on its legal
definitions. It then adapts several optimization algorithms to perform data
minimization and conducts a comprehensive evaluation in terms of their
compliance with minimization objectives as well as their impact on user
privacy. Our analysis underscores the mismatch between the privacy expectations
of data minimization and the actual privacy benefits, emphasizing the need for
approaches that account for multiple facets of real-world privacy risks.",2024-05-29,cs.LG,
http://arxiv.org/abs/2405.19466v1,"Real-world decision-making requires grappling with a perpetual lack of data
as environments change; intelligent agents must comprehend uncertainty and
actively gather information to resolve it. We propose a new framework for
learning bandit algorithms from massive historical data, which we demonstrate
in a cold-start recommendation problem. First, we use historical data to
pretrain an autoregressive model to predict a sequence of repeated
feedback/rewards (e.g., responses to news articles shown to different users
over time). In learning to make accurate predictions, the model implicitly
learns an informed prior based on rich action features (e.g., article
headlines) and how to sharpen beliefs as more rewards are gathered (e.g.,
clicks as each article is recommended). At decision-time, we autoregressively
sample (impute) an imagined sequence of rewards for each action, and choose the
action with the largest average imputed reward. Far from a heuristic, our
approach is an implementation of Thompson sampling (with a learned prior), a
prominent active exploration algorithm. We prove our pretraining loss directly
controls online decision-making performance, and we demonstrate our framework
on a news recommendation task where we integrate end-to-end fine-tuning of a
pretrained language model to process news article headline text to improve
performance.",2024-05-29,cs.LG,
http://arxiv.org/abs/2405.19465v1,"Text-Video Retrieval (TVR) aims to align relevant video content with natural
language queries. To date, most state-of-the-art TVR methods learn
image-to-video transfer learning based on large-scale pre-trained
visionlanguage models (e.g., CLIP). However, fully fine-tuning these
pre-trained models for TVR incurs prohibitively expensive computation costs. To
this end, we propose to conduct efficient text-video Retrieval with a
sparse-andcorrelated AdaPter (RAP), i.e., fine-tuning the pre-trained model
with a few parameterized layers. To accommodate the text-video scenario, we
equip our RAP with two indispensable characteristics: temporal sparsity and
correlation. Specifically, we propose a low-rank modulation module to refine
the per-image features from the frozen CLIP backbone, which accentuates salient
frames within the video features while alleviating temporal redundancy.
Besides, we introduce an asynchronous self-attention mechanism that first
selects the top responsive visual patches and augments the correlation modeling
between them with learnable temporal and patch offsets. Extensive experiments
on four TVR datasets demonstrate that RAP achieves superior or comparable
performance compared to the fully fine-tuned counterpart and other
parameter-efficient fine-tuning methods.",2024-05-29,cs.CV,
http://arxiv.org/abs/2405.19464v1,"The digital transformation of modern cities by integrating advanced
information, communication, and computing technologies has marked the epoch of
data-driven smart city applications for efficient and sustainable urban
management. Despite their effectiveness, these applications often rely on
massive amounts of high-dimensional and multi-domain data for monitoring and
characterizing different urban sub-systems, presenting challenges in
application areas that are limited by data quality and availability, as well as
costly efforts for generating urban scenarios and design alternatives. As an
emerging research area in deep learning, Generative Artificial Intelligence
(AI) models have demonstrated their unique values in data and code generation.
This survey paper aims to explore the innovative integration of generative AI
techniques and urban digital twins to address challenges in the realm of smart
cities in various urban sectors, such as transportation and mobility
management, energy system operations, building and infrastructure management,
and urban design. The survey starts with the introduction of popular generative
AI models with their application areas, followed by a structured review of the
existing urban science applications that leverage the autonomous capability of
the generative AI techniques to facilitate (a) data augmentation for promoting
urban monitoring and predictive analytics, (b) synthetic data and scenario
generation, (c) automated 3D city modeling, and (d) generative urban design and
optimization. Based on the review, this survey discusses potential
opportunities and technical strategies that integrate generative AI models into
the next-generation urban digital twins for more reliable, scalable, and
automated management of smart cities.",2024-05-29,cs.AI,
http://arxiv.org/abs/2405.19462v1,"Neural Machine Translation models are extremely data and compute-hungry.
However, not all data points contribute equally to model training and
generalization. Data pruning to remove the low-value data points has the
benefit of drastically reducing the compute budget without significant drop in
model performance. In this paper, we propose a new data pruning technique:
Checkpoints Across Time (CAT), that leverages early model training dynamics to
identify the most relevant data points for model performance. We benchmark CAT
against several data pruning techniques including COMET-QE, LASER and LaBSE. We
find that CAT outperforms the benchmarks on Indo-European languages on multiple
test sets. When applied to English-German, English-French and English-Swahili
translation tasks, CAT achieves comparable performance to using the full
dataset, while pruning up to 50% of training data. We inspect the data points
that CAT selects and find that it tends to favour longer sentences and
sentences with unique or rare words.",2024-05-29,cs.CL,
http://arxiv.org/abs/2405.19460v1,"Parsons problems are a type of programming activity that present learners
with blocks of existing code and requiring them to arrange those blocks to form
a program rather than write the code from scratch. Micro Parsons problems
extend this concept by having students assemble segments of code to form a
single line of code rather than an entire program. Recent investigations into
micro Parsons problems have primarily focused on supporting learners leaving
open the question of micro Parsons efficacy as an exam item and how students
perceive it when preparing for exams.
  To fill this gap, we included a variety of micro Parsons problems on four
exams in an introductory programming course taught in Python. We use Item
Response Theory to investigate the difficulty of the micro Parsons problems as
well as the ability of the questions to differentiate between high and low
ability students. We then compare these results to results for related
questions where students are asked to write a single line of code from scratch.
Finally, we conduct a thematic analysis of the survey responses to investigate
how students' perceptions of micro Parsons both when practicing for exams and
as they appear on exams.",2024-05-29,cs.HC,
http://arxiv.org/abs/2405.19458v1,"Diffusion models show a remarkable ability in generating images that closely
mirror the training distribution. However, these models are prone to training
data memorization, leading to significant privacy, ethical, and legal concerns,
particularly in sensitive fields such as medical imaging. We hypothesize that
memorization is driven by the overparameterization of deep models, suggesting
that regularizing model capacity during fine-tuning could be an effective
mitigation strategy. Parameter-efficient fine-tuning (PEFT) methods offer a
promising approach to capacity control by selectively updating specific
parameters. However, finding the optimal subset of learnable parameters that
balances generation quality and memorization remains elusive. To address this
challenge, we propose a bi-level optimization framework that guides automated
parameter selection by utilizing memorization and generation quality metrics as
rewards. Our framework successfully identifies the optimal parameter set to be
updated to satisfy the generation-memorization tradeoff. We perform our
experiments for the specific task of medical image generation and outperform
existing state-of-the-art training-time mitigation strategies by fine-tuning as
few as 0.019% of model parameters. Furthermore, we show that the strategies
learned through our framework are transferable across different datasets and
domains. Our proposed framework is scalable to large datasets and agnostic to
the choice of reward functions. Finally, we show that our framework can be
combined with existing approaches for further memorization mitigation.",2024-05-29,cs.CV,
http://arxiv.org/abs/2405.19456v1,"Evaluating startups in their early stages is a complex task that requires
detailed analysis by experts. While automating this process on a large scale
can significantly impact businesses, the inherent complexity poses challenges.
This paper addresses this challenge by introducing the Startup Success
Forecasting Framework (SSFF), a new automated system that combines traditional
machine learning with advanced language models. This intelligent agent-based
architecture is designed to reason, act, synthesize, and decide like a venture
capitalist to perform the analysis end-to-end. The SSFF is made up of three
main parts: - Prediction Block: Uses random forests and neural networks to make
predictions. - Analyst Block: Simulates VC analysis scenario and uses SOTA
prompting techniques - External Knowledge Block: Gathers real-time information
from external sources. This framework requires minimal input data about the
founder and startup description, enhances it with additional data from external
resources, and performs a detailed analysis with high accuracy, all in an
automated manner",2024-05-29,cs.AI,
http://arxiv.org/abs/2405.19454v1,"Recent research on the grokking phenomenon has illuminated the intricacies of
neural networks' training dynamics and their generalization behaviors. Grokking
refers to a sharp rise of the network's generalization accuracy on the test
set, which occurs long after an extended overfitting phase, during which the
network perfectly fits the training set. While the existing research primarily
focus on shallow networks such as 2-layer MLP and 1-layer Transformer, we
explore grokking on deep networks (e.g. 12-layer MLP). We empirically replicate
the phenomenon and find that deep neural networks can be more susceptible to
grokking than its shallower counterparts. Meanwhile, we observe an intriguing
multi-stage generalization phenomenon when increase the depth of the MLP model
where the test accuracy exhibits a secondary surge, which is scarcely seen on
shallow models. We further uncover compelling correspondences between the
decreasing of feature ranks and the phase transition from overfitting to the
generalization stage during grokking. Additionally, we find that the
multi-stage generalization phenomenon often aligns with a double-descent
pattern in feature ranks. These observations suggest that internal feature rank
could serve as a more promising indicator of the model's generalization
behavior compared to the weight-norm. We believe our work is the first one to
dive into grokking in deep neural networks, and investigate the relationship of
feature rank and generalization performance.",2024-05-29,cs.LG,
http://arxiv.org/abs/2405.19453v1,"Recent advancements in decentralized learning, such as Federated Learning
(FL), Split Learning (SL), and Split Federated Learning (SplitFed), have
expanded the potentials of machine learning. SplitFed aims to minimize the
computational burden on individual clients in FL and parallelize SL while
maintaining privacy. This study investigates the resilience of SplitFed to
packet loss at model split points. It explores various parameter aggregation
strategies of SplitFed by examining the impact of splitting the model at
different points-either shallow split or deep split-on the final global model
performance. The experiments, conducted on a human embryo image segmentation
task, reveal a statistically significant advantage of a deeper split point.",2024-05-29,cs.AI,
http://arxiv.org/abs/2405.19452v1,"The current state-of-the-art in quadruped locomotion is able to produce
robust motion for terrain traversal but requires the segmentation of a desired
robot trajectory into a discrete set of locomotion skills such as trot and
crawl. In contrast, in this work we demonstrate the feasibility of learning a
single, unified representation for quadruped locomotion enabling continuous
blending between gait types and characteristics. We present Gaitor, which
learns a disentangled representation of locomotion skills, thereby sharing
information common to all gait types seen during training. The structure
emerging in the learnt representation is interpretable in that it is found to
encode phase correlations between the different gait types. These can be
leveraged to produce continuous gait transitions. In addition, foot swing
characteristics are disentangled and directly addressable. Together with a
rudimentary terrain encoding and a learned planner operating in this structured
latent representation, Gaitor is able to take motion commands including desired
gait type and characteristics from a user while reacting to uneven terrain. We
evaluate Gaitor in both simulated and real-world settings on the ANYmal C
platform. To the best of our knowledge, this is the first work learning such a
unified and interpretable latent representation for multiple gaits, resulting
in on-demand continuous blending between different locomotion modes on a real
quadruped robot.",2024-05-29,cs.RO,
http://arxiv.org/abs/2405.19450v1,"Image deraining aims to remove rain streaks from rainy images and restore
clear backgrounds. Currently, some research that employs the Fourier transform
has proved to be effective for image deraining, due to it acting as an
effective frequency prior for capturing rain streaks. However, despite there
exists dependency of low frequency and high frequency in images, these
Fourier-based methods rarely exploit the correlation of different frequencies
for conjuncting their learning procedures, limiting the full utilization of
frequency information for image deraining. Alternatively, the recently emerged
Mamba technique depicts its effectiveness and efficiency for modeling
correlation in various domains (e.g., spatial, temporal), and we argue that
introducing Mamba into its unexplored Fourier spaces to correlate different
frequencies would help improve image deraining. This motivates us to propose a
new framework termed FourierMamba, which performs image deraining with Mamba in
the Fourier space. Owning to the unique arrangement of frequency orders in
Fourier space, the core of FourierMamba lies in the scanning encoding of
different frequencies, where the low-high frequency order formats exhibit
differently in the spatial dimension (unarranged in axis) and channel dimension
(arranged in axis). Therefore, we design FourierMamba that correlates Fourier
space information in the spatial and channel dimensions with distinct designs.
Specifically, in the spatial dimension Fourier space, we introduce the zigzag
coding to scan the frequencies to rearrange the orders from low to high
frequencies, thereby orderly correlating the connections between frequencies;
in the channel dimension Fourier space with arranged orders of frequencies in
axis, we can directly use Mamba to perform frequency correlation and improve
the channel information representation.",2024-05-29,cs.CV,
http://arxiv.org/abs/2406.05132v1,"The integration of language and 3D perception is crucial for developing
embodied agents and robots that comprehend and interact with the physical
world. While large language models (LLMs) have demonstrated impressive language
understanding and generation capabilities, their adaptation to 3D environments
(3D-LLMs) remains in its early stages. A primary challenge is the absence of
large-scale datasets that provide dense grounding between language and 3D
scenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset
comprising 40,087 household scenes paired with 6.2 million densely-grounded
scene-language instructions. Our results show that instruction tuning with
3D-GRAND significantly enhances grounding capabilities and reduces
hallucinations in 3D-LLMs. As part of our contributions, we propose a
comprehensive benchmark 3D-POPE to systematically evaluate hallucination in
3D-LLMs, enabling fair comparisons among future models. Our experiments
highlight a scaling effect between dataset size and 3D-LLM performance,
emphasizing the critical role of large-scale 3D-text datasets in advancing
embodied AI research. Notably, our results demonstrate early signals for
effective sim-to-real transfer, indicating that models trained on large
synthetic data can perform well on real-world 3D scans. Through 3D-GRAND and
3D-POPE, we aim to equip the embodied AI community with essential resources and
insights, setting the stage for more reliable and better-grounded 3D-LLMs.
Project website: https://3d-grand.github.io",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05131v1,"Video object segmentation approaches primarily rely on large-scale
pixel-accurate human-annotated datasets for model development. In Dense Video
Object Segmentation (DVOS) scenarios, each video frame encompasses hundreds of
small, dense, and partially occluded objects. Accordingly, the labor-intensive
manual annotation of even a single frame often takes hours, which hinders the
development of DVOS for many applications. Furthermore, in videos with dense
patterns, following a large number of objects that move in different directions
poses additional challenges. To address these challenges, we proposed a
semi-self-supervised spatiotemporal approach for DVOS utilizing a
diffusion-based method through multi-task learning. Emulating real videos'
optical flow and simulating their motion, we developed a methodology to
synthesize computationally annotated videos that can be used for training DVOS
models; The model performance was further improved by utilizing weakly labeled
(computationally generated but imprecise) data. To demonstrate the utility and
efficacy of the proposed approach, we developed DVOS models for wheat head
segmentation of handheld and drone-captured videos, capturing wheat crops in
fields of different locations across various growth stages, spanning from
heading to maturity. Despite using only a few manually annotated video frames,
the proposed approach yielded high-performing models, achieving a Dice score of
0.82 when tested on a drone-captured external test set. While we showed the
efficacy of the proposed approach for wheat head segmentation, its application
can be extended to other crops or DVOS in other domains, such as crowd analysis
or microscopic image analysis.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05130v1,"Multimodal large language models (MLLMs) fine-tuned with multimodal
instruction datasets have demonstrated remarkable capabilities in multimodal
tasks. However, fine-tuning all parameters of MLLMs has become challenging as
they usually contain billions of parameters. To address this issue, we study
parameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify
effective methods for enhancing the performance of MLLMs in scenarios where
only a limited number of parameters are trained. This paper conducts empirical
studies using four popular PEFT methods to fine-tune the LLM component of
open-source MLLMs. We present a comprehensive analysis that encompasses various
aspects, including the impact of PEFT methods on various models, parameters and
location of the PEFT module, size of fine-tuning data, model stability based on
PEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT
methods on seven datasets from two different categories: unseen and seen
datasets. Across all experiments, we show that the adapter is the
best-performing PEFT method. At the same time, fine-tuning the connector layers
leads to improved performance in most MLLMs. Code and data are available at
https://github.com/alenai97/PEFT-MLLM.git.",2024-06-07,cs.CL,
http://arxiv.org/abs/2406.05129v1,"Storing data is particularly a challenge when dealing with image data which
often involves large file sizes due to the high resolution and complexity of
images. Efficient image compression algorithms are crucial to better manage
data storage costs. In this paper, we propose a novel region-based lossy image
compression technique, called PatchSVD, based on the Singular Value
Decomposition (SVD) algorithm. We show through experiments that PatchSVD
outperforms SVD-based image compression with respect to three popular image
compression metrics. Moreover, we compare PatchSVD compression artifacts with
those of Joint Photographic Experts Group (JPEG) and SVD-based image
compression and illustrate some cases where PatchSVD compression artifacts are
preferable compared to JPEG and SVD artifacts.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05127v1,"Multimodal Large Language Models (MLLMs) have demonstrated exceptional
capabilities in processing vision-language tasks. One of the crux of MLLMs lies
in vision tokenization, which involves efficiently transforming input visual
signals into feature representations that are most beneficial for LLMs.
However, existing vision tokenizers, essential for semantic alignment between
vision and language, remain problematic. Existing methods aggressively fragment
visual input, corrupting the visual semantic integrity. To address this, this
paper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),
which groups visual features into semantic units via a dynamic clustering
algorithm, flexibly determining the number of tokens based on image complexity.
The resulting vision tokens effectively preserve semantic integrity and capture
both low-frequency and high-frequency visual features. The proposed MLLM
(Setokim) equipped with SeTok significantly demonstrates superior performance
across various tasks, as evidenced by our experimental results. The project
page is at https://chocowu.github.io/SeTok-web/.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05120v1,"Mammalian brains handle complex reasoning by integrating information across
brain regions specialized for particular sensory modalities. This enables
improved robustness and generalization versus deep neural networks, which
typically process one modality and are vulnerable to perturbations. While
defense methods exist, they do not generalize well across perturbations. We
developed a fusion model combining background and foreground features from CNNs
trained on Imagenet and Places365. We tested its robustness to
human-perceivable perturbations on MS COCO. The fusion model improved
robustness, especially for classes with greater context variability. Our
proposed solution for integrating multiple modalities provides a new approach
to enhance robustness and may be complementary to existing methods.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05119v1,"A key challenge that threatens the widespread use of neural networks in
safety-critical applications is their vulnerability to adversarial attacks. In
this paper, we study the second-order behavior of continuously differentiable
deep neural networks, focusing on robustness against adversarial perturbations.
First, we provide a theoretical analysis of robustness and attack certificates
for deep classifiers by leveraging local gradients and upper bounds on the
second derivative (curvature constant). Next, we introduce a novel algorithm to
analytically compute provable upper bounds on the second derivative of neural
networks. This algorithm leverages the compositional structure of the model to
propagate the curvature bound layer-by-layer, giving rise to a scalable and
modular approach. The proposed bound can serve as a differentiable regularizer
to control the curvature of neural networks during training, thereby enhancing
robustness. Finally, we demonstrate the efficacy of our method on
classification tasks using the MNIST and CIFAR-10 datasets.",2024-06-07,cs.LG,
http://arxiv.org/abs/2406.05114v1,"Recent research identified a temporary performance drop on previously learned
tasks when transitioning to a new one. This drop is called the stability gap
and has great consequences for continual learning: it complicates the direct
employment of continually learning since the worse-case performance at
task-boundaries is dramatic, it limits its potential as an energy-efficient
training paradigm, and finally, the stability drop could result in a reduced
final performance of the algorithm. In this paper, we show that the stability
gap also occurs when applying joint incremental training of homogeneous tasks.
In this scenario, the learner continues training on the same data distribution
and has access to all data from previous tasks. In addition, we show that in
this scenario, there exists a low-loss linear path to the next minima, but that
SGD optimization does not choose this path. We perform further analysis
including a finer batch-wise analysis which could provide insights towards
potential solution directions.",2024-06-07,cs.LG,
http://arxiv.org/abs/2406.05113v1,"We introduce LlavaGuard, a family of VLM-based safeguard models, offering a
versatile framework for evaluating the safety compliance of visual content.
Specifically, we designed LlavaGuard for dataset annotation and generative
model safeguarding. To this end, we collected and annotated a high-quality
visual dataset incorporating a broad safety taxonomy, which we use to tune VLMs
on context-aware safety risks. As a key innovation, LlavaGuard's new responses
contain comprehensive information, including a safety rating, the violated
safety categories, and an in-depth rationale. Further, our introduced
customizable taxonomy categories enable the context-specific alignment of
LlavaGuard to various scenarios. Our experiments highlight the capabilities of
LlavaGuard in complex and real-world applications. We provide checkpoints
ranging from 7B to 34B parameters demonstrating state-of-the-art performance,
with even the smallest models outperforming baselines like GPT-4. We make our
dataset and model weights publicly available and invite further research to
address the diverse needs of communities and contexts.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05111v1,"As the permeability of AI systems in interpersonal domains like the home
expands, their technical capabilities of generating explanations are required
to be aligned with user expectations for transparency and reasoning. This paper
presents insights from our ongoing work in understanding the effectiveness of
explanations in Conversational AI systems for older adults aging in place and
their family caregivers. We argue that in collaborative and multi-user
environments like the home, AI systems will make recommendations based on a
host of information sources to generate explanations. These sources may be more
or less salient based on user mental models of the system and the specific
task. We highlight the need for cross technological collaboration between AI
systems and other available sources of information in the home to generate
multiple explanations for a single user query. Through example scenarios in a
caregiving home setting, this paper provides an initial framework for
categorizing these sources and informing a potential design space for AI
explanations surrounding everyday tasks in the home.",2024-06-07,cs.HC,
http://arxiv.org/abs/2406.05109v1,"Large Generative Models (LGMs) such as GPT, Stable Diffusion, Sora, and Suno
are trained on a huge amount of language corpus, images, videos, and audio that
are extremely diverse from numerous domains. This training paradigm over
diverse well-curated data lies at the heart of generating creative and sensible
content. However, all previous graph generative models (e.g., GraphRNN, MDVAE,
MoFlow, GDSS, and DiGress) have been trained only on one dataset each time,
which cannot replicate the revolutionary success achieved by LGMs in other
fields. To remedy this crucial gap, we propose a new class of graph generative
model called Large Graph Generative Model (LGGM) that is trained on a large
corpus of graphs (over 5000 graphs) from 13 different domains. We empirically
demonstrate that the pre-trained LGGM has superior zero-shot generative
capability to existing graph generative models. Furthermore, our pre-trained
LGGM can be easily fine-tuned with graphs from target domains and demonstrate
even better performance than those directly trained from scratch, behaving as a
solid starting point for real-world customization. Inspired by Stable
Diffusion, we further equip LGGM with the capability to generate graphs given
text prompts (Text-to-Graph), such as the description of the network name and
domain (i.e., ""The power-1138-bus graph represents a network of buses in a
power distribution system.""), and network statistics (i.e., ""The graph has a
low average degree, suitable for modeling social media interactions.""). This
Text-to-Graph capability integrates the extensive world knowledge in the
underlying language model, offering users fine-grained control of the generated
graphs. We release the code, the model checkpoint, and the datasets at
https://lggm-lg.github.io/.",2024-06-07,cs.LG,
http://arxiv.org/abs/2406.05107v1,"Data exploration is a challenging process in which users examine a dataset by
iteratively employing a series of queries. While in some cases the user
explores a new dataset to become familiar with it, more often, the exploration
process is conducted with a specific analysis goal or question in mind. To
assist users in exploring a new dataset, Automated Data Exploration (ADE)
systems have been devised in previous work. These systems aim to auto-generate
a full exploration session, containing a sequence of queries that showcase
interesting elements of the data. However, existing ADE systems are often
constrained by a predefined objective function, thus always generating the same
session for a given dataset. Therefore, their effectiveness in goal-oriented
exploration, in which users need to answer specific questions about the data,
are extremely limited.
  To this end, this paper presents LINX, a generative system augmented with a
natural language interface for goal-oriented ADE. Given an input dataset and an
analytical goal described in natural language, LINX generates a personalized
exploratory session that is relevant to the user's goal. LINX utilizes a Large
Language Model (LLM) to interpret the input analysis goal, and then derive a
set of specifications for the desired output exploration session. These
specifications are then transferred to a novel, modular ADE engine based on
Constrained Deep Reinforcement Learning (CDRL), which can adapt its output
according to the specified instructions.
  To validate LINX's effectiveness, we introduce a new benchmark dataset for
goal-oriented exploration and conduct an extensive user study. Our analysis
underscores LINX's superior capability in producing exploratory notebooks that
are significantly more relevant and beneficial than those generated by existing
solutions, including ChatGPT, goal-agnostic ADE, and commercial systems.",2024-06-07,cs.DB,
http://arxiv.org/abs/2406.05096v1,"Rainfall estimation through the analysis of its impact on electromagnetic
waves has sparked increasing interest in the research community. Recent studies
have delved into its effects on cellular network performance, demonstrating the
potential to forecast rainfall levels based on electromagnetic wave attenuation
during precipitations. This paper aims to solve the problem of identifying the
nature of specific weather phenomena from the received signal level (RSL) in
4G/LTE mobile terminals. Specifically, utilizing time-series data representing
RSL, we propose a novel approach to encode time series as images and model the
task as an image classification problem, which we finally address using
convolutional neural networks (CNNs). The main benefit of the abovementioned
procedure is the opportunity to utilize various data augmentation techniques
simultaneously. This encompasses applying traditional approaches, such as
moving averages, to the time series and enhancing the generated images. We have
investigated various image data augmentation methods to identify the most
effective combination for this scenario. In the upcoming sections, we will
introduce the task of rainfall estimation and conduct a comprehensive analysis
of the dataset used. Subsequently, we will formally propose a new approach for
converting time series into images. To conclude, the paper's final section will
present and discuss the experiments conducted, providing the reader with a
brief yet comprehensive overview of the results.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05090v1,"Using feature attributions for post-hoc explanations is a common practice to
understand and verify the predictions of opaque machine learning models.
Despite the numerous techniques available, individual methods often produce
inconsistent and unstable results, putting their overall reliability into
question. In this work, we aim to systematically improve the quality of feature
attributions by combining multiple explanations across distinct methods or
their variations. For this purpose, we propose a novel approach to derive
optimal convex combinations of feature attributions that yield provable
improvements of desired quality criteria such as robustness or faithfulness to
the model behavior. Through extensive experiments involving various model
architectures and popular feature attribution techniques, we demonstrate that
our combination strategy consistently outperforms individual methods and
existing baselines.",2024-06-07,cs.LG,
http://arxiv.org/abs/2406.05088v1,"The rapid development of time series forecasting research has brought many
deep learning-based modules in this field. However, despite the increasing
amount of new forecasting architectures, it is still unclear if we have
leveraged the full potential of these existing modules within a properly
designed architecture. In this work, we propose a novel hierarchical neural
architecture search approach for time series forecasting tasks. With the design
of a hierarchical search space, we incorporate many architecture types designed
for forecasting tasks and allow for the efficient combination of different
forecasting architecture modules. Results on long-term-time-series-forecasting
tasks show that our approach can search for lightweight high-performing
forecasting architectures across different forecasting tasks.",2024-06-07,cs.LG,
http://arxiv.org/abs/2406.05087v1,"Dense retrievers are widely used in information retrieval and have also been
successfully extended to other knowledge intensive areas such as language
models, e.g., Retrieval-Augmented Generation (RAG) systems. Unfortunately, they
have recently been shown to be vulnerable to corpus poisoning attacks in which
a malicious user injects a small fraction of adversarial passages into the
retrieval corpus to trick the system into returning these passages among the
top-ranked results for a broad set of user queries. Further study is needed to
understand the extent to which these attacks could limit the deployment of
dense retrievers in real-world applications. In this work, we propose
Approximate Greedy Gradient Descent (AGGD), a new attack on dense retrieval
systems based on the widely used HotFlip method for efficiently generating
adversarial passages. We demonstrate that AGGD can select a higher quality set
of token-level perturbations than HotFlip by replacing its random token
sampling with a more structured search. Experimentally, we show that our method
achieves a high attack success rate on several datasets and using several
retrievers, and can generalize to unseen queries and new domains. Notably, our
method is extremely effective in attacking the ANCE retrieval model, achieving
attack success rates that are 17.6\% and 13.37\% higher on the NQ and MS MARCO
datasets, respectively, compared to HotFlip. Additionally, we demonstrate
AGGD's potential to replace HotFlip in other adversarial attacks, such as
knowledge poisoning of RAG systems.\footnote{Code can be find in
\url{https://github.com/JinyanSu1/AGGD}}",2024-06-07,cs.IR,
http://arxiv.org/abs/2406.05085v1,"Retrieval Augmented Generation (RAG) enhances the abilities of Large Language
Models (LLMs) by enabling the retrieval of documents into the LLM context to
provide more accurate and relevant responses. Existing RAG solutions do not
focus on queries that may require fetching multiple documents with
substantially different contents. Such queries occur frequently, but are
challenging because the embeddings of these documents may be distant in the
embedding space, making it hard to retrieve them all. This paper introduces
Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a
simple yet powerful idea: leveraging activations of Transformer's multi-head
attention layer, instead of the decoder layer, as keys for fetching
multi-aspect documents. The driving motivation is that different attention
heads can learn to capture different data aspects. Harnessing the corresponding
activations results in embeddings that represent various facets of data items
and queries, improving the retrieval accuracy for complex queries. We provide
an evaluation methodology and metrics, synthetic datasets, and real-world use
cases to demonstrate MRAG's effectiveness, showing improvements of up to 20% in
relevance over standard RAG baselines. MRAG can be seamlessly integrated with
existing RAG frameworks and benchmarking tools like RAGAS as well as different
classes of data stores.",2024-06-07,cs.CL,
http://arxiv.org/abs/2406.05084v1,"This is a long paper, an essay, on ambiguity, pragmatics, legal ecosystems,
and the expressive function of law. It is divided into two parts and fifteen
sections. The first part (Pragmatics) addresses ambiguity from the perspective
of linguistic and cognitive pragmatics in the legal field. The second part
(Computing) deals with this issue from the point of view of human-centered
design and artificial intelligence, specifically focusing on the notion and
modelling of rules and what it means to comply with the rules. This is
necessary for the scaffolding of smart legal ecosystems (SLE). I will develop
this subject with the example of the architecture, information flows, and smart
ecosystem of OPTIMAI, an EU project of Industry 4.0 for zero-defect
manufacturing (Optimizing Manufacturing Processes through Artificial
Intelligence and Virtualization).",2024-06-07,cs.CY,
http://arxiv.org/abs/2406.05082v1,"Tuning-free long video diffusion has been proposed to generate
extended-duration videos with enriched content by reusing the knowledge from
pre-trained short video diffusion model without retraining. However, most works
overlook the fine-grained long-term video consistency modeling, resulting in
limited scene consistency (i.e., unreasonable object or background
transitions), especially with multiple text inputs. To mitigate this, we
propose the Consistency Noise Injection, dubbed CoNo, which introduces the
""look-back"" mechanism to enhance the fine-grained scene transition between
different video clips, and designs the long-term consistency regularization to
eliminate the content shifts when extending video contents through noise
prediction. In particular, the ""look-back"" mechanism breaks the noise
scheduling process into three essential parts, where one internal noise
prediction part is injected into two video-extending parts, intending to
achieve a fine-grained transition between two video clips. The long-term
consistency regularization focuses on explicitly minimizing the pixel-wise
distance between the predicted noises of the extended video clip and the
original one, thereby preventing abrupt scene transitions. Extensive
experiments have shown the effectiveness of the above strategies by performing
long-video generation under both single- and multi-text prompt conditions. The
project has been available in https://wxrui182.github.io/CoNo.github.io/.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05080v1,"In the Vision-and-Language Navigation in Continuous Environments (VLN-CE)
task, the human user guides an autonomous agent to reach a target goal via a
series of low-level actions following a textual instruction in natural
language. However, most existing methods do not address the likely case where
users may make mistakes when providing such instruction (e.g. ""turn left""
instead of ""turn right""). In this work, we address a novel task of Interactive
VLN in Continuous Environments (IVLN-CE), which allows the agent to interact
with the user during the VLN-CE navigation to verify any doubts regarding the
instruction errors. We propose an Interactive Instruction Error Detector and
Localizer (I2EDL) that triggers the user-agent interaction upon the detection
of instruction errors during the navigation. We leverage a pre-trained module
to detect instruction errors and pinpoint them in the instruction by
cross-referencing the textual input and past observations. In such way, the
agent is able to query the user for a timely correction, without demanding the
user's cognitive load, as we locate the probable errors to a precise part of
the instruction. We evaluate the proposed I2EDL on a dataset of instructions
containing errors, and further devise a novel metric, the Success weighted by
Interaction Number (SIN), to reflect both the navigation performance and the
interaction effectiveness. We show how the proposed method can ask focused
requests for corrections to the user, which in turn increases the navigation
success, while minimizing the interactions.",2024-06-07,cs.RO,
http://arxiv.org/abs/2406.05079v1,"No existing dataset adequately tests how well language models can
incrementally update entity summaries - a crucial ability as these models
rapidly advance. The Incremental Entity Summarization (IES) task is vital for
maintaining accurate, up-to-date knowledge. To address this, we introduce
SUMIE, a fully synthetic dataset designed to expose real-world IES challenges.
This dataset effectively highlights problems like incorrect entity association
and incomplete information presentation. Unlike common synthetic datasets, ours
captures the complexity and nuances found in real-world data. We generate
informative and diverse attributes, summaries, and unstructured paragraphs in
sequence, ensuring high quality. The alignment between generated summaries and
paragraphs exceeds 96%, confirming the dataset's quality. Extensive experiments
demonstrate the dataset's difficulty - state-of-the-art LLMs struggle to update
summaries with an F1 higher than 80.4%. We will open source the benchmark and
the evaluation metrics to help the community make progress on IES tasks.",2024-06-07,cs.CL,
http://arxiv.org/abs/2406.05078v1,"Low Earth orbit (LEO) satellites have been envisioned as a significant
component of the sixth generation (6G) network architecture for achieving
ubiquitous coverage and seamless access. However, the implementation of LEO
satellites is largely restricted by the deployment of ground stations.
Inter-satellite links (ISLs) have been regarded as a promising technique to
fully exploit the potentials of LEO mega constellations by concatenating
multiple satellites to constitute an autonomous space network. In this article,
we present the merits of implementing ISLs in LEO mega constellations and the
representative applications empowered/inspired by ISLs. Moreover, we outline
several key technical challenges as well as potential solutions related to LEO
satellite networks with ISLs, including performance analysis for system design,
routing and load balancing, and resource allocation. Particularly, the
potential of using ISLs in enhancing in-flight connectivity is showcased with a
preliminary performance evaluation. Finally, some open issues are discussed to
inspire future research.",2024-06-07,cs.IT,
http://arxiv.org/abs/2406.05075v1,"Videos are more informative than images because they capture the dynamics of
the scene. By representing motion in videos, we can capture dynamic activities.
In this work, we introduce GPT-4 generated motion descriptions that capture
fine-grained motion descriptions of activities and apply them to three action
datasets. We evaluated several video-text models on the task of retrieval of
motion descriptions. We found that they fall far behind human expert
performance on two action datasets, raising the question of whether video-text
models understand motion in videos. To address it, we introduce a method of
improving motion understanding in video-text models by utilizing motion
descriptions. This method proves to be effective on two action datasets for the
motion description retrieval task. The results draw attention to the need for
quality captions involving fine-grained motion information in existing datasets
and demonstrate the effectiveness of the proposed pipeline in understanding
fine-grained motion during video-text retrieval.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05072v1,"Modeling dynamical systems, e.g. in climate and engineering sciences, often
necessitates solving partial differential equations. Neural operators are deep
neural networks designed to learn nontrivial solution operators of such
differential equations from data. As for all statistical models, the
predictions of these models are imperfect and exhibit errors. Such errors are
particularly difficult to spot in the complex nonlinear behaviour of dynamical
systems. We introduce a new framework for approximate Bayesian uncertainty
quantification in neural operators using function-valued Gaussian processes.
Our approach can be interpreted as a probabilistic analogue of the concept of
currying from functional programming and provides a practical yet theoretically
sound way to apply the linearized Laplace approximation to neural operators. In
a case study on Fourier neural operators, we show that, even for a discretized
input, our method yields a Gaussian closure--a structured Gaussian process
posterior capturing the uncertainty in the output function of the neural
operator, which can be evaluated at an arbitrary set of points. The method adds
minimal prediction overhead, can be applied post-hoc without retraining the
neural operator, and scales to large models and datasets. We showcase the
efficacy of our approach through applications to different types of partial
differential equations.",2024-06-07,cs.LG,
http://arxiv.org/abs/2406.05071v1,"We present Meta MMO, a collection of many-agent minigames for use as a
reinforcement learning benchmark. Meta MMO is built on top of Neural MMO, a
massively multiagent environment that has been the subject of two previous
NeurIPS competitions. Our work expands Neural MMO with several computationally
efficient minigames. We explore generalization across Meta MMO by learning to
play several minigames with a single set of weights. We release the
environment, baselines, and training code under the MIT license. We hope that
Meta MMO will spur additional progress on Neural MMO and, more generally, will
serve as a useful benchmark for many-agent generalization.",2024-06-07,cs.AI,
http://arxiv.org/abs/2406.05070v1,"The era characterized by an exponential increase in data has led to the
widespread adoption of data intelligence as a crucial task. Within the field of
data mining, frequent episode mining has emerged as an effective tool for
extracting valuable and essential information from event sequences. Various
algorithms have been developed to discover frequent episodes and subsequently
derive episode rules using the frequency function and anti-monotonicity
principles. However, currently, there is a lack of algorithms specifically
designed for mining episode rules that encompass user-specified query episodes.
To address this challenge and enable the mining of target episode rules, we
introduce the definition of targeted precise-positioning episode rules and
formulate the problem of targeted mining precise-positioning episode rules.
Most importantly, we develop an algorithm called Targeted Mining Precision
Episode Rules (TaMIPER) to address the problem and optimize it using four
proposed strategies, leading to significant reductions in both time and space
resource requirements. As a result, TaMIPER offers high accuracy and efficiency
in mining episode rules of user interest and holds promising potential for
prediction tasks in various domains, such as weather observation, network
intrusion, and e-commerce. Experimental results on six real datasets
demonstrate the exceptional performance of TaMIPER.",2024-06-07,cs.DB,
http://arxiv.org/abs/2406.05068v1,"Decision processes of computer vision models - especially deep neural
networks - are opaque in nature, meaning that these decisions cannot be
understood by humans. Thus, over the last years, many methods to provide
human-understandable explanations have been proposed. For image classification,
the most common group are saliency methods, which provide (super-)pixelwise
feature attribution scores for input images. But their evaluation still poses a
problem, as their results cannot be simply compared to the unknown ground
truth. To overcome this, a slew of different proxy metrics have been defined,
which are - as the explainability methods themselves - often built on intuition
and thus, are possibly unreliable. In this paper, new evaluation metrics for
saliency methods are developed and common saliency methods are benchmarked on
ImageNet. In addition, a scheme for reliability evaluation of such metrics is
proposed that is based on concepts from psychometric testing. The used code can
be found at
https://github.com/lelo204/ClassificationMetricsForImageExplanations .",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05064v1,"In this paper, we study multi-task structured bandit problem where the goal
is to learn a near-optimal algorithm that minimizes cumulative regret. The
tasks share a common structure and the algorithm exploits the shared structure
to minimize the cumulative regret for an unseen but related test task. We use a
transformer as a decision-making algorithm to learn this shared structure so as
to generalize to the test task. The prior work of pretrained decision
transformers like DPT requires access to the optimal action during training
which may be hard in several scenarios. Diverging from these works, our
learning algorithm does not need the knowledge of optimal action per task
during training but predicts a reward vector for each of the actions using only
the observed offline data from the diverse training tasks. Finally, during
inference time, it selects action using the reward predictions employing
various exploration strategies in-context for an unseen test task. Our model
outperforms other SOTA methods like DPT, and Algorithmic Distillation over a
series of experiments on several structured bandit problems (linear, bilinear,
latent, non-linear). Interestingly, we show that our algorithm, without the
knowledge of the underlying problem structure, can learn a near-optimal policy
in-context by leveraging the shared structure across diverse tasks. We further
extend the field of pre-trained decision transformers by showing that they can
leverage unseen tasks with new actions and still learn the underlying latent
structure to derive a near-optimal policy. We validate this over several
experiments to show that our proposed solution is very general and has wide
applications to potentially emergent online and offline strategies at test
time. Finally, we theoretically analyze the performance of our algorithm and
obtain generalization bounds in the in-context multi-task learning setting.",2024-06-07,cs.LG,
http://arxiv.org/abs/2406.05063v1,"With the emergence of large language models (LLMs), investigating if they can
surpass humans in areas such as emotion recognition and empathetic responding
has become a focal point of research. This paper presents a comprehensive study
exploring the empathetic responding capabilities of four state-of-the-art LLMs:
GPT-4, LLaMA-2-70B-Chat, Gemini-1.0-Pro, and Mixtral-8x7B-Instruct in
comparison to a human baseline. We engaged 1,000 participants in a
between-subjects user study, assessing the empathetic quality of responses
generated by humans and the four LLMs to 2,000 emotional dialogue prompts
meticulously selected to cover a broad spectrum of 32 distinct positive and
negative emotions. Our findings reveal a statistically significant superiority
of the empathetic responding capability of LLMs over humans. GPT-4 emerged as
the most empathetic, marking approximately 31% increase in responses rated as
""Good"" compared to the human benchmark. It was followed by LLaMA-2,
Mixtral-8x7B, and Gemini-Pro, which showed increases of approximately 24%, 21%,
and 10% in ""Good"" ratings, respectively. We further analyzed the response
ratings at a finer granularity and discovered that some LLMs are significantly
better at responding to specific emotions compared to others. The suggested
evaluation framework offers a scalable and adaptable approach for assessing the
empathy of new LLMs, avoiding the need to replicate this study's findings in
future research.",2024-06-07,cs.CL,
http://arxiv.org/abs/2406.05059v1,"Grasping is an important human activity that has long been studied in
robotics, computer vision, and cognitive science. Most existing works study
grasping from the perspective of synthesizing hand poses conditioned on 3D or
2D object representations. We propose GenHeld to address the inverse problem of
synthesizing held objects conditioned on 3D hand model or 2D image. Given a 3D
model of hand, GenHeld 3D can select a plausible held object from a large
dataset using compact object representations called object codes.The selected
object is then positioned and oriented to form a plausible grasp without
changing hand pose. If only a 2D hand image is available, GenHeld 2D can edit
this image to add or replace a held object. GenHeld 2D operates by combining
the abilities of GenHeld 3D with diffusion-based image editing. Results and
experiments show that we outperform baselines and can generate plausible held
objects in both 2D and 3D. Our experiments demonstrate that our method achieves
high quality and plausibility of held object synthesis in both 3D and 2D.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05055v1,"Large language models (LLMs) have demonstrated impressive performance on
reasoning tasks, which can be further improved through few-shot prompting
techniques. However, the current evaluation primarily focuses on carefully
constructed benchmarks and neglects the consideration of real-world reasoning
problems that present missing and contradictory conditions, known as
ill-defined problems. Our observations suggest that existing few-shot prompting
techniques are ineffective in such scenarios, often providing overconfident
answers or hallucination. To further study this problem, we develop a benchmark
called Problems with Missing and Contradictory conditions (PMC) and introduce
two novel metrics to evaluate the performance of few-shot prompting methods in
these scenarios. Our analysis using the PMC benchmark reveals a trade-off
dilemma between the performance of mathematical reasoning for well-defined
problems and the ability to recognize ill-defined problems. To address the
challenges posed by PMC, we propose a novel few-shot prompting method called
SMT-LIB Prompting (SLP), which utilizes the SMT-LIB language to model the
problems instead of solving them directly. Subsequently, a double-check solving
strategy checks the satisfiability and uniqueness of the solution and provides
final feedback. Extensive experiments demonstrate the superiority of our SLP
approach compared to existing few-shot prompting methods when dealing with
problems with missing and contradictory conditions. We will open-source our
benchmark and code to facilitate future research.",2024-06-07,cs.AI,
http://arxiv.org/abs/2406.05054v1,"Few-shot medical image segmentation has achieved great progress in improving
accuracy and efficiency of medical analysis in the biomedical imaging field.
However, most existing methods cannot explore inter-class relations among base
and novel medical classes to reason unseen novel classes. Moreover, the same
kind of medical class has large intra-class variations brought by diverse
appearances, shapes and scales, thus causing ambiguous visual characterization
to degrade generalization performance of these existing methods on unseen novel
classes. To address the above challenges, in this paper, we propose a
\underline{\textbf{P}}rototype correlation \underline{\textbf{M}}atching and
\underline{\textbf{C}}lass-relation \underline{\textbf{R}}easoning (i.e.,
\textbf{PMCR}) model. The proposed model can effectively mitigate false pixel
correlation matches caused by large intra-class variations while reasoning
inter-class relations among different medical classes. Specifically, in order
to address false pixel correlation match brought by large intra-class
variations, we propose a prototype correlation matching module to mine
representative prototypes that can characterize diverse visual information of
different appearances well. We aim to explore prototype-level rather than
pixel-level correlation matching between support and query features via optimal
transport algorithm to tackle false matches caused by intra-class variations.
Meanwhile, in order to explore inter-class relations, we design a
class-relation reasoning module to segment unseen novel medical objects via
reasoning inter-class relations between base and novel classes. Such
inter-class relations can be well propagated to semantic encoding of local
query features to improve few-shot segmentation performance. Quantitative
comparisons illustrates the large performance improvement of our model over
other baseline methods.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05053v1,"Generative AI and large language models hold great promise in enhancing
programming education by generating individualized feedback and hints for
learners. Recent works have primarily focused on improving the quality of
generated feedback to achieve human tutors' quality. While quality is an
important performance criterion, it is not the only criterion to optimize for
real-world educational deployments. In this paper, we benchmark language models
for programming feedback generation across several performance criteria,
including quality, cost, time, and data privacy. The key idea is to leverage
recent advances in the new paradigm of in-browser inference that allow running
these models directly in the browser, thereby providing direct benefits across
cost and data privacy. To boost the feedback quality of small models compatible
with in-browser inference engines, we develop a fine-tuning pipeline based on
GPT-4 generated synthetic data. We showcase the efficacy of fine-tuned
Llama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM's in-browser
inference engine on three different Python programming datasets. We will
release the full implementation along with a web app and datasets to facilitate
further research on in-browser language models.",2024-06-07,cs.LG,
http://arxiv.org/abs/2406.05045v1,"Second-order Recurrent Neural Networks (2RNNs) extend RNNs by leveraging
second-order interactions for sequence modelling. These models are provably
more expressive than their first-order counterparts and have connections to
well-studied models from formal language theory. However, their large parameter
tensor makes computations intractable. To circumvent this issue, one approach
known as MIRNN consists in limiting the type of interactions used by the model.
Another is to leverage tensor decomposition to diminish the parameter count. In
this work, we study the model resulting from parameterizing 2RNNs using the CP
decomposition, which we call CPRNN. Intuitively, the rank of the decomposition
should reduce expressivity. We analyze how rank and hidden size affect model
capacity and show the relationships between RNNs, 2RNNs, MIRNNs, and CPRNNs
based on these parameters. We support these results empirically with
experiments on the Penn Treebank dataset which demonstrate that, with a fixed
parameter budget, CPRNNs outperforms RNNs, 2RNNs, and MIRNNs with the right
choice of rank and hidden size.",2024-06-07,cs.LG,
http://arxiv.org/abs/2406.05041v1,"Radio Resource Management is a challenging topic in future 6G networks where
novel applications create strong competition among the users for the available
resources. In this work we consider the frequency scheduling problem in a
multi-user MIMO system. Frequency resources need to be assigned to a set of
users while allowing for concurrent transmissions in the same sub-band.
Traditional methods are insufficient to cope with all the involved constraints
and uncertainties, whereas reinforcement learning can directly learn
near-optimal solutions for such complex environments. However, the scheduling
problem has an enormous action space accounting for all the combinations of
users and sub-bands, so out-of-the-box algorithms cannot be used directly. In
this work, we propose a scheduler based on action-branching over sub-bands,
which is a deep Q-learning architecture with parallel decision capabilities.
The sub-bands learn correlated but local decision policies and altogether they
optimize a global reward. To improve the scaling of the architecture with the
number of sub-bands, we propose variations (Unibranch, Graph Neural
Network-based) that reduce the number of parameters to learn. The parallel
decision making of the proposed architecture allows to meet short inference
time requirements in real systems. Furthermore, the deep Q-learning approach
permits online fine-tuning after deployment to bridge the sim-to-real gap. The
proposed architectures are evaluated against relevant baselines from the
literature showing competitive performance and possibilities of online
adaptation to evolving environments.",2024-06-07,cs.NI,
http://arxiv.org/abs/2406.05039v1,"Referring multi-object tracking (RMOT) aims at detecting and tracking
multiple objects following human instruction represented by a natural language
expression. Existing RMOT benchmarks are usually formulated through manual
annotations, integrated with static regulations. This approach results in a
dearth of notable diversity and a constrained scope of implementation. In this
work, our key idea is to bootstrap the task of referring multi-object tracking
by introducing discriminative language words as much as possible. In specific,
we first develop Refer-KITTI into a large-scale dataset, named Refer-KITTI-V2.
It starts with 2,719 manual annotations, addressing the issue of class
imbalance and introducing more keywords to make it closer to real-world
scenarios compared to Refer-KITTI. They are further expanded to a total of
9,758 annotations by prompting large language models, which create 617
different words, surpassing previous RMOT benchmarks. In addition, the
end-to-end framework in RMOT is also bootstrapped by a simple yet elegant
temporal advancement strategy, which achieves better performance than previous
approaches. The source code and dataset is available at
https://github.com/zyn213/TempRMOT.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05038v1,"Recent advancements in sequence modeling have led to the development of the
Mamba architecture, noted for its selective state space approach, offering a
promising avenue for efficient long sequence handling. However, its application
in 3D shape generation, particularly at high resolutions, remains
underexplored. Traditional diffusion transformers (DiT) with self-attention
mechanisms, despite their potential, face scalability challenges due to the
cubic complexity of attention operations as input length increases. This
complexity becomes a significant hurdle when dealing with high-resolution voxel
sizes. To address this challenge, we introduce a novel diffusion architecture
tailored for 3D point clouds generation-Diffusion Mamba (DiM-3D). This
architecture forgoes traditional attention mechanisms, instead utilizing the
inherent efficiency of the Mamba architecture to maintain linear complexity
with respect to sequence length. DiM-3D is characterized by fast inference
times and substantially lower computational demands, quantified in reduced
Gflops, thereby addressing the key scalability issues of prior models. Our
empirical results on the ShapeNet benchmark demonstrate that DiM-3D achieves
state-of-the-art performance in generating high-fidelity and diverse 3D shapes.
Additionally, DiM-3D shows superior capabilities in tasks like 3D point cloud
completion. This not only proves the model's scalability but also underscores
its efficiency in generating detailed, high-resolution voxels necessary for
advanced 3D shape modeling, particularly excelling in environments requiring
high-resolution voxel sizes. Through these findings, we illustrate the
exceptional scalability and efficiency of the Diffusion Mamba framework in 3D
shape generation, setting a new standard for the field and paving the way for
future explorations in high-resolution 3D modeling technologies.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05036v1,"Time series forecasting has become an increasingly popular research area due
to its critical applications in various real-world domains such as traffic
management, weather prediction, and financial analysis. Despite significant
advancements, existing models face notable challenges, including the necessity
of manual hyperparameter tuning for different datasets, and difficulty in
effectively distinguishing signal from redundant features in data characterized
by strong seasonality. These issues hinder the generalization and practical
application of time series forecasting models. To solve this issues, we propose
an innovative time series forecasting model TimeSieve designed to address these
challenges. Our approach employs wavelet transforms to preprocess time series
data, effectively capturing multi-scale features without the need for
additional parameters or manual hyperparameter tuning. Additionally, we
introduce the information bottleneck theory that filters out redundant features
from both detail and approximation coefficients, retaining only the most
predictive information. This combination reduces significantly improves the
model's accuracy. Extensive experiments demonstrate that our model outperforms
existing state-of-the-art methods on 70\% of the datasets, achieving higher
predictive accuracy and better generalization across diverse datasets. Our
results validate the effectiveness of our approach in addressing the key
challenges in time series forecasting, paving the way for more reliable and
efficient predictive models in practical applications. The code for our model
is available at https://github.com/xll0328/TimeSieve.",2024-06-07,cs.LG,
http://arxiv.org/abs/2406.05035v1,"Large language models (LLMs) can be used to generate natural language
explanations (NLE) that are adapted to different users' situations. However,
there is yet to be a quantitative evaluation of the extent of such adaptation.
To bridge this gap, we collect a benchmarking dataset, Situation-Based
Explanation. This dataset contains 100 explanandums. Each explanandum is paired
with explanations targeted at three distinct audience types-such as educators,
students, and professionals-enabling us to assess how well the explanations
meet the specific informational needs and contexts of these diverse groups e.g.
students, teachers, and parents. For each ""explanandum paired with an audience""
situation, we include a human-written explanation. These allow us to compute
scores that quantify how the LLMs adapt the explanations to the situations. On
an array of pretrained language models with varying sizes, we examine three
categories of prompting methods: rule-based prompting, meta-prompting, and
in-context learning prompting. We find that 1) language models can generate
prompts that result in explanations more precisely aligned with the target
situations, 2) explicitly modeling an ""assistant"" persona by prompting ""You are
a helpful assistant..."" is not a necessary prompt technique for situated NLE
tasks, and 3) the in-context learning prompts only can help LLMs learn the
demonstration template but can't improve their inference performance. SBE and
our analysis facilitate future research towards generating situated natural
language explanations.",2024-06-07,cs.CL,
http://arxiv.org/abs/2406.05027v1,"Computing Jacobians with automatic differentiation is ubiquitous in many
scientific domains such as machine learning, computational fluid dynamics,
robotics and finance. Even small savings in the number of computations or
memory usage in Jacobian computations can already incur massive savings in
energy consumption and runtime. While there exist many methods that allow for
such savings, they generally trade computational efficiency for approximations
of the exact Jacobian. In this paper, we present a novel method to optimize the
number of necessary multiplications for Jacobian computation by leveraging deep
reinforcement learning (RL) and a concept called cross-country elimination
while still computing the exact Jacobian. Cross-country elimination is a
framework for automatic differentiation that phrases Jacobian accumulation as
ordered elimination of all vertices on the computational graph where every
elimination incurs a certain computational cost. We formulate the search for
the optimal elimination order that minimizes the number of necessary
multiplications as a single player game which is played by an RL agent. We
demonstrate that this method achieves up to 33% improvements over
state-of-the-art methods on several relevant tasks taken from diverse domains.
Furthermore, we show that these theoretical gains translate into actual runtime
improvements by providing a cross-country elimination interpreter in JAX that
can efficiently execute the obtained elimination orders.",2024-06-07,cs.LG,
http://arxiv.org/abs/2406.05023v1,"Generative adversarial networks (GANs) are machine learning models that are
used to estimate the underlying statistical structure of a given dataset and as
a result can be used for a variety of tasks such as image generation or anomaly
detection. Despite their initial simplicity, designing an effective loss
function for training GANs remains challenging, and various loss functions have
been proposed aiming to improve the performance and stability of the generative
models. In this study, loss function design for GANs is presented as an
optimization problem solved using the genetic programming (GP) approach.
Initial experiments were carried out using small Deep Convolutional GAN (DCGAN)
model and the MNIST dataset, in order to search experimentally for an improved
loss function. The functions found were evaluated on CIFAR10, with the best
function, named GANetic loss, showing exceptionally better performance and
stability compared to the losses commonly used for GAN training. To further
evalute its general applicability on more challenging problems, GANetic loss
was applied for two medical applications: image generation and anomaly
detection. Experiments were performed with histopathological, gastrointestinal
or glaucoma images to evaluate the GANetic loss in medical image generation,
resulting in improved image quality compared to the baseline models. The
GANetic Loss used for polyp and glaucoma images showed a strong improvement in
the detection of anomalies. In summary, the GANetic loss function was evaluated
on multiple datasets and applications where it consistently outperforms
alternative loss functions. Moreover, GANetic loss leads to stable training and
reproducible results, a known weak spot of GANs.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05021v1,"Detecting counterfeits on online marketplaces is challenging, and current
methods struggle with the volume of sales on platforms like eBay, while
cryptomarkets openly sell counterfeits. Leveraging information from 453
cryptomarket counterfeits, we automated a search for corresponding products on
eBay, utilizing image and text similarity metrics. We collected data twice over
4-months to analyze changes with an average of 159 eBay products per
cryptomarket item, totaling 134k products. We found identical products, which
would warrant further investigation as to whether they are counterfeits.
Results indicate increasing difficulty finding similar products over time,
moderated by product type and origin. Future improved versions of the current
system could be used to examine possible connections between cryptomarket and
surface web listings more closely and could hold practical value in supporting
the detection of counterfeits on the surface web.",2024-06-07,cs.SI,
http://arxiv.org/abs/2406.05020v1,"Modeling real-world problems with partial differential equations (PDEs) is a
prominent topic in scientific machine learning. Classic solvers for this task
continue to play a central role, e.g. to generate training data for deep
learning analogues. Any such numerical solution is subject to multiple sources
of uncertainty, both from limited computational resources and limited data
(including unknown parameters). Gaussian process analogues to classic PDE
simulation methods have recently emerged as a framework to construct fully
probabilistic estimates of all these types of uncertainty. So far, much of this
work focused on theoretical foundations, and as such is not particularly data
efficient or scalable. Here we propose a framework combining a discretization
scheme based on the popular Finite Volume Method with complementary numerical
linear algebra techniques. Practical experiments, including a spatiotemporal
tsunami simulation, demonstrate substantially improved scaling behavior of this
approach over previous collocation-based techniques.",2024-06-07,cs.LG,
http://arxiv.org/abs/2406.05013v1,"In this paper, we study how open-source large language models (LLMs) can be
effectively deployed for improving query rewriting in conversational search,
especially for ambiguous queries. We introduce CHIQ, a two-step method that
leverages the capabilities of LLMs to resolve ambiguities in the conversation
history before query rewriting. This approach contrasts with prior studies that
predominantly use closed-source LLMs to directly generate search queries from
conversation history. We demonstrate on five well-established benchmarks that
CHIQ leads to state-of-the-art results across most settings, showing highly
competitive performances with systems leveraging closed-source LLMs. Our study
provides a first step towards leveraging open-source LLMs in conversational
search, as a competitive alternative to the prevailing reliance on commercial
LLMs. Data, models, and source code will be publicly available upon acceptance
at https://github.com/fengranMark/CHIQ.",2024-06-07,cs.IR,
http://arxiv.org/abs/2406.05011v1,"In the nonlinear timeseries analysis literature, countless quantities have
been presented as new ""entropy"" or ""complexity"" measures, often with similar
roles. The ever-increasing pool of such measures makes creating a sustainable
and all-encompassing software for them difficult both conceptually and
pragmatically. Such a software however would be an important tool that can aid
researchers make an informed decision of which measure to use and for which
application, as well as accelerate novel research. Here we present
ComplexityMeasures.jl, an easily extendable and highly performant open-source
software that implements a vast selection of complexity measures. The software
provides 1530 measures with 3,834 lines of source code, averaging only 2.5
lines of code per exported quantity (version 3.5). This is made possible by its
mathematically rigorous composable design. In this paper we discuss the
software design and demonstrate how it can accelerate complexity-related
research in the future. We carefully compare it with alternative software and
conclude that ComplexityMeasures.jl outclasses the alternatives in several
objective aspects of comparison, such as computational performance, overall
amount of measures, reliability, and extendability. ComplexityMeasures.jl is
also a component of the DynamicalSystems.jl library for nonlinear dynamics and
nonlinear timeseries analysis and follows open source development practices for
creating a sustainable community of developers.",2024-06-07,cs.SE,
http://arxiv.org/abs/2406.05006v1,"Deep learning models can perform well when evaluated on images from the same
distribution as the training set. However, applying small perturbations in the
forms of noise, artifacts, occlusions, blurring, etc. to a model's input image
and feeding the model with out-of-distribution (OOD) data can significantly
drop the model's accuracy, making it not applicable to real-world scenarios.
Data augmentation is one of the well-practiced methods to improve model
robustness against OOD data; however, examining which augmentation type to
choose and how it affects the OOD robustness remains understudied. There is a
growing belief that augmenting datasets using data augmentations that improve a
model's bias to shape-based features rather than texture-based features results
in increased OOD robustness for Convolutional Neural Networks trained on the
ImageNet-1K dataset. This is usually stated as ``an increase in the model's
shape bias results in an increase in its OOD robustness"". Based on this
hypothesis, some works in the literature aim to find augmentations with higher
effects on model shape bias and use those for data augmentation. By evaluating
39 types of data augmentations on a widely used OOD dataset, we demonstrate the
impact of each data augmentation on the model's robustness to OOD data and
further show that the mentioned hypothesis is not true; an increase in shape
bias does not necessarily result in higher OOD robustness. By analyzing the
results, we also find some biases in the ImageNet-1K dataset that can easily be
reduced using proper data augmentation. Our evaluation results further show
that there is not necessarily a trade-off between in-domain accuracy and OOD
robustness, and choosing the proper augmentations can help increase both
in-domain accuracy and OOD robustness simultaneously.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.05003v1,"Collaborative robots and machine learning-based virtual agents are
increasingly entering the human workspace with the aim of increasing
productivity and enhancing safety. Despite this, we show in a ubiquitous
experimental domain, Overcooked-AI, that state-of-the-art techniques for
human-machine teaming (HMT), which rely on imitation or reinforcement learning,
are brittle and result in a machine agent that aims to decouple the machine and
human's actions to act independently rather than in a synergistic fashion. To
remedy this deficiency, we develop HMT approaches that enable iterative,
mixed-initiative team development allowing end-users to interactively reprogram
interpretable AI teammates. Our 50-subject study provides several findings that
we summarize into guidelines. While all approaches underperform a simple
collaborative heuristic (a critical, negative result for learning-based
methods), we find that white-box approaches supported by interactive
modification can lead to significant team development, outperforming white-box
approaches alone, and black-box approaches are easier to train and result in
better HMT performance highlighting a tradeoff between explainability and
interactivity versus ease-of-training. Together, these findings present three
important directions: 1) Improving the ability to generate collaborative agents
with white-box models, 2) Better learning methods to facilitate collaboration
rather than individualized coordination, and 3) Mixed-initiative interfaces
that enable users, who may vary in ability, to improve collaboration.",2024-06-07,cs.RO,
http://arxiv.org/abs/2406.05000v1,"Recent advances in text-to-image models have enabled high-quality
personalized image synthesis of user-provided concepts with flexible textual
control. In this work, we analyze the limitations of two primary techniques in
text-to-image personalization: Textual Inversion and DreamBooth. When
integrating the learned concept into new prompts, Textual Inversion tends to
overfit the concept, while DreamBooth often overlooks it. We attribute these
issues to the incorrect learning of the embedding alignment for the concept. We
introduce AttnDreamBooth, a novel approach that addresses these issues by
separately learning the embedding alignment, the attention map, and the subject
identity in different training stages. We also introduce a cross-attention map
regularization term to enhance the learning of the attention map. Our method
demonstrates significant improvements in identity preservation and text
alignment compared to the baseline methods.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.04999v1,"In this work, we introduce ProMotion, a unified prototypical framework
engineered to model fundamental motion tasks. ProMotion offers a range of
compelling attributes that set it apart from current task-specific paradigms.
We adopt a prototypical perspective, establishing a unified paradigm that
harmonizes disparate motion learning approaches. This novel paradigm
streamlines the architectural design, enabling the simultaneous assimilation of
diverse motion information. We capitalize on a dual mechanism involving the
feature denoiser and the prototypical learner to decipher the intricacies of
motion. This approach effectively circumvents the pitfalls of ambiguity in
pixel-wise feature matching, significantly bolstering the robustness of motion
representation. We demonstrate a profound degree of transferability across
distinct motion patterns. This inherent versatility reverberates robustly
across a comprehensive spectrum of both 2D and 3D downstream tasks. Empirical
results demonstrate that ProMotion outperforms various well-known specialized
architectures, achieving 0.54 and 0.054 Abs Rel error on the Sintel and KITTI
depth datasets, 1.04 and 2.01 average endpoint error on the clean and final
pass of Sintel flow benchmark, and 4.30 F1-all error on the KITTI flow
benchmark. For its efficacy, we hope our work can catalyze a paradigm shift in
universal models in computer vision.",2024-06-07,cs.CV,
http://arxiv.org/abs/2406.04998v1,"Many machine learning models are susceptible to adversarial attacks, with
decision-based black-box attacks representing the most critical threat in
real-world applications. These attacks are extremely stealthy, generating
adversarial examples using hard labels obtained from the target machine
learning model. This is typically realized by optimizing perturbation
directions, guided by decision boundaries identified through query-intensive
exact search, significantly limiting the attack success rate. This paper
introduces a novel approach using the Approximation Decision Boundary (ADB) to
efficiently and accurately compare perturbation directions without precisely
determining decision boundaries. The effectiveness of our ADB approach (ADBA)
hinges on promptly identifying suitable ADB, ensuring reliable differentiation
of all perturbation directions. For this purpose, we analyze the probability
distribution of decision boundaries, confirming that using the distribution's
median value as ADB can effectively distinguish different perturbation
directions, giving rise to the development of the ADBA-md algorithm. ADBA-md
only requires four queries on average to differentiate any pair of perturbation
directions, which is highly query-efficient. Extensive experiments on six
well-known image classifiers clearly demonstrate the superiority of ADBA and
ADBA-md over multiple state-of-the-art black-box attacks.",2024-06-07,cs.LG,
http://arxiv.org/abs/2401.17506v1,"We develop a theory of the propagation and focusing of the THz Gaussian laser
beam through the layered superconductor slab of finite thickness in the
presence of an external DC magnetic field in a nonlinear regime. We show that,
in this case, focusing of radiation, which results from the specific
nonlinearity of the medium, can be flexibly tuned by the external magnetic
field providing a new way to control THz waves. We analytically study the main
characteristics of the Gaussian beam, its waist, and the focusing distance as
the functions of wave frequency, amplitude, and the external magnetic field.
The results of analytic calculations are supported by the numerical simulation
of the electromagnetic field distribution in the slab.",2024-01-30,cond-mat.supr-con,
http://arxiv.org/abs/2401.17503v1,"The entanglement distribution networks with various topologies are mainly
implemented by active wavelength multiplexing routing strategies. However,
designing an entanglement routing scheme, which achieves the maximized network
connections and the optimal overall network efficiency simultaneously, remains
a huge challenge for quantum networks. In this article, we propose a
differentiated service entanglement routing (DSER) scheme, which firstly finds
out the lowest loss paths and supported wavelength channels with the
tensor-based path searching algorithm, and then allocates the paired channels
with the differentiated routing strategies. The evaluation results show that
the proposed DSER scheme can be performed for constructing various large scale
quantum networks.",2024-01-30,quant-ph,
http://arxiv.org/abs/2402.00077v1,"Cancer is a complex disease driven by genomic alterations, and tumor
sequencing is becoming a mainstay of clinical care for cancer patients. The
emergence of multi-institution sequencing data presents a powerful resource for
learning real-world evidence to enhance precision oncology. GENIE BPC, led by
the American Association for Cancer Research, establishes a unique database
linking genomic data with clinical information for patients treated at multiple
cancer centers. However, leveraging such multi-institutional sequencing data
presents significant challenges. Variations in gene panels result in loss of
information when the analysis is conducted on common gene sets. Additionally,
differences in sequencing techniques and patient heterogeneity across
institutions add complexity. High data dimensionality, sparse gene mutation
patterns, and weak signals at the individual gene level further complicate
matters. Motivated by these real-world challenges, we introduce the Bridge
model. It uses a quantile-matched latent variable approach to derive integrated
features to preserve information beyond common genes and maximize the
utilization of all available data while leveraging information sharing to
enhance both learning efficiency and the model's capacity to generalize. By
extracting harmonized and noise-reduced lower-dimensional latent variables, the
true mutation pattern unique to each individual is captured. We assess the
model's performance and parameter estimation through extensive simulation
studies. The extracted latent features from the Bridge model consistently excel
in predicting patient survival across six cancer types in GENIE BPC data.",2024-01-30,q-bio.GN,
http://arxiv.org/abs/2401.17495v1,"Noise in various interferometer systems can sometimes couple non-linearly to
create excess noise in the gravitational wave (GW) strain data. Third-order
statistics, such as bicoherence and biphase, can identify these couplings and
help discriminate those occurrences from astrophysical GW signals. However, the
conventional analysis can yield large bicoherence values even when no
phase-coupling is present, thereby, resulting in false identifications.
Introducing artificial phase randomization in computing the bicoherence reduces
such occurrences with negligible impact on its effectiveness for detecting true
phase-coupled disturbances. We demonstrate this property with simulated
disturbances in this work. Statistical hypothesis testing is used for
distinguishing phase-coupled disturbances from non-phase coupled ones when
employing the phase-randomized bicoherence. We also obtain an expression for
the bicoherence value that minimizes the sum of the probabilities of false
positives and false negatives. This can be chosen as a threshold for
shortlisting bicoherence triggers for further scrutiny for the presence of
non-linear coupling. Finally, the utility of the phase-randomized bicoherence
analysis in GW time-series data is demonstrated for the following three
scenarios: (1) Finding third-order statistical similarities within categories
of noise transients, such as blips and koi fish. If these non-Gaussian noise
transients, or glitches, have a common source, their bicoherence maps can have
similarities arising from common bifrequencies related to that source. (2)
Differentiating linear or non-linear phase-coupled glitches from compact binary
coalescence signals through their bicoherence maps. This is explained with a
simulated signal. (3) Identifying repeated bifrequencies in the second and
third observation runs (i.e., O2 and O3) of LIGO and Virgo.",2024-01-30,gr-qc,
http://arxiv.org/abs/2401.17493v1,"We present our work on scalable, GPU-accelerated algorithms for diffeomorphic
image registration. The associated software package is termed CLAIRE. Image
registration is a non-linear inverse problem. It is about computing a spatial
mapping from one image of the same object or scene to another. In diffeomorphic
image registration, the set of admissible spatial transformations is restricted
to maps that are smooth, one-to-one, and have a smooth inverse. We formulate
diffeomorphic image registration as a variational problem governed by transport
equations. We use an inexact, globalized (Gauss--)Newton--Krylov method for
numerical optimization. We consider semi-Lagrangian methods for numerical time
integration. Our solver features mixed-precision, hardware-accelerated
computational kernels for optimal computational throughput. We use the
message-passing interface for distributed-memory parallelism and deploy our
code on modern high-performance computing architectures. Our solver allows us
to solve clinically relevant problems in under four seconds on a single GPU. It
can also be applied to large-scale 3D imaging applications with data that is
discretized on meshes with billions of voxels. We demonstrate that our
numerical framework yields high-fidelity results in only a few seconds, even if
we search for an optimal regularization parameter.",2024-01-30,math.OC,
http://arxiv.org/abs/2401.17492v1,"Angle-resolved photoemission spectra of alkali metals exhibit a puzzling,
non-dispersing peak in the apparent density of states near the Fermi energy. We
argue that the holes left behind a significant fraction of photoejected
electrons are not wavepacket-like objects used to describe excitations of an
equilibrium Fermi liquid but, instead, are relatively localized entities
resulting from a photon-induced cavitation in the electron fluid. At the same
time, these special localized holes can be thought of as vacancies in a
transient Wigner solid. The corresponding contribution to the photoemission
current is non-dispersive and is tied to the Fermi level; it exhibits certain
similarities to photoemission from localized core orbitals such as the presence
of recoil currents. Calculated spectra are consistent with experiment. We
briefly discuss the present findings in the context of quantum measurement.",2024-01-30,cond-mat.str-el,
http://arxiv.org/abs/2401.17487v1,"The integration of a Verticle Cavity Surface-Emitting Laser (VCSEL) array and
a driving Application-Specific Integrated Circuit (ASIC) in a custom optical
array transmitter module (ATx) for operation in the detector front-end is
constructed, assembled and tested. The ATx provides 12 parallel channels with
each channel operating at 10 Gbps. The optical transmitter eye diagram passes
the eye mask and the bit-error rate (BER) less than 1E-12 transmission is
achieved at 10 Gbps/ch. The overall insertion loss including the radiation
induced attenuation is sufficiently low to meet the proposed link budget
requirement.",2024-01-30,physics.ins-det,
http://arxiv.org/abs/2401.17485v1,"The next generation of gaseous photon detectors is requested to overcome the
limitations of the available technology, in terms of resolution and robustness.
The quest for a novel photocathode, sensitive in the far vacuum ultra violet
wavelength range and more robust than present ones, motivated an R&D programme
to explore nanodiamond based photoconverters, which represent the most
promising alternative to cesium iodine. A procedure for producing the novel
photocathodes has been defined and applied on THGEMs samples. Systematic
measurements of the photo emission in different Ar/CH4 and Ar/CO2 gas mixtures
with various types of nanodiamond powders have been performed. A comparative
study of the response of THGEMs before and after coating demonstrated their
full compatibility with the novel photocathodes.",2024-01-30,physics.ins-det,
http://arxiv.org/abs/2401.17307v1,"The study of the entire population of SNRs in a galaxy helps us to understand
the underlying stellar populations, the environments, in which the SNRs are
evolving, and the stellar feedback on the ISM. The all-sky survey carried out
by the extended Roentgen Survey with an Imaging Telescope Array (eROSITA) on
board Spektrum-Roentgen-Gamma (Spektr-RG, SRG) has provided us with spatially
and spectrally resolved X-ray data of the entire Large Magellanic Cloud (LMC)
and its immediate surroundings in the soft X-ray band down to 0.2 keV. We
performed a multiwavelength analysis of previously known SNR candidates and
newly detected SNRs and SNR candidates. We applied the Gaussian gradient
magnitude (GGM) filter to the eROSITA images of the LMC to highlight the edges
of the shocked gas in order to find new SNRs. We compared the X-ray images with
those of their optical and radio counterparts to investigate the true nature of
the extended emission. We used the Magellanic Cloud Emission Line Survey
(MCELS) for the optical data. For the radio comparison, we used data from the
Australian Square Kilometre Array Pathfinder (ASKAP) survey of the LMC. Using
the VISTA survey of the Magellanic Clouds (VMC) we have investigated the
possible progenitors of the new SNRs and SNR candidates in our sample. We
present the most updated catalogue of SNRs in the LMC. The eROSITA data have
allowed us to confirm two of the previous SNR candidates and discover 16 new
extended sources. We confirm 3 of them as new SNRs, while we propose the
remaining 13 as new X-ray SNR candidates. We also present the first analysis of
the follow-up XMM-Newton observation of MCSNR J0456-6533 discovered with
eROSITA. Among the new candidates, we propose J0614-7251 (4eRASSU
J061438.1-725112) as the first X-ray SNR candidate in the outskirts of the LMC.",2024-01-30,astro-ph.HE,
http://arxiv.org/abs/2401.17304v1,"Aims. Compact white-dwarf binaries are selected from spectra obtained in the
early SDSS-V plate program. A dedicated set of SDSS plate observations were
carried out in the eFEDS field, providing spectroscopic classifications for a
significant fraction of the optically bright end (r < 22.5) of the X-ray
sample. The identification and subclassification rests on visual inspections of
the SDSS spectra, spectral variability, color-magnitude and color-color
diagrams involving optical and X-ray fluxes, optical variability and literature
work. Results. Upon visual inspection of SDSS spectra and various auxiliary
data products we have identified 26 accreting compact white-dwarf binaries
(aCWDBs) in eFEDS, of which 24 are proven X-ray emitters. Among those 26
objects are 12 dwarf novae, three WZ Sge-like disk-accreting non-magnetic CVs
with low accretion rates, five likely non-magnetic high accretion rate novalike
CVs, two magnetic CVs of the polar subcategory, and three double degenerates
(AM CVn objects). Period bouncing candidates and magnetic systems are rarer
than expected in this sample, but it is too small for a thorough statistical
analysis. Fourteen of the systems are new discoveries, of which five are
fainter than the Gaia magnitude limit. Thirteen aCWDBs have measured or
estimated orbital periods, of which five were presented here. Through a Zeeman
analysis we revise the magnetic field estimate of the polar system J0926+0105,
which is likely a low-field polar at B = 16 MG. We quantify the success of
X-ray versus optical/UV selection of compact white-dwarf binaries which will be
relevant for the full SDSS-V survey. We also identify six white-dwarf
main-sequence (WDMS) systems, among them one confirmed pre-CV at an orbital
period of 17.6 hours and another pre-CV candidate.",2024-01-30,astro-ph.SR,
http://arxiv.org/abs/2401.17303v2,"The high-energy environment of the host stars could be deleterious for their
planets. It is crucial to ascertain this contextual information to fully
characterize the atmospheres of terrestrial exoplanets. We aim to fully
characterize a unique triple system, LTT1445, with three known rocky exoplanets
around LTT 1445A. The X-ray irradiation and flaring of this system are studied
through a new 50 ks Chandra observation, which is divided into 10 ks, 10 ks,
and 30 ks segments conducted two days apart, and two months apart,
respectively. This is complemented by an archival Chandra observation
approximately one year earlier and repeated observations with eROSITA (extended
ROentgen Survey with an Imaging Telescope Array), the soft X-ray instrument on
the Spectrum-Roentgen-Gamma (SRG) mission, enabling the investigation of X-ray
flux behavior across multiple time scales. With the observed X-ray flux from
the exoplanet host star A, we estimate the photo-evaporation mass loss of each
exoplanet. With the planet modeling package, VPLanet, we predict the evolution
and anticipated current atmospheric conditions. Our Chandra observations
indicate LTT 1445C as the dominant X-ray source, with additional contribution
from LTT 1445B. LTT 1445A, a slowly-rotating star, exhibits no significant
flare activity in the new Chandra dataset. Comparing the flux incident on the
exoplanets, LTT 1445BC components do not pose a greater threat to the planets
orbiting LTT 1445A than the emission from A itself. According to the results
from the simulation, LTT 1445Ad might have the capacity to retain its water
surface.",2024-01-30,astro-ph.EP,
http://arxiv.org/abs/2401.17302v1,"The planet GJ 367 b is a recently discovered high-density sub-Earth orbiting
an M dwarf star. Its composition was modelled to be predominantly iron with a
potential remainder of a hydrogen-helium envelope. Here we report an X-ray
detection of this planet's host star for the first time, using data from the
spectro-imaging X-ray telescope eROSITA onboard the Spectrum-Roentgen-Gamma
(SRG) mission. We characterise the magnetic activity of the host star from the
X-ray data and estimate the present-day mass-loss rate of the potential
atmosphere of the planet driven by the high-energy irradiation. We find that
despite the very low activity level of the host star the potential mass loss
rate is so high that any atmospheric remainders would evaporate in about 15
million years. Since the activity level of the host star indicates that the
system is several Gigayears old, it is very unlikely that the planet currently
still hosts any atmosphere.",2024-01-30,astro-ph.SR,
http://arxiv.org/abs/2401.17301v1,"Context: Environmental soft protons have affected the performance of the
X-ray detectors on board Chandra and XMM-Newton, and they pose a threat for
future high energy astrophysics missions with larger aperture, such as ATHENA.
  Aims: We aim at estimating soft proton fluxes at the ATHENA detectors
independently of any modelisation of the external fluxes in the space
environment.We analysed the background data measured by eROSITA on board SRG,
and with the help of simulations we defined a range of values for the potential
count-rate of quiet-time soft protons focused through the mirror shells. We
used it to derive an estimate of soft proton fluxes at the ATHENA detectors,
assuming ATHENA in the same L2-orbit as SRG.
  Results: The scaling, based on the computed proton transmission yields of the
optics and optical/thermal filters of eROSITA and ATHENA, indicates that the
soft proton induced WFI and X-IFU backgrounds could be expected close to the
requirement.
  Conclusions: No soft proton fluxes detrimental to the observations have been
suffered by eROSITA during its all-sky survey in orbit around L2. Regardless of
where ATHENA will be placed (L1 or L2), our analysis suggests that increasing
somewhat the thickness of the WFI optical blocking filter, e.g. by 30%, would
help to reduce the soft proton flux onto the detector, in case the planned
magnetic diverters perform worse than expected due to soft proton
neutralisation at the mirror level.",2024-01-30,astro-ph.IM,
http://arxiv.org/abs/2401.17298v1,"Context. Cataclysmic variables with degenerate donors which have evolved past
the period minimum, also known as period-bouncers, are predicted to make up a
great portion of the cataclysmic variable population, between 40 and 70
percent. However, either due to shortcomings in the models or due to the
intrinsic faintness of these strongly evolved systems, only a few have been
confidently identified so far. Aims. We have compiled a multi-wavelength
catalog of period-bouncers and cataclysmic variables around the period minimum
from the literature in order to provide an in-depth characterization of the
elusive subclass of period-bounce CVs that will help in the identification of
new candidates. Methods. In this study we combine published or archival
multi-wavelength data with new X-ray observations from the all-sky surveys
carried out with the extended ROentgen Survey with an Imaging Telescope Array
(eROSITA) onboard the Spektrum-Roentgen-Gamma spacecraft (SRG). Our catalog
comprises 192 cataclysmic variables around the period minimum that were chosen
as likely period bounce candidates based on reported short orbital periods and
low donor mass. This sample helped us establish specific selection parameters
that have been used to compile a scorecard which rates a systems likelihood of
being a period-bouncer. Results. Our scorecard correctly assigns high scores to
the already confirmed period-bouncers in our literature catalog and it
identifies 80 additional strong period-bounce candidates present in the
literature that have not been classified as such. We established two selection
cuts based on the X-ray-to-optical flux ratio and the typical X-ray luminosity
observed from the 8 already confirmed period-bouncers with eROSITA data. These
X-ray selection cuts led to the categorization of 5 systems as new
period-bouncers, increasing their population number to 22 systems.",2024-01-30,astro-ph.SR,
http://arxiv.org/abs/2401.17294v1,"Detecting a supernova remnant (SNR) along the Galactic plane can be quite
challenging. Any new detection reduces the discrepancy between the expected and
known number of remnants. In this paper, we present results from a large
selection of radio and X-ray data that cover the position of G321.3-3.9. We
identified G321.3-3.9 as a new SNR using data collected by several radio
surveys spanning a frequency range from 200 MHz to 2300 MHz. Stacked eROSITA
data from four consecutive all-sky surveys (eRASS:4) provide spectro-imaging
information in the energy band 0.2-8.0 keV. G321.3-3.9 has an elliptical shape
with major and minor axes of about 1.7 deg x 1.1 deg. From CHIPASS and S-PASS
data, we calculate a spectral index -0.8 +- 0.4, consistent with synchrotron
emission from an expanding shell in the Sedov Taylor phase. The eROSITA data
show an X-ray diffuse structure filling almost the entire radio shell.
Depending on the tested spectral model, a plasma temperature between 0.2 keV
(VAPEC) and 0.7 keV (NEI) can be fit. The X-ray spectral analysis allowed us to
estimate the column absorption towards G321.3-3.9, which suggests a remnant
distance of about 1 kpc by comparison with optical extinction maps.",2024-01-30,astro-ph.HE,
http://arxiv.org/abs/2401.17293v1,"Context. During four all-sky surveys (eRASS1--4), eROSITA, the soft X-ray
instrument aboard Spektrum-Roentgen-Gamma (SRG) detected a new supersoft X-ray
source, eRASSU J060839.5-704014, in the direction of the Large Magellanic Cloud
(LMC). Methods. We arranged follow-up observations in the X-ray and optical
wavelengths and further searched in archival observations to reveal the nature
of the object. Results. We discover pulsations at ~374 s with a pulse profile
consistent with 100% modulation. We identify two other periodicities in the
eROSITA data, which we establish as aliases due to the sampling of the eROSITA
light curve. We identify a multi-wavelength counterpart to the X-ray source in
UVW1 and g, r, i, and z images obtained by the optical/UV monitor on XMM-Newton
and the Dark Energy Camera at the Cerro Tololo Inter-American Observatory. The
timing and spectral characteristics of the source are consistent with a double
degenerate ultra-compact binary system in the foreground of the LMC. eRASSU
J060839.5-704014 belongs to a rare class of AM CVns, which are important to
study in the context of progenitors of SN Ia and for persistent gravitational
wave detection. Conclusions. We identify eRASSU J060839.5-704014 as a new
double degenerate ultra-compact binary located in the foreground of the LMC.",2024-01-30,astro-ph.HE,
http://arxiv.org/abs/2401.17292v1,"We aim at advancing our understanding of magnetic activity and the underlying
dynamo mechanism at the end of the main-sequence. To this end, we collected
simultaneous X-ray and radio observations for a sample of M7-L0 dwarfs using
XMM-Newton jointly with the JVLA and the ATCA. We also included data from the
all-sky surveys of eROSITA on board the Russian Spektrum-Roentgen-Gamma mission
(SRG) and rotation periods from TESS together with an archival compilation of
X-ray and radio data for UCDs from the literature. We limit the sample to
objects with rotation period <1d, focusing on the study of a transition in
magnetic behavior suggested by a drastic change in the radio detection rate at
vsini \approx 38 km/s. We compiled the most up-to-date radio/X-ray luminosity
relation for 26 UCDs with rotation periods lower than 1d, finding that rapid
rotators lie the furthest away from the G\""udel-Benz relation previously
studied for earlier-type stars. Radio bursts are mainly experienced by very
fast UCDs, while X-ray flares are seen along the whole range of rotation. We
examined the L_{\rm x}/L_{\rm bol} vs P_{\rm rot} relation, finding no evident
relation between the X-ray emission and rotation, reinforcing previous
speculations on a bimodal dynamo across late-type dwarfs. One radio-detected
object has a rotation period consistent with the range of auroral bursting
sources; while it displays moderately circularly polarized emission. A radio
flare from this object is interpreted as gyrosynchrotron emission, and it
displays X-ray and optical flares. We also found a slowly rotating apparent
auroral emitter, that is also one of the X-ray brightest radio-detected UCDs.
We speculate that this UCD is experiencing a transition of its magnetic
behavior since it produces signatures expected from higher mass M dwarfs along
with emerging evidence of auroral emission.",2024-01-30,astro-ph.SR,
http://arxiv.org/abs/2401.17288v2,"Massive black holes (MBHs) are typically hosted in the centres of massive
galaxies but they appear to become rarer in lower mass galaxies, where nuclear
star clusters (NSCs) frequently appear instead. The transition region, where
both an MBH and NSC can co-exist, has been poorly studied to date and only a
few dozen galaxies are known to host them. One avenue for detecting new
galaxies with both an MBH and NSC is to look for accretion signatures of MBHs.
Here, we use new SRG/eROSITA all-sky survey eRASS:4 data to search for X-ray
signatures of accreting MBHs in NSCs, while also investigating their combined
occupation fraction. We find significant detections for 18 galaxies (~8.3%),
including one ultra-luminous X-ray source; however, only three galaxies
(NGC2903, 4212, and 4639) have X-ray luminosities that are higher than the
expected value from X-ray binaries, indicative of the presence of an MBH. In
addition, the X-ray luminosity of six galaxies (NGC2903, 3384, 4321, 4365,
4639, and 4701) differs from previous studies and could indicate the presence
of a variable active galactic nucleus. The combined occupation fraction of
accreting MBHs and NSCs becomes non-zero for galaxy masses above ~10^7.5 M_sun
and this result is slightly elevated as compared to the literature data. Our
data extend, for the first time, towards the dwarf elliptical galaxy regime and
identify promising MBH candidates for higher resolution follow-up observations.
At most galaxy masses (and with the exception of three cases), the X-ray
constraints are consistent with the expected emission from binary systems or an
Eddington fraction of at most 0.01%, assuming a black holes mass of 10^6.5
M_sun. This work confirms the known complexities in similar-type of studies,
while providing the appealing alternative of using X-ray survey data of
in-depth observations of individual targets with higher resolution instruments.",2024-01-30,astro-ph.GA,
http://arxiv.org/abs/2401.17287v1,"We study variability through simultaneous optical and X-ray observations for
the first time in a statistical sample of 256 M dwarfs. Such observations are
required to constrain the flare frequency and energetics and to understand the
physics of flares. Using light curves from extended ROentgen Survey with an
Imaging Telescope Array (eROSITA) on board the Russian Spektrum-Roentgen-Gamma
mission (SRG) and the Transiting Exoplanet Survey Satellite (TESS), we identify
256 M dwarfs with simultaneous detections. The 25 brightest or most variable in
X-rays are selected. Stellar parameters are obtained from Gaia and 2MASS, while
X-ray fluxes are derived from eROSITA count rates. Proximity (<100 pc), fast
rotation (P_rot < 9 d), and high flare frequency characterize our sample.
Optical and X-ray duty cycles correlate positively, with faster rotators
exhibiting more variability. Stars with frequent X-ray flares often coincide
with optical flares. Analyzing individual X-ray flares is hindered by eROSITA's
low cadence, mitigated by leveraging TESS optical flares and solar flare
knowledge. An exponential fit to 21 X-ray light curves post-optical flares
reveals a correlation between X-ray and optical flare energies. Despite
uncertainties due to poor eROSITA sampling, our study underscores the
significance of simultaneous all-sky surveys in diverse wavelengths for
unprecedented insights into stellar variability.",2024-01-30,astro-ph.SR,
http://arxiv.org/abs/2401.17286v1,"We present an extensive radio monitoring campaign of the nuclear transient
eRASSt J234403-352640 with the Australia Telescope Compact Array, one of the
most X-ray luminous TDE candidates discovered by the SRG/eROSITA all-sky
survey. The observations reveal a radio flare lasting more than 1000 d,
coincident with the X-ray, UV, optical, and infra-red flare of this transient
event. Through modelling of the 10 epochs of radio spectral observations
obtained, we find that the radio emission is well-described by an expanding
synchrotron emitting region, consisting of a single ejection of material
launched coincident with the optical flare. We conclude that the radio flare
properties of eRASSt J234403-352640 are consistent with the population of
radio-emitting outflows launched by non-relativistic tidal disruption events,
and that the flare is likely due to an outflow launched by a tidal disruption
event (but could also be a due to a new AGN accretion event) in a previously
turned-off AGN.",2024-01-30,astro-ph.HE,
http://arxiv.org/abs/2401.17285v2,"Although flux variability is one of the defining properties of accretion
flows onto supermassive black holes, its dependence on physical parameters such
as the mass of the compact object and the Eddington ratio remain under
discussion. In this paper we address this issue using the structure function
statistic to measure the variability at X-ray wavelengths of a sample of
optically selected QSOs with available black hole masses and Eddington ratios.
We present a new Bayesian methodology for estimating the structure function
tailored to the Poisson nature of the X-ray data. This is applied to 15,548
SDSS DRQ16 QSOs with repeat observations in the XMM-Newton archive and/or the
SRG/eROSITA All Sky Survey. The X-ray structure function monotonically
increases to time intervals of about 10-15 years, suggesting a preference for
scenarios in which instabilities of the accretion disk contribute to the X-ray
variability on long timescales. Additionally, there is evidence that the
amplitude of the stochastic X-ray flux variations rises with decreasing black
hole mass and Eddington ratio. This finding imposes stringent constraints on
empirical models of Active Galactic Nuclei variability derived from local
samples, emphasizing the significance of high-redshift population studies for
comprehending the stochastic flux variations in active black holes.",2024-01-30,astro-ph.HE,
http://arxiv.org/abs/2401.17282v1,"The first eROSITA all-sky survey (eRASS1) performed on board the
Spectrum-Roentgen-Gamma mission (SRG) provides more than 900,000 X-ray sources
in the 0.2 - 2.3 keV band located in the western hemisphere. We present
identifications of the eRASS1 sources obtained using our HamStar method, which
was designed for the identification of coronal X-ray sources. HamStar is a
Bayesian framework that estimates coronal probabilities for each eRASS1 source
based on a cross-match with optical counterparts from Gaia DR3. It considers
geometric properties, such as angular separation and positional uncertainty, as
well the additional properties of fractional X-ray flux, color, and distance.
We identify 138,800 coronal eRASS1 sources and estimate a completeness and
reliability of about 91.5% for this sample, which we confirmed with Chandra
detections. This is the largest available sample of coronal X-ray emitters and
we find nearly five times as many coronal sources as in the ROSAT all-sky
survey. The coronal eRASS1 sources are made up of all spectral types and the
onset of convection and the saturation limit are clearly visible. As opposed to
previous samples, rare source types are also well populated. About 10% of the
coronal eRASS1 sources have a correlated secondary counterpart, which is a wide
binary companion or belongs to the same stellar cluster. We also identify 6700
known unresolved binaries, and an excess of fast binary periods below 10 d.
Furthermore, the binary sequence is clearly visible in a color-magnitude
diagram. When combining the coronal eRASS1 sources with rotation modulations
from Gaia DR3, we find 3700 X-ray sources with known rotation periods, which is
the largest sample of this kind. We fitted the rotation-activity relation and
convection turnover times for our flux-limited sample. We do not detect the
low-amplitude fast rotators discovered in the Gaia DR3 sample in X-rays.",2024-01-30,astro-ph.SR,
http://arxiv.org/abs/2401.17280v1,"The extended ROentgen Survey with an Imaging Telescope Array (eROSITA) on
board the Spectrum-Roentgen-Gamma (SRG) mission with its first All-Sky Survey
(eRASS1) has offered an unprecedented, comprehensive view of the variable X-ray
sky. With enhanced sensitivity, broader energy coverage, and improved
resolution compared to prior surveys, the eRASS1 Data Release 1 (DR1) catalogue
underwent a variability analysis, focusing on a substantial subset of 128,669
sources. We performed multiple variability tests, utilizing conventional
normalized excess variance, maximum amplitude variability, and Bayesian excess
variance methods. Among the 128,669 DR1 sources, our research identified 557
objects exhibiting variability through NEV and AMPLMAX tests. After applying
suitable thresholds, 108 sources demonstrated significant variability via NEV,
while 73 did so through AMPLMAX. The utilization of the bexvar method extended
our detection capabilities to lower count rates, unveiling a total of 1307
sources manifesting variability. Furthermore, our comparative analysis spanning
2.5 years encompassed observations from consecutive eROSITA surveys, eRASS2,
eRASS3, eRASS4, and eRASS5. Notably, the Gamma-ray burst afterglow GRB 200120A,
which was the most variable DR1 source, was as expected absent in subsequent
eROSITA survey scans. Observations of the Low-Mass X-ray Binary GX 339-4 across
various eROSITA survey scans unveiled substantial variability. These outbursts
involve the movement of the inner radius of the accretion disk, fluctuating
inward and outward. Combining eROSITA and MAXI data reveals that the most
effective tracer for monitoring the onset of the outbursts is the softest
eROSITA band. Magnetically active stars are commonly found among the more
variable X-ray sources. We analyzed the AGN sample to identify variability
patterns and instances of efficiency limit violations.",2024-01-30,astro-ph.HE,
http://arxiv.org/abs/2401.17278v1,"Aims: During its all-sky survey phase, the eROSITA X-ray telescope onboard
SRG scans through the ecliptic poles every 4 hours. This extensive data set of
long-duration, frequent, and consistent observations of thousands of X-ray
sources is ideal for a detailed long-term X-ray variability analysis. However,
individual observations are short, are separated by long but consistent gaps,
and have varying exposure times. Therefore, the identification of variable
sources, and the characterisation and quantification of their variability
requires a unique methodology. We aim to develop and evaluate such methods for
eROSITA observations, focusing on sources close to the survey poles.
  Methods: We simulate eROSITA-like light curves to evaluate and quantify the
effect of survey mode observations on the measured periodogram and normalised
excess variance. We introduce a new method for estimating the normalised
intrinsic variance of a source based on the Bayesian excess variance (bexvar)
method.
  Results: We determine thresholds for identifying likely variable sources
while minimising the false-positive rate, as a function of the number of bins,
and the average count rate in the light curve. The bexvar normalised intrinsic
variance estimate is significantly more accurate than the normalised excess
variance method in the Poisson regime. At high count rates, the two methods are
comparable. We quantify the scatter in the intrinsic variance of a stationary
pink noise process, and investigate how to reduce it. Finally, we determine a
description of the excess noise in a periodogram caused by varying exposure
times throughout a light curve. Although most of these methods were developed
specifically for analysing variable AGN in the eROSITA all-sky survey, they can
also be used for the variability analysis of other datasets from other
telescopes, with slight modifications.",2024-01-30,astro-ph.IM,
http://arxiv.org/abs/2401.17277v1,"Major mergers between galaxies are predicted to fuel their central
supermassive black holes (SMBHs), particularly after coalescence. However,
determining the prevalence of active galactic nuclei (AGNs) in mergers remains
a challenge, because AGN diagnostics are sensitive to details of the central
structure (e.g., nuclear gas clouds, geometry and orientation of a dusty torus)
that are partly decoupled from SMBH accretion. X-rays, expected to be
ubiquitous among accreting systems, are detectable through non-Compton-thick
screens of obscuring material, and thus offer the potential for a more complete
assessment of AGNs in mergers. But, extant statistical X-ray studies of AGNs in
mergers have been limited by either sparse, heterogeneous, or shallow on-sky
coverage. We use new X-ray observations from the first SRG/eROSITA all-sky data
release to characterize the incidence, luminosity, and observability of AGNs in
mergers. Combining machine learning and visual classification, we identify 923
post-mergers in Dark Energy Camera Legacy Survey (DECaLS) imaging and select
4,565 interacting galaxy pairs (with separations <120 kpc and mass ratios
within 1:10) from the Sloan Digital Sky Survey. We find that galaxies with
X-ray AGNs are 2.0+/-0.24 times as likely to be identified as post-mergers
compared to non-AGN controls, and that post-mergers are 1.8+/-0.1 times as
likely to host an X-ray AGN as non-interacting controls. A multi-wavelength
census of X-ray, optical, and mid-IR-selected AGNs suggests a picture wherein
the underlying AGN fraction increases during pair-phase interactions, that
galaxy pairs within ~20 kpc become heavily obscured, and that the obscuration
often clears post-coalescence.",2024-01-30,astro-ph.GA,
http://arxiv.org/abs/2401.17276v1,"We investigate the impact of AGN feedback, on the entropy and characteristic
temperature measurements of galaxy groups detected in the SRG/eROSITA's first
All-Sky Survey (eRASS1) to shed light on the characteristics of the feedback
mechanisms. We analyze deeper eROSITA observations of 1178 galaxy groups
detected in eRASS1. We divide the sample into 271 subsamples and extract
average thermodynamic properties, including electron density, temperature, and
entropy at three characteristic radii along with the integrated temperature by
jointly analyzing X-ray images and spectra following a Bayesian approach. We
present the tightest constraints on the impact of AGN feedback through our
average entropy and characteristic temperature measurements of the largest
group sample used in X-ray studies, incorporating major systematics in our
analysis. We find that entropy shows an increasing trend with temperature in
the form of a power-law-like relation at the higher intra-group medium
temperatures, while for the low mass groups, a slight flattening is observed on
the average entropy. Overall, the observed entropy measurements agree well with
the earlier measurements in the literature. The comparisons with the
state-of-the-art cosmological hydrodynamic simulations (MillenniumTNG,
Magneticum, OWL simulations) after the applications of the selection function
calibrated for our galaxy groups reveal that observed entropy profiles in the
cores are below the predictions of simulations. At the mid-region, the entropy
measurements agree well with the Magneticum simulations, whereas the
predictions of MillenniumTNG and OWL simulations fall below observations. At
the outskirts, the overall agreement between the observations and simulations
improves, with Magneticum simulations reproducing the observations the best.
Our measurements will pave the way for more realistic AGN feedback
implementations in simulations.",2024-01-30,astro-ph.CO,
http://arxiv.org/abs/2401.17273v1,"We present the results of the first X-ray all-sky survey (eRASS1) performed
by the eROSITA instrument onboard the Spectrum-Roentgen-Gamma (SRG) mission on
X-ray emitting red giants and supergiants. Focussing on stars positioned at
high galactic latitudes above 20 deg, we construct a complete sample of such
objects using the Gaia DR3 catalog and identify a sample 96 stars appearing as
bona fide entries in the eRASS1 source catalog. Restricting again the sample to
objects nearer than 1300~pc and eliminating all catalog entries which are due
to optical contamination, we end up with a sample of 16 genuine red
giant/supergiant X-ray sources, which represent -- with the exception of one
source (CL~Hyi) -- new X-ray detections. We furthermore present a low SNR X-ray
spectrum of the nearby low activity giant Arcturus obtained from a pointed
observation with the XMM-Newton satellite and give a detailed account of our
data analysis. We show that Arcturus-like X-ray emission cannot be the
explanation for the X-ray emissions observed by eROSITA and provide a
discussion of the possible nature of the detected X-ray sources.",2024-01-30,astro-ph.SR,
http://arxiv.org/abs/2401.17479v2,"The Green's function of the discrete Sch\""odinger operator on a finite graph
is considered. This setting reproduces Laplacian and signless Laplacian by
adjusting appropriate potentials. We show two ways of the expression for the
Green's function by using graph structures. The first way is based on the
factor of the graph by subtrees which have uni-self-loops; the second way is
based on that by odd unicycle graphs.",2024-01-30,math-ph,
http://arxiv.org/abs/2401.17476v1,"We reformulate the time-independent Schr\""odinger equation as a Maurer-Cartan
equation on the superspace of eigensystems of the former equation. We then
twist the differential so that its cohomology becomes the space of solutions
with a set energy. A perturbation of the Hamiltonian corresponds to a
deformation of the twisted differential, leading to a simple recursive relation
for the eigenvalue and eigenfunction corrections.",2024-01-30,math-ph,
http://arxiv.org/abs/2401.17473v1,"In high-dimensional time series, the component processes are often assembled
into a matrix to display their interrelationship. We focus on detecting mean
shifts with unknown change point locations in these matrix time series. Series
that are activated by a change may cluster along certain rows (columns), which
forms mode-specific change point alignment. Leveraging mode-specific change
point alignments may substantially enhance the power for change point
detection. Yet, there may be no mode-specific alignments in the change point
structure. We propose a powerful test to detect mode-specific change points,
yet robust to non-mode-specific changes. We show the validity of using the
multiplier bootstrap to compute the p-value of the proposed methods, and derive
non-asymptotic bounds on the size and power of the tests. We also propose a
parallel bootstrap, a computationally efficient approach for computing the
p-value of the proposed adaptive test. In particular, we show the consistency
of the proposed test, under mild regularity conditions. To obtain the
theoretical results, we derive new, sharp bounds on Gaussian approximation and
multiplier bootstrap approximation, which are of independent interest for high
dimensional problems with diverging sparsity.",2024-01-30,stat.ME,
http://arxiv.org/abs/2401.17472v2,"It is well-known that decision-making problems from stochastic control can be
formulated by means of a forward-backward stochastic differential equation
(FBSDE). Recently, the authors of Ji et al. 2022 proposed an efficient deep
learning algorithm based on the stochastic maximum principle (SMP). In this
paper, we provide a convergence result for this deep SMP-BSDE algorithm and
compare its performance with other existing methods. In particular, by adopting
a strategy as in Han and Long 2020, we derive a-posteriori estimate, and show
that the total approximation error can be bounded by the value of the loss
functional and the discretization error. We present numerical examples for
high-dimensional stochastic control problems, both in case of drift- and
diffusion control, which showcase superior performance compared to existing
algorithms.",2024-01-30,math.OC,
http://arxiv.org/abs/2401.17471v1,"We present the design and test results of two optical data transmission ASICs
for the High-Luminosity LHC (HL-LHC) experiments. These ASICs include a
two-channel serializer (LOCs2) and a single-channel Vertical Cavity Surface
Emitting Laser (VCSEL) driver (LOCld1V2). Both ASICs are fabricated in a
commercial 0.25-um Silicon-on-Sapphire (SoS) CMOS technology and operate at a
data rate up to 8 Gbps per channel. The power consumption of LOCs2 and LOCld1V2
are 1.25 W and 0.27 W at 8-Gbps data rate, respectively. LOCld1V2 has been
verified meeting the radiation-tolerance requirements for HL-LHC experiments.",2024-01-30,physics.ins-det,
http://arxiv.org/abs/2401.17468v1,"In this work, we study the propagation of wildfires using an
advection--diffusion--reaction model which also includes convective and
radiative heat loss. An existing model is discussed \cite{asensio_2002} and a
physically consistent modification of the model is proposed. Using this, the
existence of travelling waves (TWs) in the one-dimensional case is
investigated. Prior numerical studies reveal the existence of TWs
\cite{reisch_2023}. Under the travelling wave ansatz and certain approximation,
the model is reduced to a semi-autonomous dynamical system with three unknowns
which can be analyzed by a shooting algorithm. It is hypothesized that under
mild wind speeds, TWs in both directions exist, and under strong tailwinds only
TWs in the direction of wind are possible. The theoretical implications are
investigated using both solvers for the PDE models and the shooting algorithm.
The results match, and unveil the dependence of the fronts on the parameters
consistent with the predictions.",2024-01-30,math.AP,
http://arxiv.org/abs/2401.17467v2,"A comprehensive understanding of human mobility patterns in urban areas is
essential for urban development and transportation planning. In this study, we
create entropy-based measurements to capture the geographical distribution
diversity of trip origins and destinations. Specifically, we develop
origin-entropy and destination-entropy based on taxi and ride-sharing trip
records. The origin-entropy for a given zone accounts for all the trips that
originate from this zone and calculates the level of geographical distribution
diversity of these trips destinations. Likewise, the destination-entropy for a
given zone considers all the trips that end in this zone and calculates the
level of geographical distribution diversity of these trips origins.
Furthermore, we have created an interactive geovisualization that enables
researchers to delve into and juxtapose the spatial and temporal dynamics of
origin and destination entropy, in conjunction with trip counts for both
origins and destinations. Results indicate that entropy-based measurements
effectively capture shifts in the diversity of trips geographical origins and
destinations, reflecting changes in travel decisions due to major events like
the COVID-19 pandemic. These measurements, alongside trip counts, offer a more
comprehensive understanding of urban human flows.",2024-01-30,physics.soc-ph,
http://arxiv.org/abs/2401.17466v1,"We theoretically investigate the single-particle excitation spectra of a
one-dimensional Hubbard model at half-filling using an infinite matrix-product
state and elucidate the discretized energy spectra emerging under the influence
of a dc electric field. In a weak electric-field regime, there are two kinds of
discretized spectral structures, which affect the spectra of the dynamical
current-current correlation function and the density of states. With increasing
the electric-field strength, the discretized spectra, whose period is
proportional to the strength, become dominant, and the density of states
exhibits the Wannier-Stark ladder in their spectra. To demonstrate how our
findings can be observed in experiments, we simulate time- and angle-resolved
photoemission spectroscopy using an ultrashort THz pump pulse. Our results
represent a significant step forward in understanding the states in strongly
correlated electron systems driven by a static electric field.",2024-01-30,cond-mat.str-el,
http://arxiv.org/abs/2401.17462v1,"Directed transport is a key concept for many ongoing applications including
nanoscale heat management, current rectification, source protection, and energy
harvesting. Within the context of quantum transport, we here explore the use of
nonlinear effects introduced by current-induced forces (CIFs) as a practical
way to effectively break charge and heat transport reciprocities. In
particular, we consider a simple model consisting of a mobile quantum dot (QD)
coupled to two leads, where the charge (or heat) current develops an asymmetric
behavior under inversion of voltage (or temperature) bias, thereby turning the
system into a quantum diode (or quantum thermal diode). Furthermore, we find
multiple stable positions for the QD and we show how the extraction of useful
work is possible by modulating the nonequilibrium sources along
well-established hysteresis loops. Finally, we explore a particular case where
the nonlinearity of the CIFs can be exploited to pump heat or charge, even for
systems that preserve inversion symmetry. This counterintuitive result is
attributed to a spontaneous breaking of the inversion symmetry due to the
intrinsic system's dynamics.",2024-01-30,cond-mat.mes-hall,
http://arxiv.org/abs/2401.17458v1,"In this article, we present a heuristic derivation of the on-shell equation
of the Lorentzian classicalized holographic tensor network in the presence of a
non-zero mass in the bulk spacetime. This derivation of the on-shell equation
is based on two physical assumptions. First, the Lorentzian bulk theory is in
the ground state. Second, the law of Lorentzian holographic gravity is
identified with the time-energy uncertainty principle. The arguments in this
derivation could lead to a novel picture of Lorentzian gravity as a quantum
mechanical time uncertainty based on the holographic principle and
classicalization.",2024-01-30,hep-th,
http://arxiv.org/abs/2401.17456v1,"The global shift toward electric vehicles (EVs) for climate sustainability
lacks comprehensive insights into the impact of the built environment on EV
ownership, especially in varying spatial contexts. This study, focusing on New
York State, integrates data fusion techniques across diverse datasets to
examine the influence of socioeconomic and built environmental factors on EV
ownership. The utilization of spatial regression models reveals consistent
coefficient values, highlighting the robustness of the results, with the
Spatial Lag model better at capturing spatial autocorrelation. Results
underscore the significance of charging stations within a 10-mile radius,
indicative of a preference for convenient charging options influencing EV
ownership decisions. Factors like higher education levels, lower rental
populations, and concentrations of older population align with increased EV
ownership. Utilizing publicly available data offers a more accessible avenue
for understanding EV ownership across regions, complementing traditional survey
approaches.",2024-01-30,stat.AP,
http://arxiv.org/abs/2401.17455v1,"When auditing a redistricting plan, a persuasive method is to compare the
plan with an ensemble of neutrally drawn redistricting plans. Ensembles are
generated via algorithms that sample distributions on balanced graph
partitions. To audit the partisan difference between the ensemble and a given
plan, one must ensure that the non-partisan criteria are matched so that we may
conclude that partisan differences come from bias rather than, for example,
levels of compactness or differences in community preservation. Certain
sampling algorithms allow one to explicitly state the policy-based probability
distribution on plans, however, these algorithms have shown poor mixing times
for large graphs (i.e. redistricting spaces) for all but a few specialized
measures. In this work, we generate a multiscale parallel tempering approach
that makes local moves at each scale. The local moves allow us to adopt a wide
variety of policy-based measures. We examine our method in the state of
Connecticut and succeed at achieving fast mixing on a policy-based distribution
that has never before been sampled at this scale. Our algorithm shows promise
to expand to a significantly wider class of measures that will (i) allow for
more principled and situation-based comparisons and (ii) probe for the typical
partisan impact that policy can have on redistricting.",2024-01-30,physics.soc-ph,
http://arxiv.org/abs/2401.17452v3,"Conformal prediction (CP) is a method for constructing a prediction interval
around the output of a fitted model, whose validity does not rely on the model
being correct--the CP interval offers a coverage guarantee that is
distribution-free, but relies on the training data being drawn from the same
distribution as the test data. A recent variant, weighted conformal prediction
(WCP), reweights the method to allow for covariate shift between the training
and test distributions. However, WCP requires knowledge of the nature of the
covariate shift-specifically,the likelihood ratio between the test and training
covariate distributions. In practice, since this likelihood ratio is estimated
rather than known exactly, the coverage guarantee may degrade due to the
estimation error. In this paper, we consider a special scenario where
observations belong to a finite number of groups, and these groups determine
the covariate shift between the training and test distributions-for instance,
this may arise if the training set is collected via stratified sampling. Our
results demonstrate that in this special case, the predictive coverage
guarantees of WCP can be drastically improved beyond the bounds given by
existing estimation error bounds.",2024-01-30,stat.ME,
http://arxiv.org/abs/2401.17450v2,"Noisy Intermediate-Scale Quantum (NISQ) computers face a critical limitation
in qubit numbers, hindering their progression towards large-scale and
fault-tolerant quantum computing. A significant challenge impeding scaling is
crosstalk, characterized by unwanted interactions among neighboring components
on quantum chips, including qubits, resonators, and substrate. We motivate a
general approach to systematically resolving multifaceted crosstalks in a
limited substrate area. We propose Qplacer, a frequency-aware
electrostatic-based placement framework tailored for superconducting quantum
computers, to alleviate crosstalk by isolating these components in spatial and
frequency domains alongside compact substrate design. Qplacer commences with a
frequency assigner that ensures frequency domain isolation for qubits and
resonators. It then incorporates a padding strategy and resonator partitioning
for layout flexibility. Central to our approach is the conceptualization of
quantum components as charged particles, enabling strategic spatial isolation
through a 'frequency repulsive force' concept. Our results demonstrate that
Qplacer carefully crafts the physical component layout in mitigating various
crosstalk impacts while maintaining a compact substrate size. On various device
topologies and NISQ benchmarks, Qplacer improves fidelity by an average of
36.7x and reduces spatial violations (susceptible to crosstalk) by an average
of 12.76x, compared to classical placement engines. Regarding area
optimization, compared to manual designs, Qplacer can reduce the required
layout area by 2.14x on average",2024-01-30,quant-ph,
http://arxiv.org/abs/2401.17449v2,"Topological Hall effect (THE) is a hallmark of scalar spin chirality, which
is found in static skyrmion lattices. Recent theoretical works have shown that
scalar spin chirality could also emerge dynamically from thermal spin
fluctuations. Evidence of such a mechanism was found in the kagome magnet
YMn6Sn6 where fluctuations arise from frustrated exchange interactions between
Mn kagome layers. In YMn6Sn6, the rare-earth ion Y3+ is non-magnetic. When it
is replaced by a magnetic ion (Gd3+-Ho3+), the intrinsically antiferromagnetic
Mn-Mn interlayer coupling is overwhelmed by the indirect ferromagnetic Mn-R-Mn
one, relieving frustration. This generates interesting anomalous Hall
conductivity, but not THE. Here we show that Er lies in an intermediate regime
where direct and indirect interactions closely compete, so that ErMn6Sn6 can
switch from one regime to the other by temperature, i.e., from a collinear
ferrimagnetic ground state to a spiral antiferromagnet at 78 K. The AFM phase
forms a dome in the temperature-field phase diagram. Close to the boundary of
this dome, we find a sizable fluctuations-driven THE, thus underscoring the
universality of this mechanism for generating non-zero scalar spin chirality.",2024-01-30,cond-mat.mes-hall,
http://arxiv.org/abs/2401.17448v1,"The recognition of business opportunities is the first stage in the
entrepreneurial process. The current work analyzes the effects of individuals'
possession of and access to knowledge on the probability of recognizing good
business opportunities in their area of residence. The authors use an eclectic
theoretical framework consisting of intellectual and social capital concepts.
In particular, they analyze the role of individuals' educational level, their
perception that they have the right knowledge and skills to start a business,
whether they own and manage a firm, their contacts with other entrepreneurs,
and whether they have been business angels. The hypotheses proposed here are
tested using data collected for the GEM project in Spain in 2007. The results
show that individuals' access to external knowledge through the social networks
in which they participate is fundamental for developing the capacity to
recognize new business opportunities.",2024-01-30,econ.GN,
http://arxiv.org/abs/2401.17447v1,"In statistical inference, retrodiction is the act of inferring potential
causes in the past based on knowledge of the effects in the present and the
dynamics leading to the present. Retrodiction is applicable even when the
dynamics is not reversible, and it agrees with the reverse dynamics when it
exists, so that retrodiction may be viewed as an extension of inversion, i.e.,
time-reversal. Recently, an axiomatic definition of retrodiction has been made
in a way that is applicable to both classical and quantum probability using
ideas from category theory. Almost simultaneously, a framework for information
flow in in terms of semicartesian categories has been proposed in the setting
of categorical probability theory. Here, we formulate a general definition of
retrodiction to add to the information flow axioms in semicartesian categories,
thus providing an abstract framework for retrodiction beyond classical and
quantum probability theory. More precisely, we extend Bayesian inference, and
more generally Jeffrey's probability kinematics, to arbitrary semicartesian
categories.",2024-01-30,math.CT,
http://arxiv.org/abs/2401.17440v1,"This thesis explores two important areas in the mathematical analysis of
nonlinear partial differential equations: Generalized gradient flows and vector
valued Orlicz spaces. The first part deals with the existence of strong
solutions for generalized gradient flows, overcoming challenges such as
non-coercive and infinity-valued dissipation potentials and non-monotone
subdifferential operators on non-reflexive Banach spaces. The second part
focuses on the study of Banach-valued Orlicz spaces, a flexible class of Banach
spaces for quantifying the growth of nonlinear functions. Besides improving
many known results by imposing minimal assumptions, we extend the theory by
handling infinity-valued Orlicz integrands and arbitrary Banach-values in the
duality theory. The combination of these results offers a powerful tool for
analyzing differential equations involving functions of arbitrary growth rates
and leads to a significant improvement over previous results, demonstrated
through the existence of weak solutions for a doubly nonlinear initial-boundary
value problem of Allen-Cahn-Gurtin type.",2024-01-30,math.AP,
http://arxiv.org/abs/2402.00073v1,"The outer areas of Jupiter and Saturn have multiple zonal winds, reaching the
high latitudes, that penetrate deep into the planets' interiors, as suggested
by gravity measurements. These characteristics are replicable in numerical
simulations by including both a shallow stably stratified layer, below a
convecting envelope, and increasing electrical conductivity. A dipolar magnetic
field, assumed to be generated by a dynamo below our model, is imposed. We find
that the winds' depth into the stratified layer depends on the local product of
the squared magnetic field strength and electrical conductivity. The key for
the drop-off of the zonal winds is a meridional circulation which perturbs the
density structure in the stable layer. In the stable region its dynamics is
governed by a balance between Coriolis and electromagnetic forces. Our models
suggest that a stable layer extending into weakly conducting regions could
account for the observed deep zonal wind structures.",2024-01-30,astro-ph.EP,
http://arxiv.org/abs/2401.17438v2,"While the Hamiltonians used in standard quantum mechanics are Hermitian, it
is also possible to extend the theory to non-Hermitian Hamiltonians.
Particularly interesting are non-Hermitian Hamiltonians satisfying parity-time
(PT) symmetry, or more generally pseudo-Hermiticity, since such non-Hermitian
Hamiltonians can still exhibit real eigenvalues. In this work, we present a
quantum simulation of the time-dependent non-Hermitian non-PT-symmetric
Hamiltonian used in a pseudo-Hermitian extension of the
Landau-Zener-St\""uckelberg-Majorana (LZSM) model. The simulation is implemented
on a superconducting processor by using Naimark dilation to transform a
non-Hermitian Hamiltonian for one qubit into a Hermitian Hamiltonian for a
qubit and an ancilla; postselection on the ancilla state ensures that the qubit
undergoes nonunitary time-evolution corresponding to the original non-Hermitian
Hamiltonian. We observe properties such as the dependence of transition rates
on time and the replacement of conservation of total probability by other
dynamical invariants in agreement with predictions based on a theoretical
treatment of the pseudo-Hermitian LZSM system.",2024-01-30,quant-ph,
http://arxiv.org/abs/2401.17433v1,"We assessed the benefit of combining stress cardiac CT perfusion (CCTP)
myocardial blood flow (MBF) with coronary CT angiography (CCTA) using our
innovative CCTP software. By combining CCTA and CCTP, one can uniquely identify
a flow limiting stenosis (obstructive-lesion + low-MBF) versus MVD
(no-obstructive-lesion + low-MBF. We retrospectively evaluated 104 patients
with suspected CAD, including 18 with diabetes, who underwent CCTA+CCTP. Whole
heart and territorial MBF was assessed using our automated pipeline for CCTP
analysis that included beam hardening correction; temporal scan registration;
automated segmentation; fast, accurate, robust MBF estimation; and
visualization. Stenosis severity was scored using the CCTA
coronary-artery-disease-reporting-and-data-system (CAD-RADS), with obstructive
stenosis deemed as CAD-RADS>=3. We established a threshold MBF
(MBF=199-mL/min-100g) for normal perfusion. In patients with CAD-RADS>=3,
28/37(76%) patients showed ischemia in the corresponding territory. Two
patients with obstructive disease had normal perfusion, suggesting collaterals
and/or a hemodynamically insignificant stenosis. Among diabetics, 10 of 18
(56%) demonstrated diffuse ischemia consistent with MVD. Among non-diabetics,
only 6% had MVD. Sex-specific prevalence of MVD was 21%/24% (M/F). On a
per-vessel basis (n=256), MBF showed a significant difference between
territories with and without obstructive stenosis (165 +/- 61 mL/min-100g vs.
274 +/- 62 mL/min-100g, p <0.05). A significant and negative rank correlation
(rho=-0.53, p<0.05) between territory MBF and CAD-RADS was seen. CCTA in
conjunction with a new automated quantitative CCTP approach can augment the
interpretation of CAD, enabling the distinction of ischemia due to obstructive
lesions and MVD.",2024-01-30,q-bio.TO,
http://arxiv.org/abs/2401.17432v1,"We present Decapodes, a diagrammatic tool for representing, composing, and
solving partial differential equations. Decapodes provides an intuitive
diagrammatic representation of the relationships between variables in a system
of equations, a method for composing systems of partial differential equations
using an operad of wiring diagrams, and an algorithm for deriving solvers using
hypergraphs and string diagrams. The string diagrams are in turn compiled into
executable programs using the techniques of categorical data migration, graph
traversal, and the discrete exterior calculus. The generated solvers produce
numerical solutions consistent with state-of-the-art open source tools as
demonstrated by benchmark comparisons with SU2. These numerical experiments
demonstrate the feasibility of this approach to multiphysics simulation and
identify areas requiring further development.",2024-01-30,math.NA,
http://arxiv.org/abs/2401.17431v1,"Quantum steering captures the ability of one party, Alice, to control through
quantum correlations the state at a distant location, Bob, with superior
ability than allowed by a local hidden state model. Verifying the presence of
quantum steering has implications for the certification of quantum channels,
and its connection to the metrological power of the quantum state has been
recently proved. This link is established by means of the violation of a
Cram\'er-Rao bound holding for non-steerable states: its direct assessment
would then require operation in the asymptotic regime of a large number of
repetitions. Here, we extend previous work to account explicitly for the use of
a limited number of resources, and put this modified approach to test in a
quantum optics experiment. The imperfections in the apparatus demand an
adaptation of the original test in the multiparameter setting. Our results
provide guidelines to apply such a metrological approach to the validation of
quantum channels.",2024-01-30,quant-ph,
http://arxiv.org/abs/2403.00195v1,"Pluralistic ignorance is a social-psychological phenomenon characterized by
individuals privately maintaining beliefs or opinions that diverge from what
they perceive to be the prevailing norm within a group or society. Typical
models are based on opinion dynamics involving both private and public states,
where agents' binary states undergo changes influenced by social pressure.
However, these models overlook a crucial aspect of pluralistic ignorance: if
the absence of behavioral expression matches the normative status of the
behavior, social pressure exerted by the initial group configuration is enough
to induce pluralistic ignorance. Here, we show that pluralistic ignorance is
maintained if imitation of others is not too frequent and the social influence
of the initial minority is high. Interestingly, the individuals are able to
overcome the pluralistic ignorance by the end of interactions, yet it
resurfaces at the outset of each subsequent group interaction. However, if the
likelihood of individuals imitating others becomes excessively high,
pluralistic ignorance completely dissipates in an undesirable manner:
individuals internalize the dysfunctional behavior. We also show that, if
memory is added to the internalization process, pluralistic ignorance reaches
high levels only for intermediate imitation rates.",2024-02-29,physics.soc-ph,
http://arxiv.org/abs/2403.00185v5,"Multi-machine empirical scaling predicts an extremely narrow heat exhaust
layer in future high magnetic field tokamaks, producing high power densities
that require mitigation. In the experiments presented, the width of this
exhaust layer is nearly doubled using actuators to increase turbulent transport
in the plasma edge. This is achieved in low collisionality, high confinement
edge pedestals with their gradients limited by turbulent transport instead of
large-scale, coherent instabilities. The exhaust heat flux profile width and
divertor leg diffusive spreading both double as a high frequency band of
turbulent fluctuations propagating in the electron diamagnetic direction
doubles in amplitude. The results are quantitatively reproduced in
electromagnetic XGC particle-in-cell simulations which show the heat flux
carried by electrons emerges to broaden the heat flux profile, directly
supported by Langmuir probe measurements.",2024-02-29,physics.plasm-ph,
http://arxiv.org/abs/2403.00184v1,"Low-rank matrix completion concerns the problem of estimating unobserved
entries in a matrix using a sparse set of observed entries. We consider the
non-uniform setting where the observed entries are sampled with highly varying
probabilities, potentially with different asymptotic scalings. We show that
under structured sampling probabilities, it is often better and sometimes
optimal to run estimation algorithms on a smaller submatrix rather than the
entire matrix. In particular, we prove error upper bounds customized to each
entry, which match the minimax lower bounds under certain conditions. Our
bounds characterize the hardness of estimating each entry as a function of the
localized sampling probabilities. We provide numerical experiments that confirm
our theoretical findings.",2024-02-29,stat.ML,
http://arxiv.org/abs/2403.00183v2,"Particle Image Velocimetry (PIV) data is a valuable asset in fluid mechanics.
It is capable of visualizing flow structures even in complex physics scenarios,
such as the flow at the exit of the rotor of a centrifugal fan. Machine
learning is also a successful companion to PIV in order to increase data
resolution or impute experimental gaps. While classical algorithms focus solely
on replicating data using statistical metrics, the application of Physics
Informed Neural Networks (PINN) contributes to both data reconstruction and
adherence to governing equations. The present study utilizes a convolutional
physics-informed auto-encoder to reproduce planar PIV fields in the gappy
regions while also satisfying the mass conservation equation. It proposes a
novel approach, which compromises experimental data reconstruction for
compliance with physical restrictions. Simultaneously, it is aimed to ensure
that the reconstruction error does not considerably deviate from the
uncertainty band of the test data. Turbulence scale approximation is employed
to set the relative weighting of the physical and non-physical terms in the
loss function to ensure that both objectives are achieved. All steps are
initially evaluated on a set of DNS data to demonstrate the general capability
of the network. Finally, examination of the PIV data indicates that the
proposed PINN auto-encoder can enhance reconstruction accuracy by about 28% and
29% in terms of mass conservation residual and velocity statistics,
respectively, in expense of up to 5% increase in the number of vectors with
reconstruction error higher than the uncertainty band of the PIV test data.",2024-02-29,physics.flu-dyn,
http://arxiv.org/abs/2403.00182v2,"Quantum Annealers are basically quantum computers that with high probability
can optimize certain quadratic functions on Boolean variables in constant time.
These functions are basically the Hamiltonian of Ising models that reach the
ground energy state, with a high probability, after an annealing process. They
have been proposed as a way to solve SAT.
  These Hamiltonians can be seen as Max2XOR problems, i.e. as the problem of
finding an assignment that maximizes the number of XOR clauses of at most 2
variables that are satisfied. In this paper, we present several gadgets to
reduce SAT to Max2XOR. We show how they can be used to translate SAT instances
to initial configurations of a quantum annealer.",2024-02-29,quant-ph,
http://arxiv.org/abs/2403.00172v1,"Recent research has shown the potential of Model-based Reinforcement Learning
(MBRL) to enhance energy efficiency of Heating, Ventilation, and Air
Conditioning (HVAC) systems. However, existing methods rely on black-box
thermal dynamics models and stochastic optimizers, lacking reliability
guarantees and posing risks to occupant health. In this work, we overcome the
reliability bottleneck by redesigning HVAC controllers using decision trees
extracted from existing thermal dynamics models and historical data. Our
decision tree-based policies are deterministic, verifiable, interpretable, and
more energy-efficient than current MBRL methods. First, we introduce a novel
verification criterion for RL agents in HVAC control based on domain knowledge.
Second, we develop a policy extraction procedure that produces a verifiable
decision tree policy. We found that the high dimensionality of the thermal
dynamics model input hinders the efficiency of policy extraction. To tackle the
dimensionality challenge, we leverage importance sampling conditioned on
historical data distributions, significantly improving policy extraction
efficiency. Lastly, we present an offline verification algorithm that
guarantees the reliability of a control policy. Extensive experiments show that
our method saves 68.4% more energy and increases human comfort gain by 14.8%
compared to the state-of-the-art method, in addition to an 1127x reduction in
computation overhead. Our code and data are available at
https://github.com/ryeii/Veri_HVAC",2024-02-29,eess.SY,
http://arxiv.org/abs/2403.02346v1,"Understanding realistic complex systems requires confronting significant
conceptual, theoretical and experimental limitations rooted in the persistence
of views that originated in the mechanics of simple moving bodies. We define
the category of complex multiscale stochastic systems as a useful device for
capturing the minimally required complexity of many types of phenomena. In
doing so, we provide evidence indicating that determinism, continuity and
reversibility can lead to theoretical inadequacies that manifest as
intractability, inaccuracies or non-representativeness when applied to complex
systems. We take the view that interactions are fundamental and summarize their
portrayal across many disciplines. Despite their centrality, interactions
remain largely neglected as subjects of research interest of their own. We
hypothesize that a generalized theory of interactions may help organize
evidence from multiple scientific domains towards a more unified realistic view
of systems.",2024-02-29,physics.soc-ph,
http://arxiv.org/abs/2403.00171v1,"The High Luminosity upgrade of the Large Hadron Collider (HL-LHC) at CERN
will include eight cryo-assemblies that are expected to be fabricated and
delivered to CERN by the US HL-LHC Accelerator Upgrade Project (AUP) as part of
the U.S. contributions to the HL-LHC. These cryostat assemblies are the
quadrupole magnetic components of the HL-LHC Q1 and Q3 inner triplet optical
elements in front of the two interaction points. Each cryo-assembly consists of
two 4.2 m long Nb3Sn quadrupole magnets with aperture 150 mm and operating
gradient 132.6 T/m. The first pre-series cryo-assembly has been fabricated and
successfully tested at the horizontal test facility at Fermi National
Accelerator Laboratory. In this manuscript we report the quench test results of
the LQXFA/B-01 cryo-assembly. The primary objective of the horizontal test is
full cryo-assembly qualification and validation of the performance
requirements.",2024-02-29,physics.acc-ph,
http://arxiv.org/abs/2403.00168v1,"We establish quantitative homogenization results for the popular log-normal
coefficients. Since the coefficients are neither bounded nor uniformly
elliptic, standard proofs do not apply directly. Instead, we take inspiration
from the approach developed for the nonlinear setting by the first two authors
and capitalize on large-scale regularity results by Bella, Fehrmann, and Otto
for degenerate coefficients in order to leverage an optimal control (in terms
of scaling and stochastic integrability) of oscillations and fluctuations.",2024-02-29,math.AP,
http://arxiv.org/abs/2403.00166v1,"The accuracy assessment of remote-sensing derived built-up land data
represents a specific case of binary map comparison, where class imbalance
varies considerably across rural-urban trajectories. Thus, local accuracy
characterization of such datasets requires specific strategies that are robust
to low sample sizes and different levels of class imbalance. Herein, we examine
the suitability of commonly used spatial agreement measures for their localized
accuracy characterization of built-up land layers across the rural-urban
continuum, using the Global Human Settlement Layer and a reference database of
built-up land derived from cadastral and building footprint data.",2024-02-29,physics.soc-ph,
http://arxiv.org/abs/2403.00164v1,"We study the nonhomogeneous boundary value problem for the steady-state
Navier-Stokes equations under the slip boundary conditions in two-dimensional
multiply-connected bounded domains. Employing the approach of
Korobkov-Pileckas-Russo (Ann. Math. 181(2), 769-807, 2015), we prove that this
problem has a solution if the friction coefficient is sufficiently large
compared with the kinematic viscosity constant and the curvature of the
boundary. No additional assumption (other than the necessary requirement of
zero total flux through the boundary) is imposed on the boundary data. We also
show that such an assumption on the friction coefficient is redundant for the
existence of a solution in the case when the fluxes across each connected
component of the boundary are sufficiently small, or the domain and the given
data satisfy certain symmetry conditions. The crucial ingredient of our proof
is the fact that the total head pressure corresponding to the solution to the
steady Euler equations takes a constant value on each connected component of
the boundary.",2024-02-29,math.AP,
http://arxiv.org/abs/2403.00163v1,"We present a general condition to obtain subspaces that decay uniformly in a
system governed by the Lindblad master equation and use them to perform error
mitigated quantum computation. The expectation values of dynamics encoded in
such subspaces are unbiased estimators of noise-free expectation values. In
analogy to the decoherence free subspaces which are left invariant by the
action of Lindblad operators, we show that the uniformly decaying subspaces are
left invariant (up to orthogonal terms) by the action of the dissipative part
of the Lindblad equation. We apply our theory to a system of qubits and qudits
undergoing relaxation with varying decay rates and show that such subspaces can
be used to eliminate bias up to first order variations in the decay rates
without requiring full knowledge of noise. Since such a bias cannot be
corrected through standard symmetry verification, our method can improve error
mitigation in dual-rail qubits and given partial knowledge of noise, can
perform better than probabilistic error cancellation.",2024-02-29,quant-ph,
http://arxiv.org/abs/2403.00162v1,"We investigated Relations Among Green Functions defined in an alternative
strategy for coping with the divergences, also called the Implicit
Regularization Method (IREG): the mathematical content (divergent and finite)
will remain intact until the calculations end. The divergent part will be
organized through standardized objects free of physical quantities. In
contrast, the finite part is projected in a class of well-behaved functions
that carry all the amplitudes' physical content. That relations arise in
fermionic amplitudes in even space-time dimensions, where anomalous tensors
connect to finite amplitudes as in the bubbles and triangles in two and four
dimensions. Those tensors depend on surface terms, whose non-zero values arise
from finite amplitudes as requirements of consistency with the linearity of
integration and uniqueness. Maintaining these terms implies breaking
momentum-space homogeneity and, in a later step, the Ward identities.
Meanwhile, eliminating them allows more than one mathematical expression for
the same amplitude. That is a consequence of choices related to the involved
Dirac traces. Independently of divergences, it is impossible to satisfy all
symmetry implications by simultaneously requiring vanishing surface terms and
linearity. Then we approach the 1-loop level fermionic correction for the
propagation of the graviton in a space-time D=1+1 through the action of a Weyl
fermion in curved space-time. In this context, gravitational anomalies arise,
and the amplitudes investigated have the highest degree of divergence
quadratic. That imposes a substantial algebraic effort; however, the
conclusions are in agreement with the non-gravitational amplitudes. At the end
of the calculations, we show how it is possible to fix the value of the
divergent part through the relations imposed for amplitudes.",2024-02-29,hep-th,
http://arxiv.org/abs/2403.00161v1,"This paper presents a method for thematic agreement assessment of geospatial
data products of different semantics and spatial granularities, which may be
affected by spatial offsets between test and reference data. The proposed
method uses a multi-scale framework allowing for a probabilistic evaluation
whether thematic disagreement between datasets is induced by spatial offsets
due to different nature of the datasets or not. We test our method using
real-estate derived settlement locations and remote-sensing derived building
footprint data.",2024-02-29,stat.AP,
http://arxiv.org/abs/2403.00160v1,"Most low-mass stars form in stellar clusters that also contain massive stars,
which are sources of far-ultraviolet (FUV) radiation. Theoretical models
predict that this FUV radiation produces photo-dissociation regions (PDRs) on
the surfaces of protoplanetary disks around low-mass stars, impacting planet
formation within the disks. We report JWST and Atacama Large Millimetere Array
observations of a FUV-irradiated protoplanetary disk in the Orion Nebula.
Emission lines are detected from the PDR; modelling their kinematics and
excitation allows us to constrain the physical conditions within the gas. We
quantify the mass-loss rate induced by the FUV irradiation, finding it is
sufficient to remove gas from the disk in less than a million years. This is
rapid enough to affect giant planet formation in the disk.",2024-02-29,astro-ph.GA,
http://arxiv.org/abs/2403.00156v2,"An indispensable ingredient for pair density wave (PDW) superconductivity is
the presence of an attractive pairing interaction at finite momentum. Here, we
show how this condition can be met with straightforward electron-density
interactions in multiband systems. The electron-density interaction, when
projected to the band basis, acquires form factors with nontrivial momentum
dependence and thereby exhibits a potential tendency to a finite-momentum
pairing instability. By applying a mean-field analysis to two simple multiband
models, the checkerboard lattice and three-band Hubbard model, we find that PDW
order can indeed become the leading instability if a strong nearest-neighbour
attraction is present. Moreover, the condition for the transition from a
uniform superconductor to a PDW superconductor is shown via a simple quantum
geometric argument.",2024-02-29,cond-mat.supr-con,
http://arxiv.org/abs/2403.00152v1,"Matrix product density operators (MPDOs) are tensor network representations
of locally purified density matrices where each physical degree of freedom is
associated to an environment degree of freedom. MPDOs have interesting
properties for mixed state representations: guaranteed positivity by
construction, efficient conservation of the trace and computation of local
observables. However, they have been challenging to use for noisy quantum
circuit simulation, as the application of noise increases the dimension of the
environment Hilbert space, leading to an exponential growth of bond dimensions.
MPDOs also lack a unique canonical form, due to the freedom in the choice of
basis for the environment Hilbert space, which leads to a vast variation of
bond dimensions.
  In this work, we present a systematic way to reduce the bond dimensions of
MPDOs by disentangling the purified state. We optimize the basis for the
environment Hilbert space by performing density matrix renormalization group
(DMRG)-like sweeps of local 2-qubit basis optimization. Interestingly, we find
that targeting only the disentanglement of the purified state leads to a
reduction of the environment dimension. In other words, a compact MPDO
representation requires a low-entanglement purified state.
  We apply our compression method to the emulation of noisy random quantum
circuits. Our technique allows us to keep bounded bond dimensions, and thus
bounded memory, contrary to previous works on MPDOs, while keeping reasonable
truncation fidelities.",2024-02-29,quant-ph,
http://arxiv.org/abs/2403.00151v1,"We report Monte-Carlo studies of the orientational order and melting of a 2D
skyrmion lattice containing more than one million spins. Two models have been
investigated, a microscopic model of lattice spins with Dzyaloshinskii-Moryia
interaction that possesses skyrmions, and the model in which skyrmions are
treated as point particles with repulsive interaction derived from a spin
model. They produce similar results. The skyrmion lattice exhibits a sharp
one-step transition between solid and liquid phases on temperature and the
magnetic field. This solid-liquid transition is characterized by the kink in
the magnetization. The field-temperature phase diagram is computed. We show
that the application of the field gradient to a 2D system of skyrmions produces
a solid-liquid interface that must be possible to observe in experiments.",2024-02-29,cond-mat.mtrl-sci,
http://arxiv.org/abs/2403.00146v1,"We revisit recently published high-fidelity implicit large eddy simulation
datasets obtained with a high-order discontinuous Galerkin spectral element
method and analyse them using Proper Orthogonal Decomposition (POD) as well as
Spectral Proper Orthogonal Decomposition (SPOD). The first configuration is the
MTU T161 low-pressure turbine cascade with resolved end wall boundary layers in
a clean version and one with incoming turbulent wakes. We focus on the
behaviour of the laminar separation bubble and the secondary flow system and
how these phenomena are affected by incoming wakes. The second configuration is
a transonic compressor cascade in which we analyse the unsteady behaviour of
the shock wave boundary layer interaction. Throughout the paper, we try to
discuss not only the flow physics but also insights into how the modal
decomposition techniques can help facilitate understanding and where their
limitations are.",2024-02-29,physics.flu-dyn,
http://arxiv.org/abs/2403.00142v1,"This paper extends the existing fractional Hawkes process to better model
mainshock-aftershock sequences of earthquakes. The fractional Hawkes process is
a self-exciting point process model with temporal decay kernel being a
Mittag-Leffler function. A maximum likelihood estimation scheme is developed
and its consistency is checked. It is then compared to the ETAS model on three
earthquake sequences in Southern California. The fractional Hawkes process
performs favourably against the ETAS model. Additionally, two parameters in the
fractional Hawkes process may have a fixed geophysical meaning dependent on the
study zone and the stage of the seismic cycle the zone is in.",2024-02-29,stat.AP,
http://arxiv.org/abs/2403.00140v1,"This work is motivated by in vivo experiments in which measurement are
destructive so that the variables of interest can never be observed
simultaneously when the aim is to estimate the regression coefficients of a
linear regression. Assuming that the global experiment can be decomposed into
sub experiments (corresponding for example to different doses) with distinct
first moments, we propose different estimators of the linear regression which
take account of that additional information. We consider estimators based on
moments as well as estimators based optimal transport theory. These estimators
are proved to be consistent as well as asymptotically Gaussian under weak
hypotheses. The asymptotic variance has no explicit expression, except in some
particular cases, and specific bootstrap approaches are developed to build
confidence intervals for the estimated parameter. A Monte Carlo study is
conducted to assess and compare the finite sample performances of the different
approaches.",2024-02-29,stat.ME,
http://arxiv.org/abs/2403.00139v1,"This paper analyzes a problem of optimal static hedging using derivatives in
incomplete markets. The investor is assumed to have a risk exposure to two
underlying assets. The hedging instruments are vanilla options written on a
single underlying asset. The hedging problem is formulated as a utility
maximization problem whereby the form of the optimal static hedge is
determined. Among our results, a semi-analytical solution for the optimizer is
found through variational methods for exponential, power/logarithmic, and
quadratic utility. When vanilla options are available for each underlying
asset, the optimal solution is related to the fixed points of a Lipschitz map.
In the case of exponential utility, there is only one such fixed point, and
subsequent iterations of the map converge to it.",2024-02-29,q-fin.MF,
http://arxiv.org/abs/2403.02345v1,"This paper primarily focuses on the investigation of the distribution of
certain crucial operators with respect to significant states on the (q,2)-Fock
space, for instance, the vacuum distribution of the field operator.",2024-02-29,math-ph,
http://arxiv.org/abs/2403.00124v1,"We explore the ground states of strongly interacting bosons in the
vanishingly small and weak lattices using the multiconfiguration time-dependent
Hartree method for bosons (MCTDHB) which calculate numerically exact many-body
wave function. Two new many-body phases: fragmented or quasi superfluid (QSF)
and incomplete fragmented Mott or quasi Mott insulator (QMI) are emerged due to
the strong interplay between interaction and lattice depth. Fragmentation is
utilized as a figure of merit to distinguish these two new phases. We utilize
the eigenvalues of the reduced one-body density matrix and define an order
parameter that characterizes the pathway from a very weak lattice to a deep
lattice. We provide a detailed investigation through the measures of one- and
two-body correlations and information entropy. We find that the structures in
one- and two-body coherence are good markers to understand the gradual built-up
of intra-well correlation and decay of inter-well correlation with increase in
lattice depth.",2024-02-29,cond-mat.quant-gas,
http://arxiv.org/abs/2403.00122v1,"Quantum technologies, including quantum computing, cryptography, and sensing,
among others, are set to revolutionize sectors ranging from materials science
to drug discovery. Despite their significant potential, the implications for
public health have been largely overlooked, highlighting a critical gap in
recognition and preparation. This oversight necessitates immediate action, as
public health remains largely unaware of quantum technologies as a tool for
advancement. The application of quantum principles to epidemiology and health
informatics, termed quantum health epidemiology and quantum health informatics,
has the potential to radically transform disease surveillance, prediction,
modeling, and analysis of health data. However, there is a notable lack of
quantum expertise within the public health workforce and educational pipelines.
This gap underscores the urgent need for the development of quantum literacy
among public health practitioners, leaders, and students to leverage emerging
opportunities while addressing risks and ethical considerations. Innovative
teaching methods, such as interactive simulations, games, visual models, and
other tailored platforms, offer viable solutions for bridging knowledge gaps
without the need for advanced physics or mathematics. However, the opportunity
to adapt is fleeting as the quantum era in healthcare looms near. It is
imperative that public health urgently focuses on updating its educational
approaches, workforce strategies, data governance, and organizational culture
to proactively meet the challenges of quantum disruption thereby becoming
quantum ready.",2024-02-29,physics.soc-ph,
http://arxiv.org/abs/2403.00121v2,"While there exists a rich array of matrix column subset selection problem
(CSSP) algorithms for use with interpolative and CUR-type decompositions, their
use can often become prohibitive as the size of the input matrix increases. In
an effort to address these issues, the authors in
\cite{emelianenko2024adaptive} developed a general framework that pairs a
column-partitioning routine with a column-selection algorithm. Two of the four
algorithms presented in that work paired the Centroidal Voronoi Orthogonal
Decomposition (\textsf{CVOD}) and an adaptive variant (\textsf{adaptCVOD}) with
the Discrete Empirical Interpolation Method (\textsf{DEIM})
\cite{sorensen2016deim}. In this work, we extend this framework and pair the
\textsf{CVOD}-type algorithms with any CSSP algorithm that returns linearly
independent columns. Our results include detailed error bounds for the
solutions provided by these paired algorithms, as well as expressions that
explicitly characterize how the quality of the selected column partition
affects the resulting CSSP solution.",2024-02-29,math.NA,
http://arxiv.org/abs/2403.00118v1,"Stars appear darker at their limbs than at their disk centers because at the
limb we are viewing the higher and cooler layers of stellar photospheres. Limb
darkening derived from state-of-the-art stellar atmosphere models
systematically fails to reproduce recent transiting exoplanet light curves from
the Kepler, TESS, and JWST telescopes -- stellar brightness obtained from
measurements drops less steeply towards the limb than predicted by models. All
previous models assumed atmosphere devoid of magnetic fields. Here we use our
new stellar atmosphere models computed with the 3D radiative
magneto-hydrodynamic code MURaM to show that small-scale concentration of
magnetic fields on the stellar surface affect limb darkening at a level that
allows us to explain the observations. Our findings provide a way forward to
improve the determination of exoplanet radii and especially the transmission
spectroscopy analysis for transiting planets, which relies on a very accurate
description of stellar limb darkening from the visible through the infrared.
Furthermore, our findings imply that limb darkening allows measuring the
small-scale magnetic field on stars with transiting planets.",2024-02-29,astro-ph.SR,
http://arxiv.org/abs/2403.00117v1,"Ultrathin lightsails propelled by laser radiation pressure to relativistic
speeds are currently the most promising route for flyby-based exoplanet
exploration. However, there has been a notable lack of experimental
characterization of key parameters essential for lightsail propulsion.
Therefore, a model platform for optomechanical characterization of lightsail
prototypes made from realistic materials is needed. We propose an approach for
simultaneous measurement of optical forces and driving powers, which
capitalizes on the multiphysics dynamics induced by the driving laser beam. By
modelling the lightsail with a 50-nm thick silicon nitride membrane suspended
by compliant micromechanical springs, we quantify force from off-resonantly
driven displacement and power from heating-induced mechanical mode softening.
This approach allows us to calibrate the measured forces to the driving powers
by operating the device as a mechanical bolometer. We report radiation pressure
forces of 80 fN using a collimated pump beam of 100 W/cm2 and noise-robust
common-path interferometry. As lightsails will inevitably experience non-normal
forces, we quantify the effects of incidence angle and spot size on the optical
force and explain the nonintuitive trend by edge scattering. Our results
provide a framework for comprehensive lightsail characterization and laboratory
optomechanical manipulation of macroscopic objects by radiation pressure
forces.",2024-02-29,physics.optics,
http://arxiv.org/abs/2403.00113v1,"Calculations of heat transport in crystalline materials have recently become
mainstream, thanks to machine-learned interatomic potentials that allow for
significant computational cost reductions while maintaining the accuracy of
first-principles calculations. Moment tensor potentials (MTP) are among the
most efficient and accurate models in this regard. In this study, we
demonstrate the application of MTP to the calculation of the lattice thermal
conductivity of alpha and beta Ga2O3. Although MTP is commonly employed for
lattice thermal conductivity calculations, the advantages of applying the
active learning methodology for potential generation is often overlooked. Here,
we emphasize its importance and illustrate how it enables the generation of a
robust and accurate interatomic potential while maintaining a moderate-sized
training dataset.",2024-02-29,cond-mat.mtrl-sci,
http://arxiv.org/abs/2403.00112v1,"Zenith sky brightness maps in the V and B bands of the region of Catalonia
are presented in this paper. For creating them we have used the light pollution
numerical model Illumina v2. The maps have a sampling of 5x5 km for the whole
region with an improved resolution of 1x1 km for one of the provinces within
Catalonia, Tarragona. Before creating the final maps, the methodology was
tested successfully by comparing the computed values to measurements in
nineteen different locations spread out throughout the territory. The resulting
maps have been compared to the zenith sky brightness world atlas and also to
Sky Quality Meter (SQM) dynamic measurements. When comparing to measurements we
found small differences mainly due to mismatching in the location of the points
studied, and also due to differences in the natural sky brightness and
atmospheric content. In the comparison to the world atlas some differences were
expected as we are taking into account the blocking effect of topography and
obstacles, and also due to a more precise light sources characterization. The
results of this work confirm the conclusion found in other studies that the
minimum sampling for studying sky brightness fine details is of 1x1 km.
However, a sampling of 5x5 km is interesting when studying general trends,
mainly for vast areas, due to the reduction of the time required to create the
maps.",2024-02-29,astro-ph.IM,
http://arxiv.org/abs/2403.00110v2,"The nearby LHS 1678 (TOI-696) system contains two confirmed planets and a
wide-orbit, likely-brown-dwarf companion, which orbit an M2 dwarf with a unique
evolutionary history. The host star occupies a narrow ""gap"" in the HR diagram
lower main sequence, associated with the M dwarf fully convective boundary and
long-term luminosity fluctuations. This system is one of only about a dozen M
dwarf multi-planet systems to date that hosts an ultra-short period planet
(USP). Here we validate and characterize a third planet in the LHS 1678 system
using TESS Cycle 1 and 3 data and a new ensemble of ground-based light curves.
LHS 1678 d is a 0.98 +/-0.07 Earth radii planet in a 4.97-day orbit, with an
insolation flux of 9.1 +0.9/-0.8 Earth insolations. These properties place it
near 4:3 mean motion resonance with LHS 1678 c and in company with LHS 1678 c
in the Venus zone. LHS 1678 c and d are also twins in size and predicted mass,
making them a powerful duo for comparative exoplanet studies. LHS 1678 d joins
its siblings as another compelling candidate for atmospheric measurements with
the JWST and mass measurements using high-precision radial velocity techniques.
Additionally, USP LHS 1678 b breaks the ""peas-in-a-pod"" trend in this system,
although additional planets could fill in the ""pod"" beyond its orbit. LHS
1678's unique combination of system properties and their relative rarity among
the ubiquity of compact multi-planet systems around M dwarfs makes the system a
valuable benchmark for testing theories of planet formation and evolution.",2024-02-29,astro-ph.EP,
http://arxiv.org/abs/2403.00109v1,"Kerr-induced synchronization (KIS) provides a new key tool for the control
and stabilization of the repetition rate of a cavity soliton frequency comb. It
enables direct external control of a given comb tooth of a dissipative Kerr
soliton (DKS) thanks to its capture by an injected reference laser. Efficient
KIS requires its coupling energy to be sufficiently large, and hence both the
comb tooth and intracavity reference power must be optimized, which can be
achieved through higher-order dispersion that enables phase-matched dispersive
waves (DWs), where comb teeth are on resonance. However, such a design is
highly restrictive, preventing arbitrary use of reference wavelengths away from
the DW(s). In particular, for large spectral separations from the main pump the
cavity dispersion yields large detuning between comb teeth and their respective
cavity resonances, thereby decreasing the coupling energy and rendering KIS to
be highly inefficient or practically impossible. Here, we demonstrate an
alternative KIS method where efficient synchronization can be tailored at
arbitrary modes as needed. Using a multi-color DKS created from multi-pumping a
microresonator, a synthetic DW at the second-color wavepacket can be
selectively created where otherwise dispersion is far too large for KIS to be
experimentally feasible. Since a unique group velocity for both colors exists
thanks to cross-phase modulation, the repetition rate disciplining of the
secondary color wavepacket through its KIS automatically translates into the
DKS microcomb control. We first investigate this color-KIS phenomenon
theoretically, and then experimentally demonstrate its control and tuning of
the soliton microcomb repetition rate. As a consequence, we demonstrate optical
frequency division that is uncoupled from the main pump that generates the DKS.",2024-02-29,physics.optics,
http://arxiv.org/abs/2403.00104v2,"We introduce hom-associative versions of the higher order Weyl algebras,
generalizing the construction of the first hom-associative Weyl algebras. We
then show that the higher order hom-associative Weyl algebras are simple, and
that all their one-sided ideals are principal.",2024-02-29,math.RA,
http://arxiv.org/abs/2403.00102v1,"Many experimental techniques aim at determining the Hamiltonian of a given
system. The Hamiltonian describes the system's evolution in the absence of
dissipation, and is often central to control or interpret an experiment. Here,
we theoretically propose and experimentally demonstrate a method for
Hamiltonian reconstruction from measurements over a large area of phase space,
overcoming the main limitation of previous techniques. A crucial ingredient for
our method is the presence of dissipation, which enables sampling of the
Hamiltonian through ringdown-type measurements. We apply the method to a
driven-dissipative system -- a parametric oscillator -- observed in a rotating
frame, and reconstruct the (quasi-)Hamiltonian of the system. Furthermore, we
demonstrate that our method provides direct experimental access to the
so-called symplectic norm of the stationary states of the system, which is tied
to the particle- or hole-like nature of excitations of these states. In this
way, we establish a method to unveil qualitative differences between the
fluctuations around stabilized minima and maxima of the nonlinear
out-of-equilibrium stationary states. Our method constitutes a versatile
approach to characterize a wide class of driven-dissipative systems.",2024-02-29,cond-mat.mes-hall,
http://arxiv.org/abs/2403.00101v1,"Gamma-ray bursts (GRBs) are ideal probes of the Universe at high redshift (z
> 5), pinpointing the locations of the earliest star-forming galaxies and
providing bright backlights that can be used to spectrally fingerprint the
intergalactic medium and host galaxy during the period of reionization. Future
missions such as Gamow Explorer are being proposed to unlock this potential by
increasing the rate of identification of high-z GRBs to rapidly trigger
observations from 6-10 m ground telescopes, JWST, and the Extremely Large
Telescopes. Gamow was proposed to the NASA 2021 Medium-Class Explorer (MIDEX)
program as a fast-slewing satellite featuring a wide-field lobster-eye X-ray
telescope (LEXT) to detect and localize GRBs, and a 30 cm narrow-field
multi-channel photo-z infrared telescope (PIRT) to measure their photometric
redshifts using the Lyman-alpha dropout technique. To derive the PIRT
sensitivity requirement we compiled a complete sample of GRB
optical-near-infrared afterglows from 2008 to 2021, adding a total of 66 new
afterglows to our earlier sample, including all known high-z GRB afterglows. We
performed full light-curve and spectral-energy-distribution analyses of these
afterglows to derive their true luminosity at very early times. For all the
light curves, where possible, we determined the brightness at the time of the
initial finding chart of Gamow, at different high redshifts and in different
NIR bands. We then followed the evolution of the luminosity to predict
requirements for ground and space-based follow-up. We find that a PIRT
sensitivity of 15 micro-Jy (21 mag AB) in a 500 s exposure simultaneously in
five NIR bands within 1000s of the GRB trigger will meet the Gamow mission
requirement to recover > 80% of all redshifts at z > 5.",2024-02-29,astro-ph.HE,
http://arxiv.org/abs/2403.00097v2,"We study a skew product transformation associated to an irrational rotation
of the circle [0,1]/~. This skew product keeps track of the number of times an
orbit of the rotation lands in the two complementary intervals of {0,1/2} in
the circle. We show that under certain conditions on the continued fraction
expansion of the irrational number defining the rotation, the skew product
transformation has certain dense orbits. This is in spite of the presence of
numerous non-dense orbits. We use this to construct laminations on infinite
type surfaces with exotic properties. In particular, we show that for every
infinite type surface with an isolated planar end, there is an infinite clique
of 2-filling rays based at that end. These 2-filling rays are relevant to
Bavard--Walker's loop graphs.",2024-02-29,math.DS,
http://arxiv.org/abs/2403.00094v1,"We analyse the mixing profile of a random walk on a dynamic random
permutation, focusing on the regime where the walk evolves much faster than the
permutation. Two types of dynamics generated by random transpositions are
considered: one allows for coagulation of permutation cycles only, the other
allows for both coagulation and fragmentation. We show that for both types,
after scaling time by the length of the permutation and letting this length
tend to infinity, the total variation distance between the current distribution
and the uniform distribution converges to a limit process that drops down in a
single jump. This jump is similar to a one-sided cut-off, occurs after a random
time whose law we identify, and goes from the value 1 to a value that is a
strictly decreasing and deterministic function of the time of the jump, related
to the size of the largest component in Erd\H{o}s-R\'enyi random graphs. After
the jump, the total variation distance follows this function down to 0.",2024-02-29,math.PR,
http://arxiv.org/abs/2403.00093v1,"Neuroimaging consortia can enhance reliability and generalizability of
findings by pooling data across studies to achieve larger sample sizes. To
adjust for site and MRI protocol effects, imaging datasets are often harmonized
based on healthy controls. When data from a control group were not collected,
statistical harmonization options are limited as patient characteristics and
acquisition-related variables may be confounded. Here, in a multi-study
neuroimaging analysis of Alzheimer's patients and controls, we tested whether
it is possible to generate synthetic control MRIs. For one case-control study,
we used a generative adversarial model for style-based harmonization to
generate site-specific controls. Downstream feature extraction, statistical
harmonization and group-level multi-study case-control and case-only analyses
were performed twice, using either true or synthetic controls. All effect sizes
using synthetic controls overlapped with those based on true study controls.
This line of work may facilitate wider inclusion of case-only studies in
multi-study consortia.",2024-02-29,q-bio.QM,
http://arxiv.org/abs/2403.00091v1,"Geometric frustration in two-dimensional Ising models allows for a wealth of
exotic universal behavior, both Ising and non-Ising, in the presence of quantum
fluctuations. In particular, the triangular antiferromagnet and Villain model
in a transverse field can be understood through distinct XY pseudospins, but
have qualitatively similar phase diagrams including a quantum phase transition
in the (2+1)-dimensional XY universality class. While the quantum dynamics of
modestly-sized systems can be simulated classically using tensor-based methods,
these methods become infeasible for larger lattices. Here we perform both
classical and quantum simulations of these dynamics, where our quantum
simulator is a superconducting quantum annealer. Our observations on the
triangular lattice suggest that the dominant quench dynamics are not described
by the quantum Kibble-Zurek scaling of the quantum phase transition, but rather
a faster coarsening dynamics in an effective two-dimensional XY model in the
ordered phase. Similarly, on the Villain model, the scaling exponent does not
match the Kibble-Zurek expectation. These results demonstrate the ability of
quantum annealers to simulate coherent quantum dynamics and scale beyond the
reach of classical approaches.",2024-02-29,quant-ph,
http://arxiv.org/abs/2403.00090v1,"Spatial coherence plays an important role in several real-world applications
ranging from imaging to communication. As a result, its accurate
characterization and measurement are extremely crucial for its optimal
application. However, efficient measurement of an arbitrary complex spatial
coherence function is still very challenging. In this letter, we propose an
efficient, noise-insensitive interferometric technique that combines wavefront
shearing and inversion for measuring the complex cross-spectral density
function of the class of fields, in which the cross-spectral density function
depends either on the difference of the spatial coordinates, or the squares of
spatial coordinates, or both. This class of fields are most commonly
encountered, and we experimentally demonstrate high-fidelity measurement of
many stationary and non-stationary fields.",2024-02-29,physics.optics,
http://arxiv.org/abs/2403.00086v1,"Single-phase multi-principal element alloys (MPEAs) hold promise for improved
mechanical properties as a result of multiple operative deformation modes.
However, the use of many of these alloys in structural applications is limited
as a consequence of their poor aqueous corrosion resistance. Here we introduce
a new approach for significantly improving the passivation behavior of alloys
by tuning the chemical short-range order (CSRO) parameter. We show that the
addition of only 0.03 to 0.06 mole fraction of Al to a (FeCoNi)0.9Cr0.1 alloy
changed both the magnitude and sign of the Cr-Cr CSRO parameter resulting in
passivation behavior similar to 304L stainless steel containing twice the
amount of Cr. Our analysis is based on comparing electrochemical measures of
the kinetics of passive film formation with CSRO characterizations using
time-of-flight neutron scattering, cluster expansion methods, density
functional theory and Monte Carlo techniques. Our findings are interpreted
within the framework of a recently proposed percolation theory of passivation
that examines how selective dissolution of the non-passivating alloy components
and CSRO results in excellent passive films at reduced levels of the
passivating component.",2024-02-29,cond-mat.mtrl-sci,
http://arxiv.org/abs/2403.00085v1,"The current limit on the tensor-to-scalar ratio from the BICEP/Keck
Collaboration (with r<0.036 at 95% confidence) puts pressure on early universe
models, with less than 10% of the error on r attributed to uncertainty in
Galactic foregrounds. We use the BICEP/Keck BK18 public multi-frequency
likelihood to test some further assumptions made in the foreground modeling,
finding little impact on the estimate for r. We then estimate
foreground-marginalized cosmic microwave background (CMB) B-mode polarization
bandpowers. We fit them with a multivariate offset-lognormal distribution and
construct a marginalized 'BK-lite' likelihood for the CMB B-mode spectrum with
no nuisance parameters, serving as a method demonstration for future analyses
of small sky regions, for example from the South Pole Observatory or CMB-S4.",2024-02-29,astro-ph.CO,
http://arxiv.org/abs/2403.00082v1,"Flows are omnipresent and govern the dynamics of plasma. Solar tornadoes are
a class of apparently rotating prominences, that might be formed by thermal
instability. In spectroscopic studies on thermal instability background flow is
commonly neglected. We here determine the effect of background flow on thermal
instability in cylindrical magnetic field configurations as the influence of
various parameters on the MHD spectrum. We investigate discrete thermal modes.
In an analytical study, we extend upon the literature by including a generic
background flow in a cylindrical coordinate system. The non-adiabatic MHD
equations are linearised, Fourier-analysed, and are examined to understand how
a background flow changes the continua. An approximate expression for discrete
thermal modes is derived using a WKB analysis. The analytical results are then
verified for a benchmark equilibrium using the eigenvalue code Legolas. The
eigenfunctions of discrete thermal modes are visualised in 2D and 3D. The
thermal continuum is Doppler-shifted due to the background flow, just like the
slow and Alfv\'en continua. Discrete modes are altered because the governing
equations contain flow-related terms. An approximate expression to predict the
appearance of discrete thermal modes based on the equilibrium parameters is
derived. All analytical expressions match the numerical results. The
distribution of the density perturbations of the discrete thermal modes is not
a singular condensation. 3D visualisation of the total velocity field shows
that the helical field is heavily influenced by the radial velocity
perturbation. We derived analytic expressions for non-adiabatic MHD modes of a
cylindrical equilibrium with background flow and verified them using a coronal
equilibrium. However, the equations are valid for and can be applied in other
astrophysical environments.",2024-02-29,astro-ph.SR,
http://arxiv.org/abs/2403.00080v1,"Record-breaking temperature events are now very frequently in the news,
viewed as evidence of climate change. With this as motivation, we undertake the
first substantial spatial modeling investigation of temperature record-breaking
across years for any given day within the year. We work with a dataset
consisting of over sixty years (1960-2021) of daily maximum temperatures across
peninsular Spain. Formal statistical analysis of record-breaking events is an
area that has received attention primarily within the probability community,
dominated by results for the stationary record-breaking setting with some
additional work addressing trends. Such effort is inadequate for analyzing
actual record-breaking data. Effective analysis requires rich modeling of the
indicator events which define record-breaking sequences. Resulting from novel
and detailed exploratory data analysis, we propose hierarchical conditional
models for the indicator events. After suitable model selection, we discover
explicit trend behavior, necessary autoregression, significance of distance to
the coast, useful interactions, helpful spatial random effects, and very strong
daily random effects. Illustratively, the model estimates that global warming
trends have increased the number of records expected in the past decade almost
two-fold, 1.93 (1.89,1.98), but also estimates highly differentiated climate
warming rates in space and by season.",2024-02-29,stat.ME,
http://arxiv.org/abs/2403.00073v2,"Carroll black holes with an associated Carroll temperature were introduced
recently. So far, it is unclear if they exhibit a Hawking-like effect. To solve
this, we study scalar fields on Carroll black hole backgrounds. Inspired by
anomaly methods, we derive a Hawking-like energy-momentum tensor compatible
with the Carroll temperature and the Stefan-Boltzmann law. Key steps in our
derivation are the finiteness of energy at the Carroll extremal surface and
compatibility with the Carroll Ward identities, thereby eliminating,
respectively, the Carroll-analogs of the Boulware and Unruh vacua.",2024-02-29,hep-th,
http://arxiv.org/abs/2403.00072v1,"We propose a scheme for generating a high-purity single photon on the basis
of cavity quantum electrodynamics (QED). This scheme employs a four-level
system including two excited states, two ground states, and two driving lasers;
this structure allows the suppression of the re-excitation process due to the
atomic decay, which is known to significantly degrade the single-photon purity
in state-of-the-art photon sources using a three-level system. Our analysis
shows that the re-excitation probability arbitrarily approaches zero without
sacrificing the photon generation probability when increasing the power of the
driving laser between the excited states. This advantage is achievable by using
current cavity-QED technologies. Our scheme can contribute to developing
distributed quantum computation or quantum communication with high accuracy.",2024-02-29,quant-ph,
http://arxiv.org/abs/2403.00065v1,"We present a wide-field study of the globular cluster systems (GCS) of the
elliptical galaxy NGC 3640 and its companion NGC 3641, based on observations
from Gemini Multi-Object Spectrograph/Gemini. NGC 3640 is a shell galaxy which
presents a complex morphology, which previous studies have indicated as the
sign of a recent 'dry' merger, although whether its nearest neighbour could
have influenced these substructures remains an open question. In this work, we
trace the spatial distribution of the globular clusters (GCs) as well as their
colour distribution, finding a potential bridge of red GCs that connects NGC
3640 to its less massive companion, and signs that the blue GCs were spatially
disturbed by the event that created the shells.",2024-02-29,astro-ph.GA,
http://arxiv.org/abs/2403.00063v1,"M15 is a globular cluster with a known spread in neutron-capture elements.
This paper presents abundances of neutron-capture elements for 62 stars in M15.
Spectra were obtained with the Michigan/Magellan Fiber System (M2FS)
spectrograph, covering a wavelength range from ~4430-4630 A. Spectral lines
from Fe I, Fe II, Sr I, Zr II, Ba II, La II, Ce II, Nd II, Sm II, Eu II, and Dy
II, were measured, enabling classifications and neutron-capture abundance
patterns for the stars. Of the 62 targets, 44 are found to be highly
Eu-enhanced r-II stars, another 17 are moderately Eu-enhanced r-I stars, and
one star is found to have an s-process signature. The neutron-capture patterns
indicate that the majority of the stars are consistent with enrichment by the
r-process. The 62 target stars are found to show significant star-to-star
spreads in Sr, Zr, Ba, La, Ce, Nd, Sm, Eu, and Dy, but no significant spread in
Fe. The neutron-capture abundances are further found to have slight
correlations with sodium abundances from the literature, unlike what has been
previously found; follow-up studies are needed to verify this result. The
findings in this paper suggest that the Eu-enhanced stars in M15 were enhanced
by the same process, that the nucleosynthetic source of this Eu pollution was
the r-process, and that the r-process source occurred as the first generation
of cluster stars was forming.",2024-02-29,astro-ph.SR,
http://arxiv.org/abs/2403.00060v2,"Models of accretion discs surrounding active galactic nuclei (AGNs) find vast
applications in high-energy astrophysics. The broad strategy is to parametrize
some of the key disc properties such as gas density and temperature as a
function of the radial coordinate from a given set of assumptions on the
underlying physics. Two of the most popular approaches in this context were
presented by Sirko & Goodman (2003) and Thompson et al. (2005). We present a
critical reanalysis of these widely used models, detailing their assumptions
and clarifying some steps in their derivation that were previously left unsaid.
Our findings are implemented in the pAGN module for the Python programming
language, which is the first public implementation of these accretion-disc
models. We further apply pAGN to the evolution of stellar-mass black holes
embedded in AGN discs, addressing the potential occurrence of migration traps.",2024-02-29,astro-ph.HE,
http://arxiv.org/abs/2403.00857v1,"Identifying signatures of quantum coherent behaviour in photoactive systems
that are maintained in stationary states away from thermal equilibrium is an
open problem of wide interest in a variety of physical scenarios, including
single photosynthetic complexes subjected to continuous incoherent
illumination. Here we consider a prototype light-harvesting heterodimer
exhibiting coherent and collective exciton-vibration interactions and show that
the second-order frequency-filtered correlations of fluorescence photons
provide insightful information on the influence of such coherent interactions
for different transitions, thereby yielding fundamentally different
photon-counting statistics. Furthermore, we show that coherent vibronic
mechanisms strongly affect the asymmetries characteristic of time-resolved
photon cross-correlations and manifest themselves in a time-dependent violation
of the Cauchy-Schwarz inequality bounding cross-correlations for classically
fluctuating fields. We finally discuss how such second-order correlation
asymmetry establishes important connections between coherent vibronic
interactions, directional exciton population transport, and violation of
quantum detailed balance. Our work then indicates that measurement of
two-colour photon correlation asymmetry can be an important avenue to
investigate quantum behaviour of single photoactive biomolecular and chemical
systems under incoherent illumination conditions.",2024-02-29,physics.chem-ph,
http://arxiv.org/abs/2404.00481v1,"Bayesian filtering serves as the mainstream framework of state estimation in
dynamic systems. Its standard version utilizes total probability rule and
Bayes' law alternatively, where how to define and compute conditional
probability is critical to state distribution inference. Previously, the
conditional probability is assumed to be exactly known, which represents a
measure of the occurrence probability of one event, given the second event. In
this paper, we find that by adding an additional event that stipulates an
inequality condition, we can transform the conditional probability into a
special integration that is analogous to convolution. Based on this
transformation, we show that both transition probability and output probability
can be generalized to convolutional forms, resulting in a more general
filtering framework that we call convolutional Bayesian filtering. This new
framework encompasses standard Bayesian filtering as a special case when the
distance metric of the inequality condition is selected as Dirac delta
function. It also allows for a more nuanced consideration of model mismatch by
choosing different types of inequality conditions. For instance, when the
distance metric is defined in a distributional sense, the transition
probability and output probability can be approximated by simply rescaling them
into fractional powers. Under this framework, a robust version of Kalman filter
can be constructed by only altering the noise covariance matrix, while
maintaining the conjugate nature of Gaussian distributions. Finally, we
exemplify the effectiveness of our approach by reshaping classic filtering
algorithms into convolutional versions, including Kalman filter, extended
Kalman filter, unscented Kalman filter and particle filter.",2024-03-30,stat.ML,
http://arxiv.org/abs/2404.00480v1,"A new thermodynamic theory for optical multimode systems is proposed. Theory
is based on a weighted Bose-Einstein law, and includes the state equation, the
fundamental equation for the entropy and a metric to measure the accuracy of
the thermodynamic approach. The theory is used to compare the experimental
results of two propagation regimes in multimode fibres, specifically the
self-cleaning in the normal chromatic dispersion region and the soliton
condensation in the anomalous dispersion region. Surprising similarities are
found in terms of thermodynamic parameters, suggesting a common basis for the
thermalisation processes observed in the two propagation regimes.",2024-03-30,physics.optics,
http://arxiv.org/abs/2404.00476v1,"We analyze particle number fluctuations in the crossover region near the
critical endpoint of a first-order phase transition by utilizing molecular
dynamics simulations of the classical Lennard-Jones fluid. We extend our
previous study [V.A. Kuznietsov et al., Phys. Rev. C 105, 044903 (2022)] by
incorporating longitudinal collective flow. The scaled variance of particle
number distribution inside different coordinate and momentum space acceptances
is computed through ensemble averaging and found to agree with earlier results
obtained using time averaging, validating the ergodic hypothesis for
fluctuation observables. Presence of a sizable collective flow is found to be
essential for observing large fluctuations from the critical point in momentum
space acceptances. We discuss our findings in the context of heavy-ion
collisions.",2024-03-30,nucl-th,
http://arxiv.org/abs/2404.00475v1,"In a single-item auction, a duplicitous seller may masquerade as one or more
bidders in order to manipulate the clearing price. This paper characterizes
auction formats that are shill-proof: a profit-maximizing seller has no
incentive to submit any shill bids. We distinguish between strong
shill-proofness, in which a seller with full knowledge of bidders' valuations
can never profit from shilling, and weak shill-proofness, which requires only
that the expected equilibrium profit from shilling is nonpositive. The Dutch
auction (with suitable reserve) is the unique optimal and strongly shill-proof
auction. Moreover, the Dutch auction (with no reserve) is the unique
prior-independent auction that is both efficient and weakly shill-proof. While
there are a multiplicity of strategy-proof, weakly shill-proof, and optimal
auctions; any optimal auction can satisfy only two properties in the set
{static, strategy-proof, weakly shill-proof}.",2024-03-30,econ.TH,
http://arxiv.org/abs/2404.00472v2,"The optomechanical systems produce chaotic behaviour due to nonlinear
interaction between photons and phonons, and the same systems are used to
understand the synthetic fields as well. Here, we report on the study of
chaotic behaviour in the presence of a phononic synthetic magnetic field in a
closed loop configuration consisting of a single optical mode and two
mechanical modes. The modulation phase of the mechanical coupling between the
two mechanical modes plays a critical role in determining the mechanical and
optical intensity dynamics in the nonlinear regime. Our study shows the dark
mode breaking effect in the presence of a synthetic magnetic field, which
brings about a complex way of mechanical energy exchange that causes the cavity
field to alternate between chaotic and regular behaviour periodically in
temporal domain. However in the stronger nonlinear regime the temporal dynamics
demonstrate predominantly chaotic behaviour. Besides, with the advent of
advanced fabrication technologies, this study holds promises in developing
phase tunable integrated low-power chaotic light sources to support efficient
optical secure communication systems.",2024-03-30,physics.optics,
http://arxiv.org/abs/2404.00471v1,"Photoacoustic tomography (PAT) is a rapidly-evolving medical imaging modality
that combines optical absorption contrast with ultrasound imaging depth. One
challenge in PAT is image reconstruction with inadequate acoustic signals due
to limited sensor coverage or due to the density of the transducer array. Such
cases call for solving an ill-posed inverse reconstruction problem. In this
work, we use score-based diffusion models to solve the inverse problem of
reconstructing an image from limited PAT measurements. The proposed approach
allows us to incorporate an expressive prior learned by a diffusion model on
simulated vessel structures while still being robust to varying transducer
sparsity conditions.",2024-03-30,physics.med-ph,
http://arxiv.org/abs/2404.00468v1,"Our main result, Theorem 3.3, uses Friedman's Jump Free Theorem, Theorem 2.7,
which he has shown to be independent of ZFC, the usual axioms of set theory. We
conjecture that Theorem 3.3, a straight forward translation of the statement of
Theorem 2.7 into sets and functions, is also independent of ZFC as is its
immediate Corollary 3.4. It is easy to show that a proof that P=NP will also
prove Corollary 3.4. If Corollary 3.4 is in fact independent of ZFC then a ZFC
proof of P=NP is impossible, perhaps because it is false.",2024-03-30,math.LO,
http://arxiv.org/abs/2404.00467v1,"The predictive capability of a plasma discharge model depends on accurate
representations of electron-impact collision cross sections, which determine
the key reaction rates and transport properties of the plasma. Although many
cross sections have been identified through experiments and quantum mechanical
simulations, their uncertainties are not well-investigated. We characterize the
uncertainties in electron-argon collision cross sections using a Bayesian
framework. Six collision processes -- elastic momentum transfer, ionization,
and four excitations -- are characterized with semi-empirical models, whose
parametric uncertainties effectively capture the features important to the
macroscopic properties of the plasma, namely transport properties and chemical
reaction rates. The method is designed to capture the effects of systematic
errors that lead to large discrepancies between some data sets. Specifically,
for the purposes of Bayesian inference, each of the parametric cross section
models is augmented with a Gaussian process representing systematic measurement
errors as well as model inadequacies in the parametric form. The results show
that the method is able to capture scatter in the data between the
electron-beam experiments and ab-initio quantum simulations. The calibrated
cross section models are further validated against measurements from
swarm-parameter experiments.",2024-03-30,physics.plasm-ph,
http://arxiv.org/abs/2404.00465v1,"We present a speculative exploration of the properties of a proposed new Dark
Matter (DM) candidate in a heretofore under-explored region of parameter space.
Our proposed ultra-cold candidatae has been a matter of speculation for some
time,and has recently been tentatively identified via direct-detection. While
unconventional, demonstrated existence of this DM candidate would have
wide-ranging implications for a range of fields, from particle cosmology to
exobiology and the Search for Extraterrestrial Life (SETI).",2024-03-30,astro-ph.CO,
http://arxiv.org/abs/2404.07996v1,"This paper introduces the concept of Fractal Frenet equations, a set of
differential equations used to describe the behavior of vectors along fractal
curves. The study explores the analogue of arc length for fractal curves,
providing a measure to quantify their length. It also discusses fundamental
mathematical constructs, such as the analogue of the unit tangent vector, which
indicates the curve's direction at different points, and the analogue of
curvature vector or fractal curvature vector, which characterizes its curvature
at various locations. The concept of torsion, describing the twisting and
turning of fractal curves in three-dimensional space, is also explored.
Specific examples, like the fractal helix and the fractal snowflake, illustrate
the application and significance of the Fractal Frenet equations.",2024-03-30,math.GM,
http://arxiv.org/abs/2404.00460v2,"IIn this article, we consider the weighted Steklov eigenvalue problem and the
weighted Schr\""odinger--Steklov eigenvalue problem in outward cuspidal domains.
We prove solvability of this spectral problem both in the linear and non-linear
case.",2024-03-30,math.AP,
http://arxiv.org/abs/2404.05738v1,"In this paper, a new generalization of third-order Jacobsthal bihyperbolic
polynomials is introduced. Some of the properties of presented polynomials are
given. A Vadja formula for the generalized bihyperbolic third-order Jacobsthal
polynomials is obtained. This result implies the Catalan, Cassini and d'Ocagne
identities. Moreover, generating function and matrix generators for these
polynomials are presented.",2024-03-30,math.GM,
http://arxiv.org/abs/2404.00454v1,"Helium burning is one of the most fundamental steps of stellar
nucleosynthesis, as it describes the formation of life-determining element of
carbon, while it plays a key role in the evolution of Red Giant, accreting
White Dwarfs and Neutron Stars. In this work we develop a generalized
statistical theory for the 3{\alpha} reaction, which is based on the use of the
Imaginary Time Method, along with the semi-classical Hybrid {\alpha}-Clustering
(H{\alpha}C) and Neck Model (NM) frameworks. The results compared to the
methodology and data of the NACRE collaboration, following in several orders of
magnitude discrepancies, especially at low temperatures. This may be crucial
for the early dynamics of helium burning stars.",2024-03-30,nucl-th,
http://arxiv.org/abs/2404.00453v1,"The predictability of climate anomalies in the regions of Northern Eurasia in
connection with El Nino phenomena is analyzed. Particular attention is paid to
the most likely transition in 2024 from an El Nino phase at the beginning of
the year to a La Nina phase at the end of the year, with the greatest
probability of high temperatures and dry conditions in European Russia during
the spring and summer months, as in 2010. The predictability levels of regional
climate anomalies using different El Nino indices are compared. The
relationship of the noted seasonal anomalies with atmospheric blockings is
considered, taking into account the different phases of the key modes of
climate variability like El Nino phenomena and the Pacific Decadal Oscillation.
Changes in the predictability of regional climate anomalies under global
climate change are discussed.",2024-03-30,physics.ao-ph,
http://arxiv.org/abs/2404.00449v2,"The origin and evolution of supermassive black holes (SMBHs) in our universe
have sparked controversy. In this study, we explore the hypothesis that some of
these black holes may have seeded from the direct collapse of dark energy
domains with density significantly higher than the surrounding regions. The
mechanism of the origin of such domains relies on the inflationary evolution of
a scalar field acting in D dimensions, which is associated with the
cosmological constant in our four-dimensional spacetime manifold. Inner space
quantum fluctuations of the field during inflation are responsible for the
spatial variations of the dark energy density in our space. This finding holds
particular significance, especially considering recent evidence from pulsar
timing array observations, which supports the existence of a stochastic
gravitational wave background consisting of SMBH mergers.",2024-03-30,astro-ph.CO,
http://arxiv.org/abs/2404.00448v1,"It is fascinating to predict the mass and width of the ordinary and exotic
mesons solely based on their quark content and quantum numbers. Such prediction
goes beyond conventional methodologies traditionally employed in hadron physics
for calculating or estimating these quantities. The relation between the
quantum numbers and the properties of the mesons, such as the mass and width,
is complicated in the world of particle physics. However, the deep neural
network (DNN) as a subfield of machine learning techniques provides a solution
to this problem. By analyzing large datasets, deep learning algorithms can
automatically identify complex patterns among the particles' quantum numbers,
and their mass and width, that would otherwise require complex calculations. In
this study, we present two approaches using the DNNs to estimate the mass of
some ordinary and exotic mesons. Also for the first time, the DNNs are trained
to predict the width of ordinary and exotic mesons, whose widths have not been
experimentally known. Our predictions obtained through the DNNs, will be useful
for future experimental searches.",2024-03-30,hep-ph,
http://arxiv.org/abs/2404.00440v1,"We prove sharp universal upper bounds on the number of steady and asymptotic
states of discrete- and continuous-time Markovian evolutions of open quantum
systems. We show that the bounds depend only on the dimension of the system and
not on the details of the dynamics. A comparison with similar bounds deriving
from a recent spectral conjecture for Markovian evolutions is also provided.",2024-03-30,quant-ph,
http://arxiv.org/abs/2404.00434v3,"Research on the operation of mobility systems so far has mostly focused on
minimizing cost-centered metrics such as average travel time, distance driven,
and operational costs. Whilst capturing economic indicators, such metrics do
not account for transportation justice aspects. In this paper, we present an
optimization model to plan the operation of Intermodal Autonomous
Mobility-on-Demand (I-AMoD) systems, where self-driving vehicles provide
on-demand mobility jointly with public transit and active modes, with the goal
to minimize the accessibility unfairness experienced by the population.
Specifically, we first leverage a previously developed network flow model to
compute the I-AMoD system operation in a minimum-time manner. Second, we
formally define accessibility unfairness, and use it to frame the
minimum-accessibility-unfairness problem and cast it as a linear program. We
showcase our framework for a real-world case-study in the city of Eindhoven,
NL. Our results show that it is possible to reach an operation that is on
average fully fair at the cost of a slight travel time increase compared to a
minimum-travel-time solution. Thereby we observe that the accessibility
fairness of individual paths is, on average, worse than the average values
obtained from flows, setting the stage for a discussion on the definition of
accessibility fairness itself.",2024-03-30,eess.SY,
http://arxiv.org/abs/2404.00432v1,"Feature compression is a promising direction for coding for machines.
Existing methods have made substantial progress, but they require designing and
training separate neural network models to meet different specifications of
compression rate, performance accuracy and computational complexity. In this
paper, a flexible variable-rate feature compression method is presented that
can operate on a range of rates by introducing a rate control parameter as an
input to the neural network model. By compressing different intermediate
features of a pre-trained vision task model, the proposed method can scale the
encoding complexity without changing the overall size of the model. The
proposed method is more flexible than existing baselines, at the same time
outperforming them in terms of the three-way trade-off between feature
compression rate, vision task accuracy, and encoding complexity. We have made
the source code available at
https://github.com/adnan-hossain/var_feat_comp.git.",2024-03-30,eess.IV,
http://arxiv.org/abs/2405.00041v1,"In this paper, we propose how to use objective arguments grounded in
statistical mechanics concepts in order to obtain a single number, obtained
after aggregation, which would allow to rank ""agents"", ""opinions"", ..., all
defined in a very broad sense. We aim toward any process which should a priori
demand or lead to some consensus in order to attain the presumably best choice
among many possibilities. In order to precise the framework, we discuss
previous attempts, recalling trivial ""means of scores"", - weighted or not,
Condorcet paradox, TOPSIS, etc. We demonstrate through geometrical arguments on
a toy example, with 4 criteria, that the pre-selected order of criteria in
previous attempts makes a difference on the final result. However, it might be
unjustified. Thus, we base our ""best choice theory"" on the linear response
theory in statistical mechanics: we indicate that one should be calculating
correlations functions between all possible choice evaluations, thereby
avoiding an arbitrarily ordered set of criteria. We justify the point through
an example with 6 possible criteria. Applications in many fields are suggested.
Beside, two toy models serving as practical examples and illustrative arguments
are given in an Appendix.",2024-03-30,physics.soc-ph,
http://arxiv.org/abs/2404.00424v1,"In traditional quantitative trading practice, navigating the complicated and
dynamic financial market presents a persistent challenge. Former machine
learning approaches have struggled to fully capture various market variables,
often ignore long-term information and fail to catch up with essential signals
that may lead the profit. This paper introduces an enhanced transformer
architecture and designs a novel factor based on the model. By transfer
learning from sentiment analysis, the proposed model not only exploits its
original inherent advantages in capturing long-range dependencies and modelling
complex data relationships but is also able to solve tasks with numerical
inputs and accurately forecast future returns over a period. This work collects
more than 5,000,000 rolling data of 4,601 stocks in the Chinese capital market
from 2010 to 2019. The results of this study demonstrated the model's superior
performance in predicting stock trends compared with other 100 factor-based
quantitative strategies with lower turnover rates and a more robust half-life
period. Notably, the model's innovative use transformer to establish factors,
in conjunction with market sentiment information, has been shown to enhance the
accuracy of trading signals significantly, thereby offering promising
implications for the future of quantitative trading strategies.",2024-03-30,q-fin.MF,
http://arxiv.org/abs/2404.00422v1,"We characterize the possible moments of entropy production for general
overdamped Markovian systems. We find a general formulation of the problem, and
derive a new necessary condition between the second and third moment. We
determine all possible first, second and third moments of entropy production
for a white noise process. As a consequence, we obtain a lower bound for the
skewness of the current fluctuations in dissipative devices such as
transistors, thereby demonstrating that the Gaussianity assumption widely used
in electronic engineering is thermodynamically inconsistent.",2024-03-30,cond-mat.stat-mech,
http://arxiv.org/abs/2404.00414v3,"Approximation theorem is one of the most important aspects of numerical
analysis that has evolved over the years with many different approaches. Some
of the most popular approximation methods include the Lebesgue approximation
theorem, the Weierstrass approximation, and the Fourier approximation theorem.
The limitations associated with various approximation methods are too crucial
to ignore, and thus, the nature of a specific dataset may require using a
specific approximation method for such estimates. In this report, we shall
delve into Chebyshev's polynomials interpolation in detail as an alternative
approach to reconstructing signals and compare the reconstruction to that of
the Fourier polynomials. We will also explore the advantages and limitations of
the Chebyshev polynomials and discuss in detail their mathematical formulation
and equivalence to the cosine function over a given interval [a, b].",2024-03-30,math.NA,
http://arxiv.org/abs/2404.00413v1,"Recent trends are emerging in the use of Large Language Models (LLMs) as
autonomous agents that take actions based on the content of the user text
prompts. We intend to apply these concepts to the field of Guidance,
Navigation, and Control in space, enabling LLMs to have a significant role in
the decision-making process for autonomous satellite operations. As a first
step towards this goal, we have developed a pure LLM-based solution for the
Kerbal Space Program Differential Games (KSPDG) challenge, a public software
design competition where participants create autonomous agents for maneuvering
satellites involved in non-cooperative space operations, running on the KSP
game engine. Our approach leverages prompt engineering, few-shot prompting, and
fine-tuning techniques to create an effective LLM-based agent that ranked 2nd
in the competition. To the best of our knowledge, this work pioneers the
integration of LLM agents into space research. Code is available at
https://github.com/ARCLab-MIT/kspdg.",2024-03-30,physics.space-ph,
http://arxiv.org/abs/2404.00411v1,"Machine learning is revolutionising medium-range weather prediction. However
it has only been applied to specific and individual components of the weather
prediction pipeline. Consequently these data-driven approaches are unable to be
deployed without input from conventional operational numerical weather
prediction (NWP) systems, which is computationally costly and does not support
end-to-end optimisation. In this work, we take a radically different approach
and replace the entire NWP pipeline with a machine learning model. We present
Aardvark Weather, the first end-to-end data-driven forecasting system which
takes raw observations as input and provides both global and local forecasts.
These global forecasts are produced for 24 variables at multiple pressure
levels at one-degree spatial resolution and 24 hour temporal resolution, and
are skillful with respect to hourly climatology at five to seven day lead
times. Local forecasts are produced for temperature, mean sea level pressure,
and wind speed at a geographically diverse set of weather stations, and are
skillful with respect to an IFS-HRES interpolation baseline at multiple
lead-times. Aardvark, by virtue of its simplicity and scalability, opens the
door to a new paradigm for performing accurate and efficient data-driven
medium-range weather forecasting.",2024-03-30,physics.ao-ph,
http://arxiv.org/abs/2404.00402v1,"Focusing on two-level atoms, we apply the positive P-representation to a
full-wave mixed bosonic and fermionic system of Jaynes-Cummings type and
identify an advantageous degree of freedom in the choice of the involved
nonorthogonal fermionic basis states. On this basis, we propose a stochastic
correction to the Maxwell-Bloch equations by relating them to a stochastic
differential equation on a nonclassical phase space, which captures the full
second quantization dynamics of the system. This approach explores the
connection between semiclassical and field-quantized treatments of light-matter
interaction and can potentially be used for the simulation of nonclassical
light sources while retaining the main advantages of a semiclassical model.",2024-03-30,quant-ph,
http://arxiv.org/abs/2404.00400v2,"Many transport processes in ecology, physics and biochemistry can be
described by the average time to first find a site or exit a region, starting
from an initial position. Typical mathematical treatments are based on
formulations that allow for various diffusive forms and geometries but where
only initial and final positions are taken into account. Here, we develop a
general theory for the mean first passage time (MFPT) for velocity jump
processes. For random walkers, both position and velocity are tracked and the
resulting Fokker-Planck equation takes the form of a kinetic transport
equation. Starting from the forward and backward formulations we derive a
general elliptic integro-PDE for the MFPT of a random walker starting at a
given location with a given velocity. We focus on two scenarios that are
relevant to biological modelling; the diffusive case and the anisotropic case.
For the anisotropic case we also perform a parabolic scaling, leading to a well
known anisotropic MFPT equation. To illustrate the results we consider a
two-dimensional circular domain under radial symmetry, where the MFPT equations
can be solved explicitly. Furthermore, we consider the MFPT of a random walker
in an ecological habitat that is perturbed by linear features, such as wolf
movement in a forest habitat that is crossed by seismic lines.",2024-03-30,math.AP,
http://arxiv.org/abs/2404.00394v1,"Active power curtailment of photovoltaic (PV) generation is commonly
exercised to mitigate over-voltage issues in power distribution networks.
However, fairness concerns arise as certain PV plants may experience more
significant curtailments than others depending on their locations within the
network. Existing literature tackles this issue through
fairness-promoting/aware optimization schemes. These schemes can be broadly
categorized into two types. The first type maximizes an additional fairness
objective along with the main objective of curtailment minimization. The second
type is formulated as a feedback controller, where fairness is accounted for by
assigning different weights (as feedback) in the curtailment minimization
objective for each PV plant based on previous curtailment actions. In this
work, we combine these two schemes and provide extensive analyses and
comparisons of these two fairness schemes. We compare the performance in terms
of fairness and net curtailments for several benchmark test networks.",2024-03-30,eess.SY,
http://arxiv.org/abs/2404.00393v1,"Spin-active defects in silicon carbide (SiC) are promising quantum light
sources for realizing scalable quantum technologies. In different applications,
these photoluminescent defects are often placed in a nanostructured host or
close to surfaces in order to enhance the signal from the defects. However,
proximity to the surface not only modifies frequencies of the quantum emission
from the defect, but also adversely affects their photo-stability, resulting in
blinking and/or photobleaching of the defect. These effects can be ameliorated
by passivating surfaces with optimal adsorbates. In this work, we explore
different passivation schemes using density functional theory-based
calculations. We show that a uniform surface passivation with either hydrogen
or with mixed hydrogen/hydroxyl groups completely removes surface states from
the SiC band gap, restoring the optical properties of the defects.",2024-03-30,cond-mat.mtrl-sci,
http://arxiv.org/abs/2404.00391v2,"We propose and analyse numerical schemes for a system of quasilinear,
degenerate evolution equations modelling biofilm growth as well as other
processes such as flow through porous media and the spreading of wildfires. The
first equation in the system is parabolic and exhibits degenerate and singular
diffusion, while the second is either uniformly parabolic or an ordinary
differential equation. First, we introduce a semi-implicit time discretisation
that has the benefit of decoupling the equations. We prove the positivity,
boundedness, and convergence of the time-discrete solutions to the
time-continuous solution. Then, we introduce an iterative linearisation scheme
to solve the resulting nonlinear time-discrete problems. Under weak assumptions
on the time-step size, we prove that the scheme converges irrespective of the
space discretisation and mesh. Moreover, if the problem is non-degenerate, the
convergence becomes faster as the time-step size decreases. Finally, employing
the finite element method for the spatial discretisation, we study the
behaviour of the scheme, and compare its performance to other commonly used
schemes. These tests confirm that the proposed scheme is robust and fast.",2024-03-30,math.NA,
http://arxiv.org/abs/2404.00390v1,"This article introduces a novel approach to learning monotone neural networks
through a newly defined penalization loss. The proposed method is particularly
effective in solving classes of variational problems, specifically monotone
inclusion problems, commonly encountered in image processing tasks. The
Forward-Backward-Forward (FBF) algorithm is employed to address these problems,
offering a solution even when the Lipschitz constant of the neural network is
unknown. Notably, the FBF algorithm provides convergence guarantees under the
condition that the learned operator is monotone. Building on plug-and-play
methodologies, our objective is to apply these newly learned operators to
solving non-linear inverse problems. To achieve this, we initially formulate
the problem as a variational inclusion problem. Subsequently, we train a
monotone neural network to approximate an operator that may not inherently be
monotone. Leveraging the FBF algorithm, we then show simulation examples where
the non-linear inverse problem is successfully solved.",2024-03-30,math.OC,
http://arxiv.org/abs/2404.00388v1,"This article continues the series of works by the authors on the
approximation of the electronic terms of diatomic molecules by the Morse
formula, which is the simplest anharmonic approximation of the real term U(r).
Depending on the choice of parameters, the approximation has two alternative
solutions M1(r) and M2(r), with different patterns of deviations from the real
term and its vibrational structure. The difference {\delta}(r)=U(r)-M(r)
quantitatively shows the changes in the shape of the terms during
approximation. We introduced an empirical anharmonicity function -2{\omega}_e
x(v), which characterizes the positions of vibrational levels in the potential
well; it demonstrates the distortion of the vibrational structure of the term
U(r) during the approximation. Based on the data from literature, the functions
{\delta}(r) and -2{\omega}_e x(v) were constructed for more than 20 molecules.
Here we present a group of simple terms with minimal deviations from the Morse
shape.",2024-03-30,physics.chem-ph,
http://arxiv.org/abs/2404.00382v1,"In this paper, we study non-homogeneous stochastic linear-quadratic (LQ)
optimal control problems with multi-dimensional state and regime switching. We
focus on the corresponding stochastic Riccati equation, which is the same as
that one in homogeneous stochastic LQ optimal control problem, and the adjoint
backward stochastic differential equation (BSDE), which arises from the
non-homogeneous terms in the state equation and cost functional. Both
stochastic Riccati equation and adjoint BSDE are solved by the contraction
mapping method, and are used to represent the closed-loop optimal control and
the optimal value of our problems.",2024-03-30,math.OC,
http://arxiv.org/abs/2404.00379v1,"We present a review of the Non-additive Stochastic Model for supercooled
liquids (NSM), an efficient approach for diffusive processes that provides a
suitable interpretation for the non-Arrhenius dynamics in these materials.
Based on a class of non-homogeneous continuity equations, the NSM provides
functions able to model the thermal behavior of the viscosity and diffusivity
in fragile liquids. The model defines a rigorous physical criterion that
distinguishes super-Arrhenius from sub-Arrhenius processes, establishes a
robust scale of fragility, describes fragile-to-strong curves in Angell's plot,
and provides an accurate fitting equation for experimental viscosity data as
effective as other viscosity classic models.",2024-03-30,cond-mat.stat-mech,
http://arxiv.org/abs/2404.00377v2,"The emerging field of free-electron quantum optics enables electron-photon
entanglement and holds the potential for generating nontrivial photon states
for quantum information processing. Although recent experimental studies have
entered the quantum regime, rapid theoretical developments predict that
qualitatively unique phenomena only emerge beyond a certain interaction
strength. It is thus pertinent to identify the maximal electron-photon
interaction strength and the materials, geometries, and particle energies that
enable one to approach it. We derive an upper limit to the quantum vacuum
interaction strength between free electrons and single-mode photons, which
illuminates the conditions for the strongest interaction. Crucially, we obtain
an explicit energy selection recipe for electrons and photons to achieve
maximal interaction at arbitrary separations and identify two optimal regimes
favoring either fast or slow electrons over those with intermediate velocities.
We validate the limit by analytical and numerical calculations on canonical
geometries and provide near-optimal designs indicating the feasibility of
strong quantum interactions. Our findings offer fundamental intuition for
maximizing the quantum interaction between free electrons and photons and
provide practical design rules for future experiments on electron-photon and
electron-mediated photon-photon entanglement. They should also enable the
evaluation of key metrics for applications such as the maximum power of
free-electron radiation sources and the maximum acceleration gradient of
dielectric laser accelerators.",2024-03-30,quant-ph,
http://arxiv.org/abs/2404.00374v1,"An examination of the constraints of quantum gravity leads to a clear
physical picture for how information about the initial state is transferred to
the Hawking radiation that emerges from a black hole.",2024-03-30,gr-qc,
http://arxiv.org/abs/2404.00370v1,"We consider the problem of inverse optimal control design for systems that
are not affine in the control. In particular, we consider some classes of
partial differential equations (PDEs) with quadratic convection and
counter-convection, for which the L2 norm is a control Lyapunov function (CLF)
whose derivative has either a depressed cubic or a quadratic dependence in the
boundary control input. We also consider diffusive PDEs with or without linear
convection, for which a weighted L2 norm is a CLF whose derivative has a
quadratic dependence in the control input. For each structure on the derivative
of the CLF, we achieve inverse optimality with respect to a meaningful cost
functional. For the case where the derivative of the CLF has a depressed cubic
dependence in the control, we construct a cost functional for which the unique
minimizer is the unique real root of a cubic polynomial: the Cardano-Lyapunov
controller. When the derivative of the CLF is quadratic in the control, we
construct a cost functional that is minimized by two distinct feedback laws,
that correspond to the two distinct real roots of a quadratic equation. We show
how to switch from one root to the other to reduce the control effort.",2024-03-30,eess.SY,
http://arxiv.org/abs/2404.00365v1,"A new age-distributed immuno-epidemiological model with information-based
vaccine uptake suggested in this work represents a system of
integro-differential equations for the numbers of susceptible individuals,
infected individuals, vaccinated individuals and recovered individuals. This
model describes the influence of vaccination decision on epidemic progression
in different age groups. We prove the existence and uniqueness of a positive
solution using the fixed point theory. In a particular case of age-independent
model, we determine the final size of epidemic, that is, the limiting number of
susceptible individuals at asymptotically large time. Numerical simulations
show that the information-based vaccine acceptance can significantly influence
the epidemic progression. Though the initial stage of epidemic progression is
the same for all memory kernels, as the epidemic progresses and more
information about the disease becomes available, further epidemic progression
strongly depends on the memory effect. Short-range memory kernel appears to be
more effective in restraining the epidemic outbreaks because it allows for more
responsive and adaptive vaccination decisions based on the most recent
information about the disease.",2024-03-30,q-bio.PE,
http://arxiv.org/abs/2404.00363v1,"According to the hypothesis that strange quark matter may be the true ground
state of matter at extremely high densities, strange quark stars should be
stable and could exist in the Universe. It is possible that pulsars may
actually be strange stars, but not neutron stars. Here we present a short
review on recent progresses in the field of strange quark stars. Three popular
phenomenological models widely used to describe strange quark matter are
introduced, with special attention being paid on the corresponding equation of
state in each model. Combining the equation of state with the
Tolman-Oppenheimer-Volkov equations, the inner structure and mass-radius
relation can be obtained for the whole sequence of strange stars. Strong
gravitational wave emissions may be generated by strange stars through various
mechanisms, which may help identify strange stars via observations. Especially,
close-in strange quark planets with respect to their hosts may provide a unique
test for the existence of strange quark objects. Fierce electromagnetic bursts
could also be generated by strange stars. The energy may come from the phase
transition of neutron stars to strange stars, or from the merger of binary
strange stars. The collapse of the strange star crust can also release a huge
amount of energy. It is shown that strange quark stars may be involved in short
gamma-ray bursts and fast radio bursts.",2024-03-30,astro-ph.HE,
http://arxiv.org/abs/2404.00359v1,"We present a novel prior for tree topology within Bayesian Additive
Regression Trees (BART) models. This approach quantifies the hypothetical loss
in information and the loss due to complexity associated with choosing the
wrong tree structure. The resulting prior distribution is compellingly geared
toward sparsity, a critical feature considering BART models' tendency to
overfit. Our method incorporates prior knowledge into the distribution via two
parameters that govern the tree's depth and balance between its left and right
branches. Additionally, we propose a default calibration for these parameters,
offering an objective version of the prior. We demonstrate our method's
efficacy on both simulated and real datasets.",2024-03-30,stat.ME,
http://arxiv.org/abs/2404.00355v1,"In this study, a procedure for designing a free-form lens for long-range LED
illumination is presented. The geometrical form of the proposed lens is
obtained by minimizing optical path lengths of the rays emitted from a
point-like light source. Optical ray tracing simulations of two different LEDs
and the free-form lens are performed by using Zemax OpticStudio. In addition,
the prototype of the free-form lens is manufactured by the plastic injection
molding method using PMMA material. Nine of the lenses are used to build an LED
projector in the form of a 3x3 lens matrix. The optical measurements of the
projector are compared with the results predicted in the simulations. It is
found that the beam divergence of the projector is less than 10 degrees when
using suitable LEDs in visible and near infrared regions.",2024-03-30,physics.optics,
http://arxiv.org/abs/2404.00352v1,"Stable Diffusion is a popular Transformer-based model for image generation
from text; it applies an image information creator to the input text and the
visual knowledge is added in a step-by-step fashion to create an image that
corresponds to the input text. However, this diffusion process can be corrupted
by errors from the underlying hardware, which are especially relevant for
implementations at the nanoscales. In this paper, the dependability of Stable
Diffusion is studied focusing on soft errors in the memory that stores the
model parameters; specifically, errors are injected into some critical layers
of the Transformer in different blocks of the image information creator, to
evaluate their impact on model performance. The simulations results reveal
several conclusions: 1) errors on the down blocks of the creator have a larger
impact on the quality of the generated images than those on the up blocks,
while the errors on middle block have negligible effect; 2) errors on the
self-attention (SA) layers have larger impact on the results than those on the
cross-attention (CA) layers; 3) for CA layers, errors on deeper levels result
in a larger impact; 4) errors on blocks at the first levels tend to introduce
noise in the image, and those on deep layers tend to introduce large colored
blocks. These results provide an initial understanding of the impact of errors
on Stable Diffusion.",2024-03-30,eess.IV,
http://arxiv.org/abs/2404.00348v1,"We study in this paper optimal mass transport over a strongly connected,
directed graph on a given discrete time interval. Differently from previous
literature, we do not assume full knowledge of the initial and final goods
distribution over the network nodes. In spite of the meager information, we
show that it is possible to characterize the most likely flow in two important
cases: The first one is when the initial and/or final distribution is only
known on proper subsets of the nodes. The second case is when only some moments
of the marginal distributions are known. As an important by-product, we
determine the most likely initial and final marginals on the whole state space.",2024-03-30,math.PR,
http://arxiv.org/abs/2404.00347v2,"The Vicsek-BGK equation is a kinetic model for alignment of particles moving
with constant speed between stochastic reorientation events with sampling from
a von Mises distribution. The spatially homogeneous model shows a steady state
bifurcation with exchange of stability. The main result of this work is an
extension of the bifurcation result to the spatially inhomogeneous problem
under the additional assumption of a sufficiently large Knudsen number, which
is shown to be necessary. A counterexample is derived in the form of an
exponentially increasing travelling wave solution of the linearized equation,
indicating a possible explanation of traveling bands, which have been observed
in the literature in simulations with an individual based model. The
mathematical core is the proof of linearized stability, which employs a new
hypocoercivity approach based on Laplace-Fourier transformation. The
bifurcation result includes global existence of smooth solutions for
close-to-equilibrium initial data. For large data smooth solutions might blow
up in finite time whereas weak solutions with bounded Boltzmann entropy are
shown to exist globally.",2024-03-30,math.AP,
http://arxiv.org/abs/2404.00342v1,"Hyperentangled swapping is a quantum communication technique that involves
the exchange of hyperentangled states, which are quantum states entangled in
multiple degrees of freedom, to enable secure and efficient quantum information
transfer. In this paper, we demonstrate schematics for the hyperentanglement
swapping between separate pairs of neutral atoms through the mathematical
framework of atomic Bragg diffraction, which is efficient and resistant to
decoherence, yielding deterministic results with superior overall fidelity. The
utilized cavities are in superposition state and interact with the incoming
atoms off-resonantly. Quantum information carried by the cavities is swapped
through resonant interactions with two-level auxiliary atoms. We also discuss
entanglement swapping under a delayed-choice scenario and provide a schematic
generalization covering multiple-qubit scenarios. Finally, we introduce
specific experimental parameters to demonstrate the experimental feasibility of
the scheme.",2024-03-30,quant-ph,
http://arxiv.org/abs/2404.00336v1,"We prove that the extremal length systole of the cube punctured at its
vertices is realized by the 12 curves surrounding its edges and give a
characterization of the corresponding quadratic differentials, allowing us to
estimate its value to high precision. The proof uses a mixture of exact
calculations done using branched covers and elliptic integrals, together with
estimates obtained using either the geometry of geodesic trajectories on the
cube or explicit conformal maps.",2024-03-30,math.GT,
http://arxiv.org/abs/2404.00331v1,"Self-focusing emerges as a nonlinear optical phenomenon resulting from an
intense laser field and plasma interaction. This study investigates the
self-focusing behavior of Gaussian laser beams within magnetized plasma
environments utilizing a novel approach, source-dependent expansion. By
employing source-dependent expansion, we explore the intricate dynamics of
laser beam propagation, considering the influence of plasma density and
external magnetic fields. The interplay between the beam's Gaussian profile and
the self-focusing mechanism through rigorous mathematical analysis and
numerical simulations, particularly in the presence of plasma-induced
nonlinearities are elucidated here. Our findings reveal crucial insights into
the evolution of laser beams under diverse parameters, the ponderomotive force,
relativistic factors, plasma frequency, polarization states, external magnetic
field, wavelength, and laser intensity. This research not only contributes to
advancing our fundamental understanding of laser-plasma interactions but also
holds promise for optimizing laser-driven applications.",2024-03-30,physics.plasm-ph,
http://arxiv.org/abs/2404.00326v1,"The isentropic compressible Cahn-Hilliard-Navier-Stokes equations is a system
of fourth-order partial differential equations that model the evolution of some
binary fluids under convection.
  The purpose of this paper is the design of efficient numerical schemes to
approximate the solution of initial-boundary value problems with these
equations. The efficiency stems from the implicit treatment of the high-order
terms in the equations. Our proposal is a second-order linearly
implicit-explicit time stepping scheme applied in a method of lines approach,
in which the convective terms are treated explicitly and only linear systems
have to be solved.
  Some experiments are performed to assess the validity and efficiency of this
proposal.",2024-03-30,math.NA,
http://arxiv.org/abs/2404.00327v1,"Background: Liver tumors are abnormal growths in the liver that can be either
benign or malignant, with liver cancer being a significant health concern
worldwide. However, there is no dataset for plain scan segmentation of liver
tumors, nor any related algorithms. To fill this gap, we propose Plain Scan
Liver Tumors(PSLT) and YNetr. Methods: A collection of 40 liver tumor plain
scan segmentation datasets was assembled and annotated. Concurrently, we
utilized Dice coefficient as the metric for assessing the segmentation outcomes
produced by YNetr, having advantage of capturing different frequency
information. Results: The YNetr model achieved a Dice coefficient of 62.63% on
the PSLT dataset, surpassing the other publicly available model by an accuracy
margin of 1.22%. Comparative evaluations were conducted against a range of
models including UNet 3+, XNet, UNetr, Swin UNetr, Trans-BTS, COTr, nnUNetv2
(2D), nnUNetv2 (3D fullres), MedNext (2D) and MedNext(3D fullres). Conclusions:
We not only proposed a dataset named PSLT(Plain Scan Liver Tumors), but also
explored a structure called YNetr that utilizes wavelet transform to extract
different frequency information, which having the SOTA in PSLT by experiments.",2024-03-30,eess.IV,
http://arxiv.org/abs/2404.00324v1,"A connected graph G is 3-flow-critical if G does not have a nowhere-zero
3-flow, but every proper contraction of G does. We prove that every n-vertex
3-flow-critical graph other than K_2 and K_4 has at least 5n/3 edges. This
bound is tight up to lower-order terms, answering a question of Li et al.
(2022). It also generalizes the result of Koester (1991) on the maximum average
degree of 4-critical planar graphs.",2024-03-30,math.CO,
http://arxiv.org/abs/2404.19162v1,"A scalar field with an exponential potential in FLRW universe admits the
exact solution. We uncover the hidden symmetries behind the system by utilising
the Eisenhart lift of field theories. We find that a conformal Killing vector
field in the field space exists only for a particular combination of
exponential functions which includes a single exponential potential. This
implies the existence of additional conserved quantity and explains the
integrability of the system.",2024-04-29,gr-qc,
http://arxiv.org/abs/2404.19161v1,"Silicon is a key semiconducting material for electrical devices and hybrid
quantum systems where low temperatures and zero-spin isotopic purity can
enhance quantum coherence. Electrical conductivity in Si is characterised by
carrier freeze out at around 40 K allowing microwave transmission which is a
key component for addressing spins efficiently in silicon quantum technologies.
In this work, we report an additional sharp transition of the electrical
conductivity in a Si-28 cylindrical cavity at around 1 Kelvin. This is observed
by measuring microwave resonator Whispering Gallery Mode frequencies and Q
factors with changing temperature and comparing these results with finite
element models. We attribute this change to a transition from a relaxation
mechanism-dominated to a resonant phonon-less absorption-dominated hopping
conduction regime. Characterising this regime change represents a deeper
understanding of a physical phenomenon in a material of high interest to the
quantum technology and semiconductor device community and the impact of these
results is discussed.",2024-04-29,cond-mat.mtrl-sci,
http://arxiv.org/abs/2404.19160v1,"The vacuum of an accelerating observer is defined on the local inertial
frame, which is called a ``moving frame''. However, in the discussion of the
Unruh-deWitt detector, many papers define the vacuum on a fixed frame. This
paper discusses the Unruh-deWitt detector by defining the vacuum directly on
the local inertial frame, showing that the problem of the Stokes phenomenon can
be solved by using the exact WKB.",2024-04-29,hep-th,
http://arxiv.org/abs/2404.19158v1,"The three-dimensional organization of chromatin is thought to play an
important role in controlling gene expression. Specificity in expression is
achieved through the interaction of transcription factors and other nuclear
proteins with particular sequences of DNA. At unphysiological concentrations
many of these nuclear proteins can phase-separate in the absence of DNA, and it
has been hypothesized that, in vivo, the thermodynamic forces driving these
phases help determine chromosomal organization. However it is unclear how DNA,
itself a long polymer subject to configurational transitions, interacts with
three-dimensional protein phases. Here we show that a long compressible polymer
can be coupled to interacting protein mixtures, leading to a generalized
prewetting transition where polymer collapse is coincident with a locally
stabilized liquid droplet. We use lattice Monte-Carlo simulations and a
mean-field theory to show that these phases can be stable even in regimes where
both polymer collapse and coexisting liquid phases are unstable in isolation,
and that these new transitions can be either abrupt or continuous. For polymers
with internal linear structure we further show that changes in the
concentration of bulk components can lead to changes in three-dimensional
polymer structure. In the nucleus there are many distinct proteins that
interact with many different regions of chromatin, potentially giving rise to
many different Prewet phases. The simple systems we consider here highlight
chromatin's role as a lower-dimensional surface whose interactions with
proteins are required for these novel phases.",2024-04-29,physics.bio-ph,
http://arxiv.org/abs/2404.19157v1,"Large neural networks trained on large datasets have become the dominant
paradigm in machine learning. These systems rely on maximum likelihood point
estimates of their parameters, precluding them from expressing model
uncertainty. This may result in overconfident predictions and it prevents the
use of deep learning models for sequential decision making. This thesis
develops scalable methods to equip neural networks with model uncertainty. In
particular, we leverage the linearised Laplace approximation to equip
pre-trained neural networks with the uncertainty estimates provided by their
tangent linear models. This turns the problem of Bayesian inference in neural
networks into one of Bayesian inference in conjugate Gaussian-linear models.
Alas, the cost of this remains cubic in either the number of network parameters
or in the number of observations times output dimensions. By assumption,
neither are tractable. We address this intractability by using stochastic
gradient descent (SGD) -- the workhorse algorithm of deep learning -- to
perform posterior sampling in linear models and their convex duals: Gaussian
processes. With this, we turn back to linearised neural networks, finding the
linearised Laplace approximation to present a number of incompatibilities with
modern deep learning practices -- namely, stochastic optimisation, early
stopping and normalisation layers -- when used for hyperparameter learning. We
resolve these and construct a sample-based EM algorithm for scalable
hyperparameter learning with linearised neural networks. We apply the above
methods to perform linearised neural network inference with ResNet-50 (25M
parameters) trained on Imagenet (1.2M observations and 1000 output dimensions).
Additionally, we apply our methods to estimate uncertainty for 3d tomographic
reconstructions obtained with the deep image prior network.",2024-04-29,stat.ML,
http://arxiv.org/abs/2404.19153v2,"Chiral magnets are materials which possess unique helical arrangements of
magnetic moments, which give rise to nonreciprocal transport and fascinating
physics phenomena. On the one hand, their exploration is guided by the
prospects of unconventional signal processing, computation schemes and magnetic
memory. On the other hand, progress in applications is hindered by the
challenging materials synthesis, limited scalability and typically low critical
temperature. Here, we report the creation and exploration of artificial chiral
magnets (ACMs) at room temperature. By employing a mass production compatible
deposition technology, we synthesize ACMs, which consist of helical Ni surfaces
on central cylinders. Using optical microscopy, we reveal nonreciprocal magnon
transport at GHz frequencies. It is controlled by programmable toroidal moments
which result from the ACM's geometrical handedness and field-dependent spin
chirality. We present materials-by-design rules which optimize the helically
curved ferromagnets for 3D nonreciprocal transport at room temperature and zero
magnetic field.",2024-04-29,cond-mat.mtrl-sci,
http://arxiv.org/abs/2404.19152v1,"Traditional ferroelectrics are limited by Neumann's principle, which confines
exploration of ferroelectrics within polar point groups. Our recent work [Nat.
Commun. 15, 135, (2024)] proposes the concept of fractional quantum
ferroelectricity (FQFE) that extend the playground of ferroelectricity to
non-polar point groups. Here, we apply group theory and introduce an efficient
symmetry strategy to identify FQFE candidates. Integrated with a
high-throughput screening scheme, we go through 171,527 materials and identify
202 potential FQFE candidates, which are already experimentally synthesized. In
addition, we point out that the essence of FQFE is fractional atomic
displacements with respect to lattice vectors, which can actually result in
both fractional (type-I) and integer (type-II) quantized polarization,
respectively. Through performing first-principles calculations, we verify the
symmetry-predicted switchable FQFE properties in bulk AlAgS2 and monolayer
HgI2. Notably, AlAgS2 exhibits an ultra-low switching barrier of 23 meV/f.u.
and interlocked in-plane/out-of-plane polarization, while HgI2 demonstrates
large spontaneous polarization of 42 {\mu}C/cm2. Our findings not only advance
the understanding on FQFE, but also offer guidance for experimental exploration
and design of novel ferroelectric materials.",2024-04-29,cond-mat.mtrl-sci,
http://arxiv.org/abs/2404.19150v1,"Let k be a field finitely generated over its prime subfield. We prove that
the quotient of the Brauer group of a product of varieties over k by the Brauer
groups of factors has finite exponent. The bulk of the proof concerns p-primary
torsion in characteristic p. Our approach gives a more direct proof of the
boundedness of the p-primary torsion of the Brauer group of an abelian variety,
as recently proved by D'Addezio. We show that the transcendental Brauer group
of a Kummer surface over k has finite exponent, but can be infinite when k is
an infinite field of positive characteristic. This answers a question of Zarhin
and the author.",2024-04-29,math.AG,
http://arxiv.org/abs/2404.19145v2,"Bootstrap is a popular methodology for simulating input uncertainty. However,
it can be computationally expensive when the number of samples is large. We
propose a new approach called \textbf{Orthogonal Bootstrap} that reduces the
number of required Monte Carlo replications. We decomposes the target being
simulated into two parts: the \textit{non-orthogonal part} which has a
closed-form result known as Infinitesimal Jackknife and the \textit{orthogonal
part} which is easier to be simulated. We theoretically and numerically show
that Orthogonal Bootstrap significantly reduces the computational cost of
Bootstrap while improving empirical accuracy and maintaining the same width of
the constructed interval.",2024-04-29,stat.ME,
http://arxiv.org/abs/2404.19144v1,"I propose a locally robust semiparametric framework for estimating causal
effects using the popular examiner IV design, in the presence of many examiners
and possibly many covariates relative to the sample size. The key ingredient of
this approach is an orthogonal moment function that is robust to biases and
local misspecification from the first step estimation of the examiner IV. I
derive the orthogonal moment function and show that it delivers multiple
robustness where the outcome model or at least one of the first step components
is misspecified but the estimating equation remains valid. The proposed
framework not only allows for estimation of the examiner IV in the presence of
many examiners and many covariates relative to sample size, using a wide range
of nonparametric and machine learning techniques including LASSO, Dantzig,
neural networks and random forests, but also delivers root-n consistent
estimation of the parameter of interest under mild assumptions.",2024-04-29,econ.EM,
http://arxiv.org/abs/2404.19140v1,"Quantum many-body systems serve as a suitable working medium for realizing
quantum thermal machines (QTMs) by offering distinct advantages such as
cooperative many-body effects, and performance boost at the quantum critical
points. However, the bulk of the existing literature exploring the criticality
of many-body systems in the context of QTMs involves models sans the electronic
interactions, which are non-trivial to deal with and require sophisticated
numerical techniques. Here we adopt the prototypical Hubbard model in two
dimensions (2D) in the framework of the line graph Lieb-kagome lattice for the
working medium of a multi-functional QTM. We resort to a non-perturbative,
static path approximated (SPA) Monte Carlo technique to deal with the repulsive
Hubbard model. We observe that in a Stirling cycle, in both the interacting and
non-interacting limits, the heat engine function dominates and its performance
gets better when the strain is induced from the kagome to the Lieb limit, while
for the reverse the refrigeration action is preferred. Further, we show that
the QTM performs better when the difference between the temperatures of the two
baths is lower and the QTM reaches the Carnot limit in this regime. Further, we
extensively study the performance of the QTM in the repulsive Hubbard
interacting regime where the magnetic orders come into the picture. We explore
the performance of the QTM along the quantum critical points and in the large
interaction limit.",2024-04-29,quant-ph,
http://arxiv.org/abs/2404.19133v2,"We develop a fast and scalable numerical approach to solve Wasserstein
gradient flows (WGFs), particularly suitable for high-dimensional cases. Our
approach is to use general reduced-order models, like deep neural networks, to
parameterize the push-forward maps such that they can push a simple reference
density to the one solving the given WGF. The new dynamical system is called
parameterized WGF (PWGF), and it is defined on the finite-dimensional parameter
space equipped with a pullback Wasserstein metric. Our numerical scheme can
approximate the solutions of WGFs for general energy functionals effectively,
without requiring spatial discretization or nonconvex optimization procedures,
thus avoiding some limitations of classical numerical methods and more recent
deep-learning-based approaches. A comprehensive analysis of the approximation
errors measured by Wasserstein distance is also provided in this work.
Numerical experiments show promising computational efficiency and verified
accuracy on various WGF examples using our approach.",2024-04-29,math.NA,
http://arxiv.org/abs/2404.19131v1,"The Earth is no longer the only known celestial body containing one or more
liquid phases. The Cassini spacecraft has discovered seas of hydrocarbons at
the surface of Titan, while a series of corroborating evidences argue in favour
of the existence of an aqueous ocean beneath the icy crust of several moons.
Capillarity embraces a family of physical processes occurring at the free
surface of a liquid. These phenomena depend on the liquid properties and on the
local planetary conditions. Capillarity may have important direct or indirect
implications on the geoscientific and astrobiological points of view. In this
paper, we discuss capillarity physics among solar system objects and expected
consequences for planetary science.",2024-04-29,astro-ph.EP,
http://arxiv.org/abs/2404.19127v1,"Subdata selection is a study of methods that select a small representative
sample of the big data, the analysis of which is fast and statistically
efficient. The existing subdata selection methods assume that the big data can
be reasonably modeled using an underlying model, such as a (multinomial)
logistic regression for classification problems. These methods work extremely
well when the underlying modeling assumption is correct but often yield poor
results otherwise. In this paper, we propose a model-free subdata selection
method for classification problems, and the resulting subdata is called PED
subdata. The PED subdata uses decision trees to find a partition of the data,
followed by selecting an appropriate sample from each component of the
partition. Random forests are used for analyzing the selected subdata. Our
method can be employed for a general number of classes in the response and for
both categorical and continuous predictors. We show analytically that the PED
subdata results in a smaller Gini than a uniform subdata. Further, we
demonstrate that the PED subdata has higher classification accuracy than other
competing methods through extensive simulated and real datasets.",2024-04-29,stat.ME,
http://arxiv.org/abs/2404.19120v1,"Riemann surfaces are among the simplest and most basic geometric objects.
They appear as key players in many branches of physics, mathematics, and other
sciences. Despite their widespread significance, how to compute distances
between pairs of points on compact Riemann surfaces is surprisingly unknown,
unless the surface is a sphere or a torus. This is because on higher-genus
surfaces, the distance formula involves an infimum over infinitely many terms,
so it cannot be evaluated in practice. Here we derive a computable distance
formula for a broad class of Riemann surfaces. The formula reduces the infimum
to a minimum over an explicit set consisting of finitely many terms. We also
develop a distance computation algorithm, which cannot be expressed as a
formula, but which is more computationally efficient on surfaces with high
genuses. We illustrate both the formula and the algorithm in application to
generalized Bolza surfaces, which are a particular class of highly symmetric
compact Riemann surfaces of any genus greater than 1.",2024-04-29,math.GT,
http://arxiv.org/abs/2405.15785v1,"We introduce the notion of cosymplectic structure on Jacobi-Jordan algebras,
and we state that they are related to symplectic Jacobi-Jordan algebras. We
show, in particular, that they support a right-skew-symmetric product. We also
study the double extension constructions of cosymplectic Jacobi-Jordan algebras
and give a complete classification in dimension five.",2024-04-29,math.RA,
http://arxiv.org/abs/2404.19118v1,"Platform trials are multi-arm designs that simultaneously evaluate multiple
treatments for a single disease within the same overall trial structure. Unlike
traditional randomized controlled trials, they allow treatment arms to enter
and exit the trial at distinct times while maintaining a control arm
throughout. This control arm comprises both concurrent controls, where
participants are randomized concurrently to either the treatment or control
arm, and non-concurrent controls, who enter the trial when the treatment arm
under study is unavailable. While flexible, platform trials introduce a unique
challenge with the use of non-concurrent controls, raising questions about how
to efficiently utilize their data to estimate treatment effects. Specifically,
what estimands should be used to evaluate the causal effect of a treatment
versus control? Under what assumptions can these estimands be identified and
estimated? Do we achieve any efficiency gains? In this paper, we use structural
causal models and counterfactuals to clarify estimands and formalize their
identification in the presence of non-concurrent controls in platform trials.
We also provide outcome regression, inverse probability weighting, and doubly
robust estimators for their estimation. We discuss efficiency gains,
demonstrate their performance in a simulation study, and apply them to the ACTT
platform trial, resulting in a 20% improvement in precision.",2024-04-29,stat.ME,
http://arxiv.org/abs/2405.00070v1,"Synthetic microbiomes offer new possibilities for modulating microbiota, to
address the barriers in multidtug resistance (MDR) research. We present a
Bayesian optimization approach to enable efficient searching over the space of
synthetic microbiome variants to identify candidates predictive of reduced MDR.
Microbiome datasets were encoded into a low-dimensional latent space using
autoencoders. Sampling from this space allowed generation of synthetic
microbiome signatures. Bayesian optimization was then implemented to select
variants for biological screening to maximize identification of designs with
restricted MDR pathogens based on minimal samples. Four acquisition functions
were evaluated: expected improvement, upper confidence bound, Thompson
sampling, and probability of improvement. Based on each strategy, synthetic
samples were prioritized according to their MDR detection. Expected
improvement, upper confidence bound, and probability of improvement
consistently produced synthetic microbiome candidates with significantly fewer
searches than Thompson sampling. By combining deep latent space mapping and
Bayesian learning for efficient guided screening, this study demonstrated the
feasibility of creating bespoke synthetic microbiomes with customized MDR
profiles.",2024-04-29,q-bio.QM,
http://arxiv.org/abs/2404.19116v1,"Starting from Robbins (1952), the literature on experimentation via
multi-armed bandits has wed exploration and exploitation. Nonetheless, in many
applications, agents' exploration and exploitation need not be intertwined: a
policymaker may assess new policies different than the status quo; an investor
may evaluate projects outside her portfolio. We characterize the optimal
experimentation policy when exploration and exploitation are disentangled in
the case of Poisson bandits, allowing for general news structures. The optimal
policy features complete learning asymptotically, exhibits lots of persistence,
but cannot be identified by an index a la Gittins. Disentanglement is
particularly valuable for intermediate parameter values.",2024-04-29,econ.TH,
http://arxiv.org/abs/2404.19115v2,"The electrical impedance tomography (EIT) problem of estimating the unknown
conductivity distribution inside a domain from boundary current or voltage
measurements requires the solution of a nonlinear inverse problem. Sparsity
promoting hierarchical Bayesian models have been shown to be very effective in
the recovery of almost piecewise constant solutions in linear inverse problems.
We demonstrate that by exploiting linear algebraic considerations it is
possible to organize the calculation for the Bayesian solution of the nonlinear
EIT inverse problem via finite element methods with sparsity promoting priors
in a computationally efficient manner. The proposed approach uses the Iterative
Alternating Sequential (IAS) algorithm for the solution of the linearized
problems. Within the IAS algorithm, a substantial reduction in computational
complexity is attained by exploiting the low dimensionality of the data space
and an adjoint formulation of the Tikhonov regularized solution that
constitutes part of the iterative updating scheme. Numerical tests illustrate
the computational efficiency of the proposed algorithm. The paper sheds light
also on the convexity properties of the objective function of the maximum a
posteriori (MAP) estimation problem.",2024-04-29,math.NA,
http://arxiv.org/abs/2404.19107v1,"We report on follow-up observations of the Seyfert 1.9 galaxy IC 3599 with
the NASA Neil Gehrels Swift mission. The detection of a second X-ray outburst
in 2010 by Swift after the first discovery of a bright X-ray outburst in 1990
by ROSAT led to the suggestion of two very different explanations: The first
one assumed that IC 3599 exhibits outbursts due to repeated partial tidal
stripping of a star, predicting another outburst of IC 3599 in 2019/2020. The
second, alternative scenario assumed that the event observed in X-rays is due
to an accretion disk instability which would suggest a much longer period
between the large outbursts. Our continued monitoring campaign by Swift allowed
us to test the first scenario which predicted a repetition of high amplitude
flaring activity in 2019/2020. We do not find any evidence of dramatic flaring
activity with factors of 100 since the last X-ray outburst seen in 2010. These
observations support the accretion disk scenario. Further, while IC 3599
remains in low emission states, the long-term X-ray light curve of IC 3599
reveals ongoing strong variability of a factor of a few. The most remarkable
event is a mini flare of a factor of 10 in X-rays in December 2022. After that
flare, the otherwise supersoft X-ray spectrum shows an exceptional hardening,
reminiscent of a temporary corona formation.",2024-04-29,astro-ph.HE,
http://arxiv.org/abs/2404.19106v1,"Starting from the effective Hamiltonian arising from the tight binding model,
we study the behaviour of low-lying excitations for bilayer graphene placed in
periodic external magnetic fields by using irreducible second order
supersymmetry transformations. The coupled system of equations describing these
excitations is reduced to a pair of periodic Schr\""odinger Hamiltonians
intertwined by a second order differential operator. The direct implementation
of more general second-order supersymmetry transformations allows to create
nonsingular Schr\""odinger potentials with periodicity defects and bound states
embedded in the forbidden bands, which turn out to be associated to
quasiperiodic magnetic superlattices.",2024-04-29,cond-mat.mes-hall,
http://arxiv.org/abs/2404.19101v1,"The trending term ""filament"" is extensively used in the interstellar medium
(ISM) and the star formation community, and is believed to be one of the most
important objects that gauge molecular cloud and star formation. However, the
physical definition of these ubiquitous, elongated, high contrast features is
poorly defined and still actively debated. Despite the absence of a unified
consensus, filaments are believed to be involved in many important physical
processes from galaxy structure formation to the emergence of protostellar
objects. Therefore, understanding how filaments form, what constrains their
growth, and their general physical properties, are extremely important for
theorists and observers who study the dynamics of the ISM and consequent star
formations. This review serves as a collection of the community's views and
develops the concept of ""filaments"" in the context of the ISM and star-forming
clouds. Observationally, filaments are seen across the entire sky and often
carry an aspect ratio of the order of hundreds. In the context of the ISM,
filaments are believed to form by stretching and tearing from magnetized ISM
turbulence. ISM filaments are subjected to heating and cooling phases, and are
likely to be magnetically aligned. Cold clouds are formed inside ISM due to
turbulence instability. This review updates the understanding of ISM filaments
in the community.",2024-04-29,astro-ph.GA,
http://arxiv.org/abs/2404.19099v1,"In this work, we are interested in problems that are related to the physical
phenomena of diffusion. We will focus on the theoretical aspect of the study,
such as existence, uniqueness and non-explosive solutions. We will weaken the
conditions imposed on the coefficients of the stochastic differential equations
(SDE) that model some diffusion phenomena of mechanics. The work will be based
on a general non-explosion criterion and we will obtain sufficient conditions
so that the solution for a certain class of diffusions does not explode. We
will construct Lyapunov functions that ensure the non-explosion of the
solutions. Two important oscillators, namely the Duffing and the Van Der Pol
oscillators, belong to this class. The Euler-Maruyama method is applied to
these two oscillators to give us a simulation solution for them.",2024-04-29,math.DS,
http://arxiv.org/abs/2404.19098v1,"In the present work, the ideal shear strength (Tis) of dilute Ni34XZ ternary
alloys (X or Z = Al, Co, Cr, Fe, Mn, Mo, Nb, Si, Ti) are predicted by
first-principles calculations based on density functional theory (DFT) in terms
of pure alias shear deformations. The results show that within the
concentration up to 8.3% of the alloying elements, Tis increases with
composition in binary systems with Mn, Fe, Co in ascending order, and decreases
with composition with Nb, Si, Mo, Ti, Al, Cr in descending order. Combined with
Ni34XZ in the present work and Ni11X in the literature from DFT-based
calculations, the composition dependence of Tis in binary and ternary systems
is modeled using the CALculation of PHAse Diagrams (CALPHAD) approach
considering lattice instability, indicating that atomic bonding strength
significantly influences Tis. Furthermore, correlational analyses show that
Burgers vector and elastic constant C11 affect Tis the most out of the
elemental features.",2024-04-29,cond-mat.mtrl-sci,
http://arxiv.org/abs/2404.19096v1,"Data-driven controllers design is an important research problem, in
particular when data is corrupted by the noise. In this paper, we propose a
data-driven min-max model predictive control (MPC) scheme using noisy
input-state data for unknown linear time-invariant (LTI) system. The unknown
system matrices are characterized by a set-membership representation using the
noisy input-state data. Leveraging this representation, we derive an upper
bound on the worst-case cost and determine the corresponding optimal
state-feedback control law through a semidefinite program (SDP). We prove that
the resulting closed-loop system is robustly stabilized and satisfies the input
and state constraints. Further, we propose an adaptive data-driven min-max MPC
scheme which exploits additional online input-state data to improve closed-loop
performance. Numerical examples show the effectiveness of the proposed methods.",2024-04-29,eess.SY,
http://arxiv.org/abs/2404.19091v1,"In this paper, we prove Schwartz estimates for Hodge Laplacian and Dirac
operators on semisimple Lie groups. Alongside, we gives a version of Kuga lemma
for its Lie algebra cohomology. This is a generalization of similar results on
symmetric spaces. The main purpose of such estimates is to study the heat
problem not only in the scalar case, but also for sections of vector bundles on
homogeneous spaces using Fourier analysis.",2024-04-29,math.DG,
http://arxiv.org/abs/2404.19085v1,"This work presents a theoretical analysis of the motion of a tracer colloid
driven by a time-dependent force through a viscoelastic fluid. The recoil of
the colloid after application of a strong force is determined. It provides
insights into the elastic forces stored locally in the fluid and their
weakening by plastic processes. We generalize the mode coupling theory of
microrheology to include time-dependent forces. After deriving the equations of
motion for the tracer correlator and simplifying to a schematic model we apply
the theory to a switch-off force protocol that features the recoiling of the
tracer after cessation of the driving. We also include Langevin dynamics
simulations to compare to the results of the theory. A non-monotonic trend of
the recoil amplitude is found in the theory and confirmed in the simulations.
The linear-response approximation is also verified in the small-force regime.
While the overall agreement between simulation and theory is good, simulation
shows that the theory predicts a too strong non-monotonous dependence of the
recoil distance on the applied force.",2024-04-29,cond-mat.soft,
http://arxiv.org/abs/2404.19083v1,"Breast cancer is one of the leading causes of mortality among women
worldwide. Early detection and risk assessment play a crucial role in improving
survival rates. Therefore, annual or biennial mammograms are often recommended
for screening in high-risk groups. Mammograms are typically interpreted by
expert radiologists based on the Breast Imaging Reporting and Data System
(BI-RADS), which provides a uniform way to describe findings and categorizes
them to indicate the level of concern for breast cancer. Recently, machine
learning (ML) and computational approaches have been developed to automate and
improve the interpretation of mammograms. However, both BI-RADS and the
ML-based methods focus on the analysis of data from the present and sometimes
the most recent prior visit. While it is clear that temporal changes in image
features of the longitudinal scans should carry value for quantifying breast
cancer risk, no prior work has conducted a systematic study of this. In this
paper, we extend a state-of-the-art ML model to ingest an arbitrary number of
longitudinal mammograms and predict future breast cancer risk. On a large-scale
dataset, we demonstrate that our model, LoMaR, achieves state-of-the-art
performance when presented with only the present mammogram. Furthermore, we use
LoMaR to characterize the predictive value of prior visits. Our results show
that longer histories (e.g., up to four prior annual mammograms) can
significantly boost the accuracy of predicting future breast cancer risk,
particularly beyond the short-term. Our code and model weights are available at
https://github.com/batuhankmkaraman/LoMaR.",2024-04-29,eess.IV,
http://arxiv.org/abs/2404.19082v1,"Quantum computing's potential is immense, promising super-polynomial
reductions in execution time, energy use, and memory requirements compared to
classical computers. This technology has the power to revolutionize scientific
applications such as simulating many-body quantum systems for molecular
structure understanding, factorization of large integers, enhance machine
learning, and in the process, disrupt industries like telecommunications,
material science, pharmaceuticals and artificial intelligence. However, quantum
computing's potential is curtailed by noise, further complicated by
non-stationary noise parameter distributions across time and qubits. This
dissertation focuses on the persistent issue of noise in quantum computing,
particularly non-stationarity of noise parameters in transmon processors.",2024-04-29,quant-ph,
http://arxiv.org/abs/2404.19075v1,"4D time-space reconstruction of dynamic events or deforming objects using
X-ray computed tomography (CT) is an extremely ill-posed inverse problem.
Existing approaches assume that the object remains static for the duration of
several tens or hundreds of X-ray projection measurement images (reconstruction
of consecutive limited-angle CT scans). However, this is an unrealistic
assumption for many in-situ experiments that causes spurious artifacts and
inaccurate morphological reconstructions of the object. To solve this problem,
we propose to perform a 4D time-space reconstruction using a distributed
implicit neural representation (DINR) network that is trained using a novel
distributed stochastic training algorithm. Our DINR network learns to
reconstruct the object at its output by iterative optimization of its network
parameters such that the measured projection images best match the output of
the CT forward measurement model. We use a continuous time and space forward
measurement model that is a function of the DINR outputs at a sparsely sampled
set of continuous valued object coordinates. Unlike existing state-of-the-art
neural representation architectures that forward and back propagate through
dense voxel grids that sample the object's entire time-space coordinates, we
only propagate through the DINR at a small subset of object coordinates in each
iteration resulting in an order-of-magnitude reduction in memory and compute
for training. DINR leverages distributed computation across several compute
nodes and GPUs to produce high-fidelity 4D time-space reconstructions even for
extremely large CT data sizes. We use both simulated parallel-beam and
experimental cone-beam X-ray CT datasets to demonstrate the superior
performance of our approach.",2024-04-29,eess.IV,
http://arxiv.org/abs/2404.19074v1,"Recent theoretical investigations have revealed unconventional transport
mechanisms within high Brilliouin zones of two-dimensional superlattices.
Electrons can navigate along channels we call superwires, gently guided without
brute force confinement. Such dynamical confinement is caused by weak
superlattice deflections, markedly different from the static or energetic
confinement observed in traditional wave guides or one-dimensional electron
wires. The quantum properties of superwires give rise to elastic dynamical
tunneling, linking disjoint regions of the corresponding classical phase space,
and enabling the emergence of several parallel channels. This paper provides
the underlying theory and mechanisms that facilitate dynamical tunneling
assisted by chaos in periodic lattices. Moreover, we show that the mechanism of
dynamical tunneling can be effectively conceptualized through the lens of a
paraxial approximation. Our results further reveal that superwires
predominantly exist within flat bands, emerging from eigenstates that represent
linear combinations of conventional degenerate Bloch states. Finally, we
quantify tunneling rates across various lattice configurations, and demonstrate
the tunneling can be suppressed in a controlled fashion, illustrating potential
implications in future nanodevices.",2024-04-29,cond-mat.mes-hall,
http://arxiv.org/abs/2404.19073v1,"We consider the problem of inferring the conditional independence graph (CIG)
of a sparse, high-dimensional, stationary matrix-variate Gaussian time series.
All past work on high-dimensional matrix graphical models assumes that
independent and identically distributed (i.i.d.) observations of the
matrix-variate are available. Here we allow dependent observations. We consider
a sparse-group lasso-based frequency-domain formulation of the problem with a
Kronecker-decomposable power spectral density (PSD), and solve it via an
alternating direction method of multipliers (ADMM) approach. The problem is
bi-convex which is solved via flip-flop optimization. We provide sufficient
conditions for local convergence in the Frobenius norm of the inverse PSD
estimators to the true value. This result also yields a rate of convergence. We
illustrate our approach using numerical examples utilizing both synthetic and
real data.",2024-04-29,stat.ML,
http://arxiv.org/abs/2404.19072v1,"Cross-flow turbine blades encounter a relatively undisturbed inflow for the
first half of each rotational cycle (""upstream sweep"") and then pass through
their own wake for the latter half (""downstream sweep""). While most research on
cross-flow turbine optimization focuses on the power-generating upstream sweep,
we use singled-bladed turbine experiments to show that the downstream sweep
strongly affects time-averaged performance. We find that power generation from
the upstream sweep continues to increase beyond the optimal tip-speed ratio. In
contrast, the power consumption from the downstream sweep begins to increase
approximately linearly beyond the optimal tip-speed ratio due in part to an
increasingly unfavorable orientation of lift and drag relative to the rotation
direction as well as high tangential blade velocities. Downstream power
degradation increases faster than upstream power generation, indicating the
downstream sweep strongly influences the optimal tip-speed ratio. In addition,
particle image velocimetry data is obtained inside the turbine swept area at
three tip-speed ratios. This illuminates the mechanisms underpinning the
observed performance degradation in the downstream sweep and motivates an
analytical model for a limited case with high induction. Performance results
are shown to be consistent across 55 unique combinations of chord-to-radius
ratio, preset pitch angle, and Reynolds number, underscoring the general
relevance of the downstream sweep.",2024-04-29,physics.flu-dyn,
http://arxiv.org/abs/2404.19067v1,"In this paper, we explore using the Harrow-Hassidim-Lloyd (HHL) algorithm to
address scientific and engineering problems through quantum computing utilizing
the NWQSim simulation package on high-performance computing. Focusing on
domains such as power-grid management and heat transfer problems, we
demonstrate the correlations of the precision of quantum phase estimation,
along with various properties of coefficient matrices, on the final solution
and quantum resource cost in iterative and non-iterative numerical methods such
as Newton-Raphson method and finite difference method, as well as their impacts
on quantum error correction costs using Microsoft Azure Quantum resource
estimator. We conclude the exponential resource cost from quantum phase
estimation before and after quantum error correction and illustrate a potential
way to reduce the demands on physical qubits. This work lays down a preliminary
step for future investigations, urging a closer examination of quantum
algorithms' scalability and efficiency in domain applications.",2024-04-29,quant-ph,
http://arxiv.org/abs/2405.00737v1,"This note is intended to explain the proof of two facts about quadrature
domains: first, they are essentially unique if they exist; and second, they do
exist for a large class of weight functions. The proofs roughly follow Sakai's
""Solutions to the obstacle problem as Green potentials,"" but are presented at
an easier level.",2024-04-29,math.AP,
http://arxiv.org/abs/2404.19062v1,"There is currently a strong interest in the collective behavior of chiral
active particles that can propel and rotate themselves. In the presence of
alignment interactions for many chiral particles, chiral self-propulsion can
induce vortex patterns in the velocity fields. However, these emerging patterns
are non-permanent, and do not induce global vorticity. Here we combine
theoretical arguments and computer simulations to predict a so-far unknown
class of collective behavior. We show that, for chiral active particles,
vortices with significant dynamical coherence emerge spontaneously. They
originate from the interplay between attraction interactions and chirality in
the absence of alignment interactions. Depending on parameters, the vortices
can either feature a constant vorticity or a vorticity that oscillates
periodically in time, resulting in self-reverting vortices. Our results may
guide future experiments to realize customized collective phenomena such as
spontaneously rotating gears and patterns with a self-reverting order.",2024-04-29,cond-mat.soft,
http://arxiv.org/abs/2404.19061v1,"We use a continuum, two-fluid approach to study a mixture of two active
nematic fluids. Even in the absence of thermodynamically-driven ordering, for
mixtures of different activities we observe turbulent microphase separation,
where domains form and disintegrate chaotically in an active turbulent
background. This is a weak effect if there is no elastic nematic alignment
between the two fluid components, but is greatly enhanced in the presence of an
elastic alignment or substrate friction. We interpret the results in terms of
relative flows between the two species which result from active anchoring at
concentration gradients. Our results may have relevance in interpreting
epithelial cell sorting and the dynamics of multi-species bacterial colonies.",2024-04-29,cond-mat.soft,
http://arxiv.org/abs/2404.19059v1,"We randomize the implicit two-stage Runge-Kutta scheme in order to improve
the rate of convergence (with respect to a deterministic scheme) and stability
of the approximate solution (with respect to the solution generated by the
explicit scheme). For stability analysis, we use Dahlquist's concept of
A-stability, adopted to randomized schemes by considering three notions of
stability: asymptotic, mean-square, and in probability. The randomized implicit
RK2 scheme proves to be A-stable asymptotically and in probability but not in
the mean-square sense.",2024-04-29,math.NA,
http://arxiv.org/abs/2404.19053v1,"The specification of a covariance function is of paramount importance when
employing Gaussian process models, but the requirement of positive definiteness
severely limits those used in practice. Designing flexible stationary
covariance functions is, however, straightforward in the spectral domain, where
one needs only to supply a positive and symmetric spectral density. In this
work, we introduce an adaptive integration framework for efficiently and
accurately evaluating covariance functions and their derivatives at irregular
locations directly from \textit{any} continuous, integrable spectral density.
In order to make this approach computationally tractable, we employ high-order
panel quadrature, the nonuniform fast Fourier transform, and a Nyquist-informed
panel selection heuristic, and derive novel algebraic truncation error bounds
which are used to monitor convergence. As a result, we demonstrate several
orders of magnitude speedup compared to naive uniform quadrature approaches,
allowing us to evaluate covariance functions from slowly decaying, singular
spectral densities at millions of locations to a user-specified tolerance in
seconds on a laptop. We then apply our methodology to perform gradient-based
maximum likelihood estimation using a previously numerically infeasible
long-memory spectral model for wind velocities below the atmospheric boundary
layer.",2024-04-29,stat.CO,
http://arxiv.org/abs/2404.19047v1,"Quantum technologies and experiments often require preparing systems in
low-temperature states. Here, we investigate cooling schemes using feedback
protocols modeled with a Quantum Fokker-Planck Master Equation (QFPME) recently
derived by Annby-Andersson et. al. (Phys. Rev. Lett. 129, 050401, 2022). This
equation describes systems under continuous weak measurements, with feedback
based on the outcome of these measurements. We apply this formalism to study
the cooling and trapping of a harmonic oscillator for several protocols based
on position and/or momentum measurements. We find that the protocols can cool
the oscillator down to, or close to, the ground state for suitable choices of
parameters. Our analysis provides an analytically solvable case study of
quantum measurement and feedback and illustrates the application of the QFPME
to continuous quantum systems.",2024-04-29,quant-ph,
http://arxiv.org/abs/2404.19046v1,"We extend the earlier linear studies of cosmological peculiar velocities to
Friedmann universes with nonzero spatial curvature. In the process, we also
compare our results with those obtained in cosmologies with Euclidean spatial
sections. Employing relativistic cosmological perturbation theory, we first
provide the differential formulae governing the evolution of peculiar
velocities on all Friedmann backgrounds. The technical complexities of the
curved models, however, mean that analytic solutions are possible only in
special, though characteristic, moments in the lifetime of these universes.
Nevertheless, our solutions exhibit persistent patterns that make us confident
enough to generalise them. Thus, we confirm earlier claims that, compared to
the Newtonian studies, the relativistic analysis supports considerably stronger
linear growth-rates for peculiar-velocity perturbations. This result holds
irrespective of the background curvature. Moreover, for positive curvature, the
peculiar growth-rate is found to be faster than that obtained in a spatially
flat Friedman universe. In contrast, linear peculiar velocities appear to grow
at a slower pace when their Friedmann host is spatially open. Extrapolating
them to the present, our results seem to suggest faster bulk peculiar motions
in overdense, rather than in underdense, regions of the universe.",2024-04-29,gr-qc,
http://arxiv.org/abs/2404.19039v1,"We study two quantifications of being a homology sphere for hyperbolic
3-manifolds, one geometric and one topological: the spectral gap for the
Laplacian on coclosed 1-forms and the size of the first torsion homology group.
We first construct a sequence of closed hyperbolic integer homology spheres
with volume tending to infinity and a uniform coclosed 1-form spectral gap.
This answers a question asked by Lin--Lipnowski. We also find sequences of
hyperbolic rational homology spheres with the same properties that
geometrically converge to a tame limit manifold. Moreover, we show that any
such sequence must have unbounded torsion homology growth. Finally we show that
a sequence of closed hyperbolic rational homology 3-spheres with uniformly
bounded rank and a uniform coclosed 1-form spectral gap must have torsion
homology that grows exponentially in volume.",2024-04-29,math.GT,
http://arxiv.org/abs/2404.19037v1,"Strong first-order phase transitions (SFOPT) during the evolution of the
Higgs potential in the early universe not only allow for the dynamical
generation of the observed matter-antimatter asymmetry, they can also source a
stochastic gravitational wave (GW) background possibly detectable with future
space-based gravitational waves interferometers. As SFOPTs are
phenomenologically incompatible with the Standard Model (SM) Higgs sector, the
observation of GWs from SFOPTs provides an exciting interplay between cosmology
and particle physics in the search for new physics. With the C++ code BSMPTv3,
we present for the first time a tool that performs the whole chain from the
particle physics model to the gravitational wave spectrum. Extending the
previous versions BSMPTv1 and v2, it traces the phases of beyond-SM (BSM) Higgs
potentials and is capable of treating multiple vacuum directions and multi-step
phase transitions. During the tracing, it checks for discrete symmetries, flat
directions, and electroweak symmetry restoration, and finally reports the
transition history. The transition probability from the false to the true
vacuum is obtained from the solution of the bounce equation which allows for
the calculation of the nucleation, percolation and completion temperatures. The
peak amplitude and frequency of the GWs originating from sound waves and
turbulence, are evaluated after the calculation of the thermal parameters at
the transition temperature, and finally the signal-to-noise ratio at LISA is
provided. The code BSMPTv3 is a powerful self-contained tool that comes more
than timely and will be of great benefit for investigations of the vacuum
structure of the early universe of not only simple but also complicated Higgs
potentials involving several vacuum directions, with exciting applications in
the search for new physics.",2024-04-29,hep-ph,
http://arxiv.org/abs/2404.19036v2,"Quantum control techniques play an important role in manipulating and
harnessing the properties of different quantum systems, including isolated
atoms. Here, we propose to achieve quantum control over a single on-surface
atomic spin using Landau-Zener-St\""uckelberg-Majorana (LZSM) interferometry
implemented with Scanning Tunneling Microscopy (STM). Specifically, we model
how the application of time-dependent, non-resonant AC electric fields across
the STM tip-surface gap makes it possible to achieve precise quantum state
manipulation in an isolated Fe atom on a MgO/Ag(100) surface. We propose a
protocol to combine Landau Zener tunneling with LZSM interferometry that
permits one to measure the quantum spin tunneling of an individual Fe atom. The
proposed experiments can be implemented with ESR-STM instrumentation, opening a
new venue in the research of on-surface single spin control.",2024-04-29,quant-ph,
http://arxiv.org/abs/2404.19032v1,"We introduce fermionic machine learning (FermiML), a machine learning
framework based on fermionic quantum computation. FermiML models are expressed
in terms of parameterized matchgate circuits, a restricted class of quantum
circuits that map exactly to systems of free Majorana fermions. The FermiML
framework allows for building fermionic counterparts of any quantum machine
learning (QML) model based on parameterized quantum circuits, including models
that produce highly entangled quantum states. Importantly, matchgate circuits
are efficiently simulable classically, thus rendering FermiML a flexible
framework for utility benchmarks of QML methods on large real-world datasets.
We initiate the exploration of FermiML by benchmarking it against unrestricted
PQCs in the context of classification with random quantum kernels. Through
experiments on standard datasets (Digits and Wisconsin Breast Cancer), we
demonstrate that FermiML kernels are on-par with unrestricted PQC kernels in
classification tasks using support-vector machines. Furthermore, we find that
FermiML kernels outperform their unrestricted candidates on multi-class
classification, including on datasets with several tens of relevant features.
We thus show how FermiML enables us to explore regimes previously inaccessible
to QML methods.",2024-04-29,quant-ph,
http://arxiv.org/abs/2404.19030v1,"We present a novel computational method for direct numerical simulations of
particle-laden flows with fully-resolved particles (PR-DNS). The method is
based on the recently developed Volume-Filtering Immersed Boundary method [Dave
et al, Journal of Computational Physics, 487:112136, 2023] derived by
volume-filtering the transport equations. This approach is mathematically and
physically rigorous, in contrast to other PR-DNS methods which rely on ad-hoc
numerical schemes to impose no-slip boundary conditions on the surface of
particles. With the present PR-DNS strategy, we show that the ratio of filter
size to particle diameter acts as a parameter that controls the level of
fidelity. In the limit where this ratio is very small, a well-resolved PR-DNS
is obtained. Conversely, when the ratio of filter size to particle diameter is
large, a classic point-particle method is obtained. The discretization of the
filtered equations is discussed and compared to other PR-DNS strategies based
on direct-forcing immersed boundary methods. Numerical examples with
sedimenting resolved particles are discussed.",2024-04-29,physics.flu-dyn,
http://arxiv.org/abs/2404.19029v2,"Inspired by recent developments in the study of the model of double scaled
SYK (DSSYK), as elucidated in a recent paper, we embark on a re-evaluation of
the Sachdev-Ye-Kitaev (SYK) model. Our motivation stems from the insights
gained from the DSSYK model, particularly its ability to capture essential
features of quantum dynamics and gravitational effects. In this work, we delve
into the SYK model, uncovering precise solutions for the two-point function and
self-energy that have not been previously reported. Building upon the
advancements made in particle physics phenomenology, we extend the SYK model to
encompass tensor field theory. Through the incorporation of a cutoff term to
ensure convergence, we substantially advance our understanding of quantum
many-body physics. Our investigation extends to experimental parameter
estimation and the exploration of cutoff dependency in random couplings,
providing invaluable insights into system dynamics. The introduction of a novel
tensor field theory replaces conventional fermionic degrees of freedom with
tensorial counterparts, leading to the discovery of intriguing phase transition
phenomena characterized by a first-order transition. Furthermore, we elucidate
a direct linear relationship between the coupling parameter and the cutoff
scale. These findings not only shed light on emergent behavior across both
high-energy physics and condensed matter systems but also pave the way for
further theoretical and experimental exploration, inspired by the recent
advancements in the SYK model.",2024-04-29,hep-th,
http://arxiv.org/abs/2404.19028v3,"This paper aims to introduce a new statistical learning technique based on
sparsity promoting for data-driven modeling and control of solar photovoltaic
(PV) systems. Compared with conventional sparse regression techniques that
might introduce computational complexities when the number of candidate
functions increases, an innovative algorithm, named adaptive regulated sparse
regression (ARSR) is proposed that adaptively regulates the hyperparameter
weights of candidate functions to best represent the dynamics of PV systems.
Utilizing this algorithm, open-loop and closed-loop models of single-stage and
two-stage PV systems are obtained from measurements and are utilized for
control design purposes. Moreover, it is demonstrated that the proposed
data-driven approach can successfully be employed for fault analysis studies,
which distinguishes its capabilities compared with other data-driven
techniques. Finally, the proposed approach is validated through real-time
simulations.",2024-04-29,eess.SY,
http://arxiv.org/abs/2404.19027v3,"Variational Quantum Eigensolvers (VQE) are a promising approach for finding
the classically intractable ground state of a Hamiltonian. The Unitary Block
Optimization Scheme (UBOS) is a state-of-the-art VQE method which works by
sweeping over gates and finding optimal parameters for each gate in the
environment of other gates. UBOS improves the convergence time to the ground
state by an order of magnitude over Stochastic Gradient Descent (SGD). It
nonetheless suffers in both rate of convergence and final converged energies in
the face of highly noisy expectation values coming from shot noise. Here we
develop two classical post-processing techniques which improve UBOS especially
when measurements have large noise. Using Gaussian Process Regression (GPR), we
generate artificial augmented data using original data from the quantum
computer to reduce the overall error when solving for the improved parameters.
Using Double Robust Optimization plus Rejection (DROPR), we prevent outlying
data which are atypically noisy from resulting in a particularly erroneous
single optimization step thereby increasing robustness against noisy
measurements. Combining these techniques further reduces the final relative
error that UBOS reaches by a factor of three without adding additional quantum
measurement or sampling overhead. This work further demonstrates that
developing techniques which use classical resources to post-process quantum
measurement results can significantly improve VQE algorithms.",2024-04-29,quant-ph,
http://arxiv.org/abs/2405.19566v1,"In the 1960s, it was proposed that in small indirect band-gap materials,
excitons can spontaneously form because the density of carriers is too low to
screen the attractive Coulomb interaction between electrons and holes. The
result is a novel strongly interacting insulating phase known as an excitonic
insulator. Here we employ scanning tunnelling microscopy (STM) and spectroscopy
(STS) to show that the enhanced Coulomb interaction in quantum-confined
elemental Sb nanoflakes drives the system to the excitonic insulator state. The
unique feature of the excitonic insulator, a charge density wave (CDW) without
periodic lattice distortion, is directly observed. Furthermore, STS shows a gap
induced by the CDW near the Fermi surface. Our observations suggest that the
Sb(110) nanoflake is an excitonic insulator.",2024-05-29,cond-mat.mtrl-sci,
http://arxiv.org/abs/2405.19565v1,"Evolutionary game dynamics on networks typically consider the competition
among simple strategies such as cooperation and defection in the Prisoner's
Dilemma and summarize the effect of population structure as network
reciprocity. However, it remains largely unknown regarding the evolutionary
dynamics involving multiple powerful strategies typically considered in
repeated games, such as the zero-determinant (ZD) strategies that are able to
enforce a linear payoff relationship between them and their co-players. Here,
we consider the evolutionary dynamics of always cooperate (AllC), extortionate
ZD (extortioners), and unbending players in lattice populations based on the
commonly used death-birth updating. Out of the class of unbending strategies,
we consider a particular candidate, PSO Gambler, a machine-learning-optimized
memory-one strategy, which can foster reciprocal cooperation and fairness among
extortionate players. We derive analytical results under weak selection and
rare mutations, including pairwise fixation probabilities and long-term
frequencies of strategies. In the absence of the third unbending type,
extortioners can achieve a half-half split in equilibrium with unconditional
cooperators for sufficiently large extortion factors. However, the presence of
unbending players fundamentally changes the dynamics and tilts the system to
favor unbending cooperation. Most surprisingly, extortioners cannot dominate at
all regardless of how large their extortion factor is, and the long-term
frequency of unbending players is maintained almost as a constant. Our
analytical method is applicable to studying the evolutionary dynamics of
multiple strategies in structured populations. Our work provides insights into
the interplay between network reciprocity and direct reciprocity, revealing the
role of unbending strategies in enforcing fairness and suppressing extortion.",2024-05-29,physics.soc-ph,
http://arxiv.org/abs/2405.19557v1,"The power and the probability of electromagnetic radiation from an electron
in a constant background tensor field violating Lorentz invariance are
calculated. The case of a background field of the quasielectric type is
considered. The angular distribution and the polarization of the radiation are
studied. Using present experimental constraints on the background field
strength, it is shown that the radiation effect can manifest itself under
astrophysical conditions at ultrahigh electron energy.",2024-05-29,hep-ph,
http://arxiv.org/abs/2405.19556v1,"Tests of general relativity with gravitational waves typically introduce
parameters for putative deviations and combine information from multiple events
by characterizing the population distribution of these parameters through a
hierarchical model. Although many tests include multiple such parameters,
hierarchical tests have so far been unable to accommodate this
multidimensionality, instead restricting to separate one-dimensional analyses
and discarding information about parameter correlations. In this paper, we
extend population tests of general relativity to handle an arbitrary number of
dimensions. We demonstrate this framework on the two-dimensional
inspiral-merger-ringdown consistency test, and derive new constraints from the
latest LIGO-Virgo-KAGRA catalog, GWTC-3. We obtain joint constraints for the
two parameters introduced by the classic formulation of this test, revealing
their correlation structure both at the individual-event and population levels.
We additionally propose a new four-dimensional formulation of the
inspiral-merger-ringdown test that we show contains further information. As in
past work, we find the GW190814 event to be an outlier; the 4D analysis yields
further insights on how the low mass and spin of this event biases the
population results. Without (with) this event, we find consistency with general
relativity at the 60\% (92\%) credible level in the 2D formulation, and 76\%
(80\%) for the 4D formulation. This multi-dimensional framework can be
immediately applied to other tests of general relativity in any number of
dimensions, including the parametrized post-Einsteinian tests and ringdown
tests.",2024-05-29,gr-qc,
http://arxiv.org/abs/2405.19555v1,"We show that lattice isomorphisms between the lattices of slowly oscillating
functions on chain-connected proper metric spaces induce coarsely equivalent
homeomorphisms. This result leads to a Banach-Stone-type theorem for these
lattices. Furthermore, we provide a representation theorem that characterizes
linear lattice isomorphisms among these lattices.",2024-05-29,math.GN,
http://arxiv.org/abs/2405.19554v1,"The recent 1/2-equation model of turbulence is a simplification of the
standard Kolmogorov-Prandtl 1-equation URANS model. Surprisingly, initial
numerical tests indicated that the 1/2-equation model produces comparable
velocity statistics at reduced cost. It is also a test problem and first step
for developing numerical analysis to address a full 1-equation model. This
report begins the numerical analysis of the 1/2 equation model. Stability,
convergence and error estimates are proven for a semi-discrete and fully
discrete approximation. Finally, numerical tests are conducted to validate our
convergence theory.",2024-05-29,math.NA,
http://arxiv.org/abs/2405.19546v1,"We propose a convex optimization approach to determine perturbations in the
initial conditions of a weather phenomenon as control inputs for quantitative
weather control. We first construct a sensitivity matrix of outputs, such as
accumulated precipitation, to the initial conditions, such as temperature and
humidity, through sensitivity analysis of a numerical weather prediction model.
We then solve a convex optimization problem to find optimal perturbations in
the initial conditions to realize the desired spatial distribution of the
targeting outputs. We implement the proposed method in a benchmark of a warm
bubble experiment and show that it realizes desired spatial distributions of
accumulated precipitation, such as a reference distribution and the reduced
maximum value.",2024-05-29,physics.ao-ph,
http://arxiv.org/abs/2405.19545v1,"The relaxation in a vacuum capacitor-resistor circuit is comprised of two
exponential decays, one caused by surface charge and the other by the decay of
energy stored between the capacitor plates. A simple phenomenological model of
this relaxation is shown to be supported by measurements even though Maxwell's
equations are difficult to apply in this case. Similar behavior is also
observed for polypropylene capacitors, indicating that this surface charge
effect is applicable to all capacitors and potentially other circuit
components.",2024-05-29,physics.app-ph,
http://arxiv.org/abs/2405.19543v1,"We show that any minimal Cayley graph of a (finitely generated) generalized
dihedral or nilpotent group has chromatic number at most 3, while 4 colors are
sometimes necessary for soluble groups. On the other hand we construct graphs
of unbounded chromatic number that admit a proper edge coloring such that each
cycle has some color at least twice. The latter disproves a conjecture of
Babai'78 that would have implied that all minimal Cayley graphs have bounded
chromatic number -- a problem that remains open.",2024-05-29,math.CO,
http://arxiv.org/abs/2406.04363v1,"The article `Bell Nonlocality in Classical Systems Coexisting with Other
System Types' by Chiribella et al. defines `classical' in a quantum context in
a way that ignores noncommuting quantum projectors, and is hence inconsistent
with Hilbert-space quantum theory.",2024-05-29,quant-ph,
http://arxiv.org/abs/2405.19537v1,"Parameterized quantum circuits play a key role for the development of quantum
variational algorithms in the realm of the NISQ era. Knowing their actual
capability of performing different kinds of tasks is then of the utmost
importance. By comparing them with a prototypical class of universal random
circuits we have found that their approach to the asymptotic complexity defined
by the Haar measure is faster, needing less gates to reach it. Topology has
been revealed crucial for this. The majorization criterion has proven as a
relevant complementary tool to the expressibility and the mean entanglement.",2024-05-29,quant-ph,
http://arxiv.org/abs/2405.19535v1,"We investigate the thermodynamics as well as the population dynamics of
ecosystems based on a stochastic approach in which the number of individuals of
the several species of the ecosystem are treated as stochastic variables. The
several species are connected by feeding relationships that are understood as
unidirectional processes in which a certain amount of biomass is exchanged
between species. We show that the equations for the averages in the numbers of
individuals are that given by the deterministic approach. We determine the
fluxes of mass, energy, and entropy as well as the rate of the entropy
production. This last quantity, which has a central role in the present
stochastic approach, is obtained by a formula appropriate for unidirectional
transitions. The flux of energy across the ecosystem is shown to be
proportional to the flux of entropy to the environment.",2024-05-29,cond-mat.stat-mech,
http://arxiv.org/abs/2406.00058v1,"The article explores the arithmetic of multiplication as a model of many
valued projective logic. It is demonstrated that closed numerical intervals
within this framework constitute Heyting algebras. The conditions for these
algebras to be Boolean are identified. The article claims have undergone
numerical verification. Paths for generalization to normed linear spaces are
delineated.",2024-05-29,math.LO,
http://arxiv.org/abs/2405.19527v1,"The integration of traditional fixed-route transit (FRT) and more flexible
microtransit has been touted as a means of improving mobility and access to
opportunity, increasing transit ridership, and promoting environmental
sustainability. To help evaluate integrated FRT and microtransit public transit
(PT) system (henceforth ``integrated fixed-flex PT system'') designs, we
propose a high-fidelity modeling framework that provides reliable estimates for
a wide range of (i) performance metrics and (ii) integrated fixed-flex PT
system designs. We formulate the mode choice equilibrium problem as a
fixed-point problem wherein microtransit demand is a function of microtransit
performance, and microtransit performance depends on microtransit demand. We
propose a detailed agent-based simulation modeling framework that includes (i)
a binary logit mode choice model (private auto vs. transit), (ii) a
supernetwork-based model and pathfinding algorithm for multi-modal transit path
choice where the supernetwork includes pedestrian, FRT, and microtransit
layers, (iii) a detailed mobility-on-demand fleet simulator called FleetPy to
model the supply-demand dynamics of the microtransit service. In this paper, we
illustrate the capabilities of the modeling framework by analyzing integrated
fixed-flex PT system designs that vary the following design parameters: FRT
frequencies and microtransit fleet size, service region structure, virtual stop
coverage, and operating hours. We include case studies in downtown San Diego
and Lemon Grove, California. The computational results show that the proposed
modeling framework converges to a mode choice equilibrium. Moreover, the
scenario results imply that introducing a new microtransit service decreases
FRT ridership and requires additional subsidies, but it significantly increases
job accessibility and slightly reduces total VMT.",2024-05-29,eess.SY,
http://arxiv.org/abs/2405.19523v3,"Recently, Cronie et al. (2024) introduced the notion of cross-validation for
point processes and a new statistical methodology called Point Process Learning
(PPL). In PPL one splits a point process/pattern into a training and a
validation set, and then predicts the latter from the former through a
parametrised Papangelou conditional intensity. The model parameters are
estimated by minimizing a point process prediction error; this notion was
introduced as the second building block of PPL. It was shown that PPL
outperforms the state-of-the-art in both kernel intensity estimation and
estimation of the parameters of the Gibbs hard-core process. In the latter
case, the state-of-the-art was represented by pseudolikelihood estimation. In
this paper we study PPL in relation to Takacs-Fiksel estimation, of which
pseudolikelihood is a special case. We show that Takacs-Fiksel estimation is a
special case of PPL in the sense that PPL with a specific loss function
asymptotically reduces to Takacs-Fiksel estimation if we let the
cross-validation regime tend to leave-one-out cross-validation. Moreover, PPL
involves a certain type of hyperparameter given by a weight function which
ensures that the prediction errors have expectation zero if and only if we have
the correct parametrisation. We show that the weight function takes an explicit
but intractable form for general Gibbs models. Consequently, we propose
different approaches to estimate the weight function in practice. In order to
assess how the general PPL setup performs in relation to its special case
Takacs-Fiksel estimation, we conduct a simulation study where we find that for
common Gibbs models we can find loss functions and hyperparameters so that PPL
typically outperforms Takacs-Fiksel estimation significantly in terms of mean
square error. Here, the hyperparameters are the cross-validation parameters and
the weight function estimate.",2024-05-29,stat.ME,
http://arxiv.org/abs/2405.19518v1,"This paper explores the potential of a hybrid modeling approach that combines
machine learning (ML) with conventional physics-based modeling for weather
prediction beyond the medium range. It extends the work of Arcomano et al.
(2022), which tested the approach for short- and medium-range weather
prediction, and the work of Arcomano et al. (2023), which investigated its
potential for climate modeling. The hybrid model used for the forecast
experiments of the paper is based on the low-resolution, simplified
parameterization atmospheric general circulation model (AGCM) SPEEDY. In
addition to the hybridized prognostic variables of SPEEDY, the current version
of the model has three purely ML-based prognostic variables. One of these is
6~h cumulative precipitation, another is the sea surface temperature, while the
third is the heat content of the top 300 m deep layer of the ocean. The model
has skill in predicting the El Ni\~no cycle and its global teleconnections with
precipitation for 3-7 months depending on the season. The model captures
equatorial variability of the precipitation associated with Kelvin and Rossby
waves and MJO. Predictions of the precipitation in the equatorial region have
skill for 15 days in the East Pacific and 11.5 days in the West Pacific. Though
the model has low spatial resolution, for these tasks it has prediction skill
comparable to what has been published for high-resolution, purely
physics-based, conventional operational forecast models.",2024-05-29,physics.ao-ph,
http://arxiv.org/abs/2405.19516v1,"This paper introduces PanoRadar, a novel RF imaging system that brings RF
resolution close to that of LiDAR, while providing resilience against
conditions challenging for optical signals. Our LiDAR-comparable 3D imaging
results enable, for the first time, a variety of visual recognition tasks at
radio frequency, including surface normal estimation, semantic segmentation,
and object detection. PanoRadar utilizes a rotating single-chip mmWave radar,
along with a combination of novel signal processing and machine learning
algorithms, to create high-resolution 3D images of the surroundings. Our system
accurately estimates robot motion, allowing for coherent imaging through a
dense grid of synthetic antennas. It also exploits the high azimuth resolution
to enhance elevation resolution using learning-based methods. Furthermore,
PanoRadar tackles 3D learning via 2D convolutions and addresses challenges due
to the unique characteristics of RF signals. Our results demonstrate
PanoRadar's robust performance across 12 buildings.",2024-05-29,eess.SP,
http://arxiv.org/abs/2405.19515v2,"The upcoming Proton Improvement Plan-II (PIP-II), designated for enhancements
to the Fermilab accelerator complex, features a new 800 MeV superconducting
linac and a Beam Transfer Line (BTL) to transport the beam to the existing
Booster synchrotron. To mitigate the space charge tune shift associated with a
high intensity accumulated beam, the low emittance linac beam is used to paint
the ring phase space both transversely and longitudinally. To prevent losses
caused by particles injected outside the rf separatrix while painting
longitudinal phase space, the momentum spread of the incoming beam should not
exceed 2.1 x 10^-4. Detailed simulations showed that due to space charge, the
rms momentum spread increases to 4 x 10^-4 while it is transported in the BTL
--about twice the allowable limit. In this paper, we outline a mitigation
strategy involving a debuncher cavity. We discuss location, operating
frequency, and gap voltage under both nominal and perturbed beam conditions,
specifically accounting for momentum jitter. The impact of cavity misalignments
is also assessed. The paper concludes by recommending an optimized
configuration.",2024-05-29,physics.acc-ph,
http://arxiv.org/abs/2405.20352v1,"Bias correction is a common pre-processing step applied to climate model data
before it is used for further analysis. This article introduces an efficient
adaptation of a well-established bias-correction method - quantile mapping -
for global horizontal irradiance (GHI) that ensures corrected data is
physically plausible through incorporating measurements of clearsky GHI. The
proposed quantile mapping method is fit on reanalysis data to first bias
correct for regional climate models (RCMs) and is tested on RCMs forced by
general circulation models (GCMs) to understand existing biases directly from
GCMs. Additionally, we adapt a functional analysis of variance methodology that
analyzes sources of remaining biases after implementing the proposed quantile
mapping method and considered biases by climate region. This analysis is
applied to four sets of climate model output from NA-CORDEX and compared
against data from the National Solar Radiation Database produced by the
National Renewable Energy Lab.",2024-05-29,stat.AP,
http://arxiv.org/abs/2405.19511v1,"The two planets of the HAT-P-11 system represent fascinating dynamical
puzzles due to their significant eccentricities and orbital misalignments. In
particular, HAT-P-11 b is on a close-in orbit that tides should have
circularized well within the age of the system. Here we propose a two-step
dynamical process that can reproduce all intriguing aspects of the system. We
first invoke planet-planet scattering to generate significant eccentricities
and mutual inclinations between the planets. We then propose that this
misalignment initiated von-Zeipel-Lidov-Kozai cycles and high-eccentricity
migration that ultimately brought HAT-P-11 b to its present-day orbit. We find
that this scenario is fully consistent only when significant tidally-driven
radius inflation is accounted for during the tidal migration. We present a
suite of N-body simulations exploring each phase of evolution and show that
this scenario is consistent with all observational posteriors and the reported
age of the system.",2024-05-29,astro-ph.EP,
http://arxiv.org/abs/2405.19510v1,"Planet-star and planet-planet obliquity encode a planetary system's dynamical
history, but both obliquities are hard to measure for misaligned systems with
close-in companions. HAT-P-11 is a K4 star with two known planets: a close-in,
misaligned super-Neptune with a approx 5-day orbit, and an outer super-Jupiter
with a approx 10-year orbit. In this work we present a joint orbit fit of
HAT-P-11 system with astrometry and RV data.
  By combining our results with previous constraints on the orientation of the
star and the inner planet, we find that all three angular momenta -- those of
the star, planet b, and planet c -- are significantly misaligned. We confirm
the status of planet c as a super-Jupiter, with 3.06 pm 0.42 Jupiter mass, at a
semimajor axis of 4.192 pm 0.07 AU, and planet b's minimum mass of 0.073 pm
0.0053 Jupiter mass. We present the posterior probability distribution of
obliquity between star A and planet c, and between planet b and planet c.",2024-05-29,astro-ph.EP,
http://arxiv.org/abs/2405.19508v1,"We give an elementary new proof of the hot spots conjecture for L-shaped
domains. This result, in addition to a new eigenvalue inequality, allows us to
locate the hot spots in Swiss cross translation surfaces. We then prove, in
several cases, that first mixed Dirichlet-Neumann eigenfunctions of the
Laplacian on L-shaped domains also have no interior critical points. As a
combination of these results, we prove the hot spots conjecture for five
classes of domains tiled by L-shaped domains, including a class of non-simply
connected domains. An interesting feature of the proofs is that we make
positive use of the lack of regularity of eigenfunctions on non-convex
polygons.",2024-05-29,math.AP,
http://arxiv.org/abs/2405.19503v1,"We report on the trapping of single rubidium atoms in large arrays of optical
tweezers comprising up to 2088 sites in a cryogenic environment at 6 K. Our
approach relies on the use of microscope objectives that are in-vacuum but at
room temperature, in combination with windowless thermal shields into which the
objectives are protruding to ensure a cryogenic environment for the trapped
atoms. To achieve enough optical power for efficient trapping, we combine two
lasers at slightly different wavelengths. We discuss the performance and
limitations of our design. Finally, we demonstrate atom-by-atom rearrangement
of an 828-atom target array using moving optical tweezers controlled by a
field-programmable gate array.",2024-05-29,physics.atom-ph,
http://arxiv.org/abs/2405.19500v1,"In the present paper, an algorithm for the numerical solution of the external
Dirichlet generalized harmonic problem for a sphere by the method of
probabilistic solution (MPS) is given, where generalized indicates that a
boundary function has a finite number of first kind discontinuity curves. The
algorithm consists of the following main stages: (1) the transition from an
infinite domain to a finite domain by an inversion; (2) the consideration of a
new Dirichlet generalized harmonic problem on the basis of Kelvin theorem for
the obtained finite domain; (3) the numerical solution of the new problem for
the finite domain by the MPS, which in turn is based on a computer simulation
of the Weiner process; (4) finding the probabilistic solution of the posed
generalized problem at any fixed points of the infinite domain by the solution
of the new problem. For illustration, numerical examples are considered and
results are presented.",2024-05-29,math.NA,
http://arxiv.org/abs/2405.19497v1,"Audio domain transfer is the process of modifying audio signals to match
characteristics of a different domain, while retaining the original content.
This paper investigates the potential of Gaussian Flow Bridges, an emerging
approach in generative modeling, for this problem. The presented framework
addresses the transport problem across different distributions of audio signals
through the implementation of a series of two deterministic probability flows.
The proposed framework facilitates manipulation of the target distribution
properties through a continuous control variable, which defines a certain
aspect of the target domain. Notably, this approach does not rely on paired
examples for training. To address identified challenges on maintaining the
speech content consistent, we recommend a training strategy that incorporates
chunk-based minibatch Optimal Transport couplings of data samples and noise.
Comparing our unsupervised method with established baselines, we find
competitive performance in tasks of reverberation and distortion manipulation.
Despite encoutering limitations, the intriguing results obtained in this study
underscore potential for further exploration.",2024-05-29,eess.AS,
http://arxiv.org/abs/2405.19496v2,"Increase of primary beam power for neutrino beam-lines leads to a reduced
lifespan for production targets. New concepts for robust targets are emerging
from the field of High Power Targetry (HPT); one idea being investigated by the
HPT R&D Group at Fermilab is an electrospun nanofiber target. As part of their
evaluation, samples with different densities were sent to the HiRadMat facility
at CERN for thermal shock tests. The samples with the higher density,
irradiated under a high intensity beam pulse, exhibit major damage at the
impact site whereas those with the lower density show no apparent damage. The
exact cause of this failure was unclear at the time. In this paper, we present
the results of multiphysics simulations of the thermal shock experienced by the
nanofiber targets that suggest the failure originates from the reduced
permeability of the high density sample to air flow. The air present in the
porous target expands due to heating from the beam, but is unable to flow
freely in the high density sample, resulting in a larger back pressure that
blows apart the nanofiber mat. We close with a discussion on how to further
validate this hypothesis.",2024-05-29,physics.acc-ph,
http://arxiv.org/abs/2405.19495v1,"Code Large Language Models (Code LLMs) have emerged as powerful tools,
revolutionizing the software development landscape by automating the coding
process and reducing time and effort required to build applications. This paper
focuses on training Code LLMs to specialize in the field of quantum computing.
We begin by discussing the unique needs of quantum computing programming, which
differ significantly from classical programming approaches or languages. A Code
LLM specializing in quantum computing requires a foundational understanding of
quantum computing and quantum information theory. However, the scarcity of
available quantum code examples and the rapidly evolving field, which
necessitates continuous dataset updates, present significant challenges.
Moreover, we discuss our work on training Code LLMs to produce high-quality
quantum code using the Qiskit library. This work includes an examination of the
various aspects of the LLMs used for training and the specific training
conditions, as well as the results obtained with our current models. To
evaluate our models, we have developed a custom benchmark, similar to
HumanEval, which includes a set of tests specifically designed for the field of
quantum computing programming using Qiskit. Our findings indicate that our
model outperforms existing state-of-the-art models in quantum computing tasks.
We also provide examples of code suggestions, comparing our model to other
relevant code LLMs. Finally, we introduce a discussion on the potential
benefits of Code LLMs for quantum computing computational scientists,
researchers, and practitioners. We also explore various features and future
work that could be relevant in this context.",2024-05-29,quant-ph,
http://arxiv.org/abs/2405.19494v1,"We propose a scheme to generate robust optomechanical entanglement. This
scheme is based on a Backward Stimulated Brillouin Scattering (BSBS) process,
which is hosted within an optomechanical structure. Our benchmark system
consists of an acoustic (mechanical) mode coupled to two optical modes through
the BSBS (radiation pressure) process. For a moderate values of the effective
mechanical coupling, the BSBS induces a relatively weak entanglement. This
entanglement is greatly enhanced, for at least up to one order of magnitude,
when the mechanical coupling strength is strong enough. The generated
entanglement is robust enough against thermal fluctuation. Our work provides a
new scheme for entanglement generation based on BSBS effect, and can be
extended to microwaves and hybrid optomechanical structures. Such a generated
entangled states can be used for quantum information processing, quantum
sensing, and quantum computing.",2024-05-29,quant-ph,
http://arxiv.org/abs/2405.19492v1,"Purpose: To develop an open-source and easy-to-use segmentation model that
can automatically and robustly segment most major anatomical structures in MR
images independently of the MR sequence.
  Materials and Methods: In this study we extended the capabilities of
TotalSegmentator to MR images. 298 MR scans and 227 CT scans were used to
segment 59 anatomical structures (20 organs, 18 bones, 11 muscles, 7 vessels, 3
tissue types) relevant for use cases such as organ volumetry, disease
characterization, and surgical planning. The MR and CT images were randomly
sampled from routine clinical studies and thus represent a real-world dataset
(different ages, pathologies, scanners, body parts, sequences, contrasts, echo
times, repetition times, field strengths, slice thicknesses and sites). We
trained an nnU-Net segmentation algorithm on this dataset and calculated Dice
similarity coefficients (Dice) to evaluate the model's performance.
  Results: The model showed a Dice score of 0.824 (CI: 0.801, 0.842) on the
test set, which included a wide range of clinical data with major pathologies.
The model significantly outperformed two other publicly available segmentation
models (Dice score, 0.824 versus 0.762; p<0.001 and 0.762 versus 0.542;
p<0.001). On the CT image test set of the original TotalSegmentator paper it
almost matches the performance of the original TotalSegmentator (Dice score,
0.960 versus 0.970; p<0.001).
  Conclusion: Our proposed model extends the capabilities of TotalSegmentator
to MR images. The annotated dataset
(https://zenodo.org/doi/10.5281/zenodo.11367004) and open-source toolkit
(https://www.github.com/wasserth/TotalSegmentator) are publicly available.",2024-05-29,eess.IV,
http://arxiv.org/abs/2405.19490v2,"High Power Targetry (HPT) R&D is critical in the context of increasing beam
intensity and energy for next generation accelerators. Many target concepts and
novel materials are being developed and tested for their ability to withstand
extreme beam environments; the HPT R&D Group at Fermilab is developing an
electrospun nanofiber material for this purpose. The performance of these
nanofiber targets is sensitive to their construction parameters, such as the
packing density of the fibers. Lowering the density improves the survival of
the target, but reduces the secondary particle yield. Optimizing the lifetime
and production efficiency of the target poses an interesting design problem,
and in this paper we study the applicability of Bayesian optimization to its
solution. We first describe how to encode the nanofiber target design problem
as the optimization of an objective function, and how to evaluate that function
with computer simulations. We then explain the optimization loop setup.
Thereafter, we present the optimal design parameters suggested by the
algorithm, and close with discussions of limitations and future refinements.",2024-05-29,physics.acc-ph,
http://arxiv.org/abs/2405.19489v1,"A method for increasing efficiency of radio frequency (RF) amplifier
employing laterally diffused metal oxide semiconductor (LDMOS) transistors
coupled to an RF exciter depending on the emission mode of modulated RF input
signals generated by exciter, if exciter output signal is of a type where
modulated RF signals do not have continuously varying envelope, biasing the
LDMOS transistor in the RF amplifier with fixed quiescent drain current and
fixed drain voltage supply to cause LDMOS transistors to operate in compression
and if exciter output signal is of a type where modulated RF signals do have
continuously varying envelope, biasing the LDMOS transistors in the RF
amplifier for linear operation.",2024-05-29,eess.SP,
http://arxiv.org/abs/2405.19486v1,"Despite their benefits in terms of simplicity, low computational cost and
data requirement, parametric machine learning algorithms, such as linear
discriminant analysis, quadratic discriminant analysis or logistic regression,
suffer from serious drawbacks including linearity, poor fit of features to the
usually imposed normal distribution and high dimensionality. Batch kernel-based
nonparametric classifier, which overcomes the linearity and normality of
features constraints, represent an interesting alternative for supervised
classification problem. However, it suffers from the ``curse of dimension"". The
problem can be alleviated by the explosive sample size in the era of big data,
while large-scale data size presents some challenges in the storage of data and
the calculation of the classifier. These challenges make the classical batch
nonparametric classifier no longer applicable. This motivates us to develop a
fast algorithm adapted to the real-time calculation of the nonparametric
classifier in massive as well as streaming data frameworks. This online
classifier includes two steps. First, we consider an online principle
components analysis to reduce the dimension of the features with a very low
computation cost. Then, a stochastic approximation algorithm is deployed to
obtain a real-time calculation of the nonparametric classifier. The proposed
methods are evaluated and compared to some commonly used machine learning
algorithms for real-time fetal well-being monitoring. The study revealed that,
in terms of accuracy, the offline (or Batch), as well as, the online
classifiers are good competitors to the random forest algorithm. Moreover, we
show that the online classifier gives the best trade-off accuracy/computation
cost compared to the offline classifier.",2024-05-29,stat.ML,
http://arxiv.org/abs/2405.19485v1,"A promising area of applications for quantum computing is in linear algebra
problems. In this work, we introduce two new quantum t-SVD (tensor-SVD)
algorithms. The first algorithm is largely based on previous work that proposed
a quantum t-SVD algorithm for context-aware recommendation systems. The new
algorithm however seeks to address and fix certain drawbacks to the original,
and is fundamentally different in its approach compared to the existing work.
The second algorithm proposed uses a hybrid variational approach largely based
on a known variational quantum SVD algorithm.",2024-05-29,quant-ph,
http://arxiv.org/abs/2405.19484v1,"We study caustics of an elliptical paraboloid and the history of their
various representations from 3D models in XIX century to the recent computer
graphics. In the paper two ways of generating the surface, one with cartesian
coordinates using formula for principal curvatures, and the other one with
parabolic coordinates using Seidel's formula were demonstrated. By finding the
intersection curves of these caustics with the paraboloid we extend the
solution of F. Caspari for classical Apollonius problem about the number of
concurrent normals to the points of the paraboloid itself. A complete
classification of all possible cases of intersections of these caustics with
their paraboloid is given.",2024-05-29,math.DG,
http://arxiv.org/abs/2405.19483v1,"We explore a class of splitting schemes employing implicit-explicit (IMEX)
time-stepping to achieve accurate and energy-stable solutions for thin-film
equations and Cahn-Hilliard models with variable mobility. This splitting
method incorporates a linear, constant coefficient implicit step, facilitating
efficient computational implementation. We investigate the influence of
stabilizing splitting parameters on the numerical solution computationally,
considering various initial conditions. Furthermore, we generate
energy-stability plots for the proposed methods, examining different choices of
splitting parameter values and timestep sizes. These methods enhance the
accuracy of the original bi-harmonic-modified (BHM) approach, while preserving
its energy-decreasing property and achieving second-order accuracy. We present
numerical experiments to illustrate the performance of the proposed methods.",2024-05-29,math.NA,
http://arxiv.org/abs/2405.19482v1,"We study Malliavin differentiability for the solutions of a stochastic
differential equation with drift of super-linear growth. Assuming we have a
monotone drift with polynomial growth, we prove Malliavin differentiability of
any order. As a consequence of this result, under the H\""ormander's hypothesis
we prove that the density of the solution's law with respect to the Lebesgue
measure is infinitely differentiable. To avoid non-integrability problems due
to the unbounded drift, we follow an approach based on the concepts of Ray
Absolute Continuity and Stochastic Gate\^aux Differentiability.",2024-05-29,math.PR,
http://arxiv.org/abs/2405.19481v1,"This paper proposes a novel waveform design method named COSMIC
(Connectivity-Oriented Sensing Method for Imaging and Communication). These
waveforms are engineered to convey communication symbols while adhering to an
extended orthogonality condition, enabling their use in generating radio images
of the environment. A Multiple-Input Multiple-Output (MIMO) Radar-Communication
(RadCom) device transmits COSMIC waveforms from each antenna simultaneously
within the same time window and frequency band, indicating that orthogonality
is not achieved by space, time, or frequency multiplexing. Indeed,
orthogonality among the waveforms is achieved by leveraging the degrees of
freedom provided by the assumption that the field of view is limited or
significantly smaller than the transmitted signals' length. The RadCom device
receives and processes the echoes from an infinite number of infinitesimal
scatterers within its field of view, constructing an electromagnetic image of
the environment. Concurrently, these waveforms can also carry information to
other connected network entities. This work provides the algebraic concepts
used to generate COSMIC waveforms. Moreover, an opportunistic optimization of
the imaging and communication efficiency is discussed. Simulation results
demonstrate that COSMIC waveforms enable accurate environmental imaging while
maintaining acceptable communication performances.",2024-05-29,eess.SP,
http://arxiv.org/abs/2405.19478v1,"Competition during range expansions is of great interest from both practical
and theoretical view points. Experimentally, range expansions are often studied
in homogeneous Petri dishes, which lack spatial anisotropy that might be
present in realistic populations. Here, we analyze a model of anisotropic
growth, based on coupled Kardar-Parisi-Zhang and
Fisher-Kolmogorov-Petrovsky-Piskunov equations that describe surface growth and
lateral competition. Compared to a previous study of isotropic growth,
anisotropy relaxes a constraint between parameters of the model. We completely
characterize spatial patterns and invasion velocities in this generalized
model. In particular, we find that strong anisotropy results in a distinct
morphology of spatial invasion with a kink in the displaced strain ahead of the
boundary between the strains. This morphology of the out-competed strain is
similar to a shock wave and serves as a signature of anisotropic growth.",2024-05-29,nlin.PS,
http://arxiv.org/abs/2405.19477v1,"We investigate the stellar and nebular properties of 9 H II regions in the
spiral galaxy M101 with far-ultraviolet (FUV; ~900-2000 \r{A}) and optical
(~3200-10000 \r{A}) spectra. We detect significant C III] 1907,1909 nebular
emission in 7 regions, but O III] 1666 only in the lowest-metallicity region.
We produce new analytic functions of the carbon ICF as a function of
metallicity in order to perform a preliminary C/O abundance analysis. The FUV
spectra also contain numerous stellar emission and P-Cygni features that we fit
with luminosity-weighted combinations of single-burst Starburst99 and BPASS
models. We find that the best-fit Starburst99 models closely match the observed
very-high-ionization P-Cygni features, requiring very-hot, young (~< 3 Myr),
metal-enriched massive stars. The youngest stellar populations are strongly
correlated with broad He II emission, nitrogen Wolf-Rayet (WR) FUV and optical
spectral features, and enhanced N/O gas abundances. Thus, the short-lived WR
phase may be driving excess emission in several N P-Cygni wind features (955
\r{A}, 991 \r{A}, 1720 \r{A}) that bias the stellar continuum fits to higher
metallicities relative to the gas-phase metallicities. Accurate
characterization of these H II regions requires additional inclusion of WR
stars in the stellar population synthesis models. Our FUV spectra demonstrate
that the ~900-1200 \r{A} FUV can provide a strong test-bed for future WR
atmosphere and evolution models.",2024-05-29,astro-ph.GA,
http://arxiv.org/abs/2405.19473v1,"We consider families of strongly indefinite systems of elliptic PDE and
investigate bifurcation from a trivial branch of solutions by using the
spectral flow. The novelty in our approach is a refined version of a comparison
principle that was originally proved by Pejsachowicz in a joint work with the
third author, and which is based on an index theorem for a certain class of
Fredholm operators that is of independent interest. Finally, we use our
findings for a bifurcation problem on shrinking domains that originates from
works of Morse and Smale.",2024-05-29,math.AP,
http://arxiv.org/abs/2405.19472v1,"Over the last three decades, several experimental initiatives have been
launched with the goal of observing radio-frequency signals produced by
ultra-high energy neutrinos (UHEN) interacting in solid media. Observed
neutrino event signatures comprise impulsive signals with duration of order the
inverse of the antenna+system bandwidth, superimposed upon an incoherent
(typically white noise) thermal noise spectrum. Although bulk volume scattering
(VS) of external radio-frequency signals is well-studied within the radar and
glaciological communities, and can, in principle, contribute to that
background. However, thus far, neutrino experiments have neglected to account
for VS when projecting event rates. Herein, we present both model-dependent and
model-independent constraints on both coherent, and also incoherent volume
scattering, and assess their impact on UHEN experiments. We find that VS
contributions are only weakly constrained by extant data; stronger limits may
be obtained with dedicated calibration experiments.",2024-05-29,astro-ph.IM,
http://arxiv.org/abs/2405.19468v1,"In this paper, we study the problem, initiated by Grothendieck, of the
transfer of the properties of two preadic noetherian algebras over a field to
their completed tensor product.",2024-05-29,math.AC,
http://arxiv.org/abs/2405.19467v1,"Many stellar configurations, including white dwarfs, neutron stars, black
holes, supermassive stars, and star clusters, rely on relativistic effects. The
Tolman-Oppenheimer-Volkoff (TOV) equation of the polytropic gas sphere is
ultimately a hydrostatic equilibrium equation developed from the general
relativity framework. In the modified Rieman Liouville (mRL) frame, we
formulate the fractional TOV (FTOV) equations and introduce an analytical
solution. Using power series expansions to solve the fractional TOV equations
yields a limited physical range to the convergent power series solution.
Therefore, the two techniques of Euler-Abel transformation and Pade
approximation have been combined to improve the convergence of the obtained
series solutions. For all possible values of the relativistic parameters
(\sigma), we calculated twenty fractional gas models for the polytropic indexes
n=0, 0.5, 1, 1.5, 2. Investigating the impacts of fractional and relativistic
parameters on the models revealed fascinating phenomena; the two effects for
n=0.5 are that the sphere's volume and mass decrease with increasing \sigma and
the fractional parameter (\alpha). For n=1, the volume decreases when
\sigma=0.1 and then increases when \sigma=0.2 and 0.3. The volume of the sphere
reduces as both \sigma and \alpha increase for n=1.5 and n=2. We calculated the
maximum mass and the corresponding minimum radius of the white dwarfs modeled
with polytropic index n=3 and several fractional and relativistic parameter
values. We obtained a mass limit for the white dwarfs somewhat near the
Chandrasekhar limit for the integer models with small relativistic parameters
(\alpha=1, \sigma=0.001). The situation is altered by lowering the fractional
parameter; the mass limit increases to Mlimit=1.63348 M at \alpha=0.95 and
\sigma=0.001.",2024-05-29,gr-qc,
http://arxiv.org/abs/2405.19448v2,"The LBNF Absorber consists of thirteen 6061-T6 aluminum core blocks. The core
blocks are water cooled with de-ionized (DI) water which becomes radioactive
during beam operations. The cooling water flows through gun-drilled channels in
the core blocks. The cooling water is supplied by the LBNF Absorber Radioactive
Water (RAW) cooling system which is designed as per ASME B31.3 Normal Fluid
Service [1]. An uninhibited beam accident pulse striking the water channels was
identified as a credible accident scenario. In this study, it is assumed that
the beam pulse hits the Absorber directly without interacting with any of the
other upstream beamline components. The beam parameters used for the LBNF beam
are 120 GeV, 2.4 MW with a 1.2 s cycle time. The accident pulse lasts for 10
{\mu}s. The maximum energy is deposited in the 3rd aluminum core block. For the
sake of simplicity, it is assumed that the accident pulse strikes the 1 in. ID
water channel directly. The analysis here simulates the pressure rise in the
water during and after the beam pulse and its effects on the aluminum piping
components that deliver water to the core blocks. The weld strengths as
determined by the Load and Resistance Factor Design (LRDF) and the Allowable
Strength Design (ASD) are compared to the forces generated in the weld owing to
the pressure spike. A transient structural analysis was used to determine the
equivalent membrane, peak, and bending stresses and they were com-pared to
allowable limits.",2024-05-29,physics.acc-ph,
http://arxiv.org/abs/2405.19446v1,"Fracton phases of matter constitute an interesting point of contact between
condensed matter and high-energy physics. The limited mobility property of
fracton quasiparticles finds applications in many different contexts, including
quantum information, spin liquids, elasticity, hydrodynamics, gravity and
holography. In this paper we adopt a field theoretical approach to investigate
the three dimensional action of a rank-2 symmetric tensor field invariant under
the covariant fracton symmetry. The theory appears as a non-topological higher
rank generalization of the ordinary Chern-Simons model, depending only on the
traceless part of the tensor gauge field. After defining a field strength, a
rank-2 traceless ``electric'' field and a ``magnetic'' vector field are
identified, in analogy with the standard Chern-Simons ones. Once matter is
introduced, a Hall-like behaviour with fractonic features emerges. In
particular, our model shows a Hall-like dipole current, together with a
vectorial ``flux-attachment'' relation for dipoles. This gives a possible
starting point for a fracton - vortex duality. A gauge-fixing term is then
introduced, from which propagators are computed and the counting of the degrees
of freedom is performed. Finally, the energy-momentum tensor is shown to be
conserved and the integrated energy density is proved to be zero, which reminds
the topological nature of the standard Chern-Simons model.",2024-05-29,hep-th,
http://arxiv.org/abs/2405.19445v1,"The temporal evolution of weak shocks in radiative media is theoretically
investigated in this work. The structure of radiative shocks has traditionally
been studied in a stationary framework. Their systematic classification is
complex because layers of optically thin and thick regions alternate to form a
radiatively-driven precursor and a temperature-relaxation layer, between which
the hydrodynamic shock is embedded. In this work, we analyze the formation of
weak shocks when two radiative plasmas with different pressures are put in
contact. Applying a reductive perturbative method yields a Burgers-type
equation that governs the temporal evolution of the perturbed variables
including the radiation field. The conditions upon which optically thin and
thick solutions exist have been derived and expressed as a function of the
shock strength and Boltzmann number. Below a certain Boltzmann number
threshold, weak shocks always become optically thick asymptotically in time,
while thin solutions appear as transitory structures. The existence of an
optically thin regime is related to the presence of an overdense layer in the
compressed material. Scaling laws for the characteristic formation time and
shock width are provided for each regime. The theoretical analysis is supported
by FLASH simulations, and a comprehensive test-case has been designed to
benchmark radiative hydrodynamic codes.",2024-05-29,physics.plasm-ph,
http://arxiv.org/abs/2405.19443v1,"Crossbridge binding and force in active muscle is dependent on the radial
spacing between the thick and the thin filaments. This radial lattice spacing
has been shown through spatially explicit modeling and experimental efforts to
greatly affect isometric, force production in muscle. It has recently been
suggested that this radial spacing might also be able to drive differences in
mechanical function, or net work, under dynamic oscillations like those which
occur in muscles in vivo. However, previous spatially explicit models either
had no radial spacing dependence, meaning the lattice spacing could not be
investigated, or did include radial spacing dependence but could not reproduce
in vivo net work during dynamic oscillations and only investigated isometric
contractions. Here we show the first spatially explicit model to include radial
crossbridge dependence which can produce mechanical function similar to real
muscle. Using this spatially explicit model of a half sarcomere, we show that
when oscillated at strain amplitudes and frequencies like those in the hawk
moth Manduca sexta, net work does depend on the lattice spacing. In addition,
we can prescribe a trajectory of lattice spacing changes in the spatially
explicit half sarcomere model and investigate the extent to which the time
course of lattice spacing changes can affect mechanical function. We simulated
a half sarcomere undergoing dynamic oscillations and prescribed the Poisson's
ratio of the lattice to be either 0 (constant lattice spacing) or 0.5
(isovolumetric lattice spacing changes). We also simulated net work using
lattice spacing data taken from Manduca sexta which has a variable Poisson's
ratio. Our simulation indicates that the lattice spacing can change the
mechanical function of muscle, and that in some cases a 1 nm difference can
switch the net work of the half sarcomere model from motor like to brake like.",2024-05-29,physics.bio-ph,
http://arxiv.org/abs/2405.19435v1,"Previous studies conclusively show that pencil-and-paper lecture-tutorials
(LTs) are incredibly effective at increasing student engagement and learning
gains on a variety of topics when compared to traditional lecture. LTs in
astronomy are post-lecture activities developed with the intention of helping
students engage with conceptual and reasoning difficulties around a specific
topic with the end goal of them developing a more expert-like understanding of
astrophysical concepts. To date, all astronomy LTs have been developed for
undergraduate courses taught in-person. Increases in online course enrollments
and the COVID-19 pandemic further highlighted the need for additional
interactive, research-based, curricular materials designed for online
classrooms. To this end, we developed and assessed the efficacy of an
innovative, interactive LT designed to teach planet formation in asynchronous,
online, introductory astronomy courses for undergraduates. We utilized the
Planet Formation Concept Inventory to compare learning outcomes between courses
that implemented the new online, interactive LT, and those that used either a
lecture-only approach or utilized a standard pencil-and-paper LT on the same
topic. Overall, learning gains from the standard pencil-and-paper LT were
statistically indistinguishable from the in-person implementation of the online
LT and both of these conditions outperformed the lecture-only condition.
However, when implemented asynchronously, learning gains from the online LT
were lower and not significantly above the lecture-only condition. While
improvements can be made to improve the online LT in the future, the current
discipline ideas still outperform traditional lecture, and can be used as a
tool to teach planet formation effectively.",2024-05-29,physics.ed-ph,
http://arxiv.org/abs/2405.19432v1,"We study the effects of correlations in a random environment on a random
walker. The dependence of its asymptotic speed on the correlations is a
nonperturbative effect as it is not captured by a homogeneous version of the
same environment. For a slowly cooling environment, the buildup of correlations
modifies the walker's speed and, by so, realizes acceleration. We remark on the
possible relevance in the discussion of cosmic acceleration as traditionally
started from the Friedmann equations, which, from a statistical mechanical
point of view, would amount to a mean-field approximation. Our environment is
much simpler though, with transition rates sampled from the one-dimensional
Ising model and allowing exact results and detailed velocity characteristics.",2024-05-29,cond-mat.stat-mech,
http://arxiv.org/abs/2405.19427v1,"The formalism of generalized quantum histories allows a symmetrical treatment
of space and time correlations, by taking different traces of the same history
density matrix. We recall how to characterize spatial and temporal entanglement
in this framework. An operative protocol is presented, to map a history state
into the ket of a static composite system. We show, by examples, how the
Leggett-Garg and the temporal CHSH inequalities can be violated in our
approach.",2024-05-29,quant-ph,
http://arxiv.org/abs/2406.05128v1,"Training the linear prediction (LP) operator end-to-end for audio synthesis
in modern deep learning frameworks is slow due to its recursive formulation. In
addition, frame-wise approximation as an acceleration method cannot generalise
well to test time conditions where the LP is computed sample-wise. Efficient
differentiable sample-wise LP for end-to-end training is the key to removing
this barrier. We generalise the efficient time-invariant LP implementation from
the GOLF vocoder to time-varying cases. Combining this with the classic
source-filter model, we show that the improved GOLF learns LP coefficients and
reconstructs the voice better than its frame-wise counterparts. Moreover, in
our listening test, synthesised outputs from GOLF scored higher in quality
ratings than the state-of-the-art differentiable WORLD vocoder.",2024-06-07,eess.AS,
http://arxiv.org/abs/2406.05124v1,"Byzantine fault-tolerant consensus protocols have provable safety and
liveness properties for static validator sets. In practice, however, the
validator set changes over time, potentially eroding the protocol's security
guarantees. For example, systems with accountable safety may lose some of that
accountability over time as adversarial validators exit. As a result, protocols
must rate limit entry and exit so that the set changes slowly enough to ensure
security. Here, the system designer faces a fundamental trade-off. Slower exits
increase friction, making it less attractive to stake in the first place.
Faster exits provide more utility to stakers but weaken the protocol's
security.
  This paper provides the first systematic study of exit queues for
Proof-of-Stake blockchains. Given a collection of validator-set consistency
constraints imposed by the protocol, the social planner's goal is to provide a
constrained-optimal mechanism that minimizes disutility for the participants.
We introduce the MINSLACK mechanism, a dynamic capacity first-come-first-served
queue in which the amount of stake that can exit in a period depends on the
number of previous exits and the consistency constraints. We show that MINSLACK
is optimal when stakers equally value the processing of their withdrawal. When
stakers values are heterogeneous, the optimal mechanism resembles a priority
queue with dynamic capacity. However, this mechanism must reserve exit capacity
for the future in case a staker with a much higher need for liquidity arrives.
We conclude with a survey of known consistency constraints and highlight the
diversity of existing exit mechanisms.",2024-06-07,econ.TH,
http://arxiv.org/abs/2406.05118v1,"The SRG/eROSITA All-Sky Survey (eRASS) is expected to contain ~100 quasars
that emitted their light when the universe was less than a billion years old,
i.e. at z>5.6. By selection, these quasars populate the bright end of the AGN
X-ray luminosity function and their count offers a powerful demographic
diagnostic of the parent super-massive black hole population. Of the >~ 400
quasars that have been discovered at z>5.6 to date, less than 15 % have been
X-ray detected. We present a pilot survey to uncover the elusive X-ray luminous
end of the distant quasar population. We have designed a quasar selection
pipeline based on optical, infrared and X-ray imaging data from DES DR2, VHS
DR5, CatWISE2020 and the eRASS. The core selection method relies on SED
template fitting. We performed optical follow-up spectroscopy with the
Magellan/LDSS3 instrument for the redshift confirmation of a subset of
candidates. We have further obtained a deeper X-ray image of one of our
candidates with Chandra ACIS-S. We report the discovery of five new quasars in
the redshift range 5.6 < z < 6.1. Two of these quasars are detected in eRASS
and are by selection X-ray ultra-luminous. These quasars are also detected at
radio frequencies. The first one is a broad absorption line quasar which shows
significant X-ray dimming over 3.5 years, i.e. about 6 months in the quasar
rest frame. The second radio-detected quasar is a jetted source with compact
morphology. We show that a blazar configuration is likely for this source,
making it the second most distant blazar known to date. With our pilot study,
we demonstrate the power of eROSITA as a discovery machine for luminous quasars
in the epoch of reionization. The X-ray emission of the two eROSITA detected
quasars are likely to be driven by different high-energetic emission mechanisms
a diversity which will be further explored in a future systematic
full-hemisphere survey.",2024-06-07,astro-ph.GA,
http://arxiv.org/abs/2406.05115v1,"We present a generator for lepton nucleon collisions in the DIS regime,
focusing in particular on processes with a massive lepton and/or a massive
quark in the final state. We have built a full code matching NLO QCD
corrections to parton shower Monte Carlo programs in the POWHEG-BOX framework.
Our code can be used to compute NLO+PS accurate fully differential predictions
for neutral current and charged current processes, including processes with an
incoming tau neutrino, and/or including charm quarks in the final state. We
also made comparisons with available data and predictions for the new neutrino
experiments at CERN.",2024-06-07,hep-ph,
http://arxiv.org/abs/2406.05112v1,"The quantum conductance and its classical wave analogue, the transmittance,
are given by the sum of the eigenvalues of the transmission matrix. The lowest
transmission eigenvalue in diffusive media might be expected to play a
negligible role in the conductance, and, in any case, to be too small to be
observed. Here, we observe the lowest transmission eigenchannel in microwave
waveguides, though it is orders of magnitude below the nominal noise level, and
show that the transmittance is pulled down by global correlation among
transmission eigenvalues and among zeros and poles of the transmission matrix.
Transmission vanishes either when the energy density on the sample output
vanishes at topological transmission zeros or when the longitudinal velocity
vanishes precisely at the crossover to a new channel. This lowers the
conductance by an amount proportional to the modulation of the density of
states. In accord with the correspondence principle, the conductance approaches
Ohms law as the number of channels increases with sample width. The exploration
of the transmission matrix opens the door to a new understanding of mesoscopic
transport and ultrasensitive detection techniques.",2024-06-07,cond-mat.mes-hall,
http://arxiv.org/abs/2406.05108v1,"Physics informed neural networks have been gaining popularity due to their
unique ability to incorporate physics laws into data-driven models, ensuring
that the predictions are not only consistent with empirical data but also align
with domain-specific knowledge in the form of physics equations. The
integration of physics principles enables the method to require less data while
maintaining the robustness of deep learning in modeling complex dynamical
systems. However, current PINN frameworks are not sufficiently mature for
real-world ODE systems, especially those with extreme multi-scale behavior such
as mosquito population dynamical modelling. In this research, we propose a PINN
framework with several improvements for forward and inverse problems for ODE
systems with a case study application in modelling the dynamics of mosquito
populations. The framework tackles the gradient imbalance and stiff problems
posed by mosquito ordinary differential equations. The method offers a simple
but effective way to resolve the time causality issue in PINNs by gradually
expanding the training time domain until it covers entire domain of interest.
As part of a robust evaluation, we conduct experiments using simulated data to
evaluate the effectiveness of the approach. Preliminary results indicate that
physics-informed machine learning holds significant potential for advancing the
study of ecological systems.",2024-06-07,q-bio.PE,
http://arxiv.org/abs/2406.05105v1,"One of the most exciting benefits of solar small-scale brightening is their
oscillations, this study investigated the properties of small-scale brightening
(SSBs) in different regions of the Sun and found that there are differences and
similarities in the properties of oscillated and non-oscillated SSBs in
different regions of the Sun, including quiet Sun (QS), the adjacent to active
regions (AAR), and coronal hole (CH). The damping per period (Q-factor) and
maximum Doppler velocity of SSBs varied depending on the region, with the less
bright internetwork SSBs in QS having lower damping time (120 seconds) and
higher maximum Doppler velocities (47 km/s) compared to the brighter network
SSBs (with 216 seconds & 37 km/s, respectively), while in AAR, internetwork
SSBs tend to have higher damping time (about of 220 seconds) and wider maximum
Doppler velocity (10 to 140 km/s) ranges compared to network SSBs (130 seconds
& 10 to 85 km/s). In CH, both types of SSBs show similar damping time (120
seconds), but internetwork SSBs tend to have higher maximum Doppler velocities
(100 km/s) compared to network SSBs (85 km/s). Also, it was pointed out that
the majority of network SSBs in AARs are in the overdamping mode, while in QS,
internetwork SSBs demonstrate overdamping behavior and oscillated network SSBs
exhibit critical damping behavior. It is important to bear in mind, however,
that the physical mechanisms underlying the damping of SSBs may vary depending
on the local plasma conditions and magnetic environment.",2024-06-07,astro-ph.SR,
http://arxiv.org/abs/2406.05104v1,"In this article we consider moment problems equivalent to null
controllability of some linear parabolic partial differential equations in
space dimension higher than one. For these moment problems, we prove existence
of an associated biorthogonal family and estimate its norm. The considered
setting requires the space domain to be a cylinder and the evolution operator
to be tensorized. Roughly speaking, we assume that the so-called
Lebeau-Robbiano spectral inequality holds but only for the eigenvectors of the
transverse operator. In the one dimensional tangent variable we assume the
solvability of block moment problem as introduced in [Benabdallah, Boyer and
Morancey - \textit{Ann. H. Lebesgue.} 3 (2020)]. We apply this abstract
construction of biorthogonal families to the characterization of the minimal
time for simultaneous null controllability of two heat-like equations in a
cylindrical domain. To the best of our knowledge, this result is unattainable
with other known techniques.",2024-06-07,math.AP,
http://arxiv.org/abs/2406.05100v1,"We study finite temperature dynamical correlation functions of the
magnetization operator in the one-dimensional Ising quantum field theory. Our
approach is based on a finite temperature form factor series and on a Fredholm
determinant representation of the correlators. While for space-like separations
the Fredholm determinant can be efficiently evaluated numerically, for the
time-like region it has convergence issues inherited from the form factor
series. We develop a method to compute the correlation functions at time-like
separations based on the analytic continuation of the space-time coordinates to
complex values. Using this numerical technique, we explore all space-time and
temperature regimes in both the ordered and disordered phases including short,
large, and near-light-cone separations at low and high temperatures. We confirm
the existing analytic predictions for the asymptotic behavior of the
correlations except in the case of space-like correlations in the paramagnetic
phase. For this case we derive a new closed form expression for the correlation
length that has some unusual properties: it is a non-analytic function of both
the space-time direction and the temperature, and its temperature dependence is
non-monotonic.",2024-06-07,cond-mat.stat-mech,
http://arxiv.org/abs/2406.05098v1,"In this work, we develop a simulation-based model to predict the density
split (DSS) and second-order shear and clustering statistics. A
simulation-based model has the potential to model highly non-linear scales
where current DSS models fail. To build this model, we use the
\texttt{AbacusSummit} N-body simulation suite from which we measure all
necessary statistics and train an emulator based on \texttt{CosmoPower}. In
that context, we discuss possible improvements for future emulators to make the
measurement less noisy and biased, resulting in more accurate and precise model
predictions.
  Regarding the emulator's accuracy, we find that the most important aspect is
the average of the summary statistics over multiple-shot noise realizations of
the foreground galaxies. However, these results probably depend on the chosen
number density of the foreground galaxies. Regarding the parameter forecast
based on preliminary LOWZxUNIONS data, we find that DSS has more constraining
power to derive cosmological parameters and that the joint analysis with
second-order statistics is particularly useful for extracting parameters of the
galaxy-halo connection.",2024-06-07,astro-ph.CO,
http://arxiv.org/abs/2406.05094v1,"The European carbon market plays a pivotal role in the European Union's
ambitious target of achieving carbon neutrality by 2050. Understanding the
intricacies of factors influencing European Union Emission Trading System (EU
ETS) market prices is paramount for effective policy making and strategy
implementation. We propose the use of the Information Imbalance, a recently
introduced non-parametric measure quantifying the degree to which a set of
variables is informative with respect to another one, to study the
relationships among macroeconomic, economic, uncertainty, and energy variables
concerning EU ETS prices. Our analysis shows that in Phase 3 commodity related
variables such as the ERIX index are the most informative to explain the
behaviour of the EU ETS market price. Transitioning to Phase 4, financial
fluctuations take centre stage, with the uncertainty in the EUR/CHF exchange
rate emerging as a crucial determinant. These results reflect the disruptive
impacts of the COVID-19 pandemic and the energy crisis in reshaping the
importance of the different variables. Beyond variable analysis, we also
propose to leverage the Information Imbalance to address the problem of
mixed-frequency forecasting, and we identify the weekly time scale as the most
informative for predicting the EU ETS price. Finally, we show how the
Information Imbalance can be effectively combined with Gaussian Process
regression for efficient nowcasting and forecasting using very small sets of
highly informative predictors.",2024-06-07,q-fin.ST,
http://arxiv.org/abs/2406.05093v1,"The interactions between the Magellanic Clouds significantly affect the shape
and distribution of the young stellar population, particularly in the periphery
of the Small Magellanic Cloud (SMC). We present the first far-UV (FUV) map of
the north-east SMC-Shell region using the Ultra Violet Imaging Telescope (UVIT)
onboard AstroSat. The detected FUV stars are combined with Gaia Early Data
Release 3 data to create a FUV-optical catalog of ~ 14,400 stars. FUV-optical
colour-magnitude diagrams are used along with isochrones to estimate the
stellar ages. The detected stars are formed in multiple episodes. We identified
two episodes of star formation (~ 60 and ~ 260 Myr ago) where the episode at ~
260 Myr is linked to the recent interaction with the Large Magellanic Cloud
(LMC) and the episode at ~ 60 Myr is linked to the pericentric passage of the
SMC around our Galaxy. The median proper motion (PM) and velocity dispersion
are found to be similar to the SMC main body, indicating that this region has
not experienced significant tidal effects. The FUV stellar surface density and
the dispersion in PM suggest an extent of the inner SMC in the north-east
direction to be around 2.2 deg. We detect arm-like and arc-like structures in
the FUV stellar density map, and their kinematics appear to be similar to the
SMC main body. These extended outer features are the spatial stellar
overdensities formed over multiple episodes of star formation, but without
apparent kinematic distinction.",2024-06-07,astro-ph.GA,
http://arxiv.org/abs/2406.05091v1,"Understanding the characteristics of young stellar populations is essential
for deriving insights into star formation processes within parent molecular
clouds and the influence of massive stars. This study focuses on YSOs in the G
045.49+00.04 molecular cloud, including three UC HII regions. Using infrared
photometric data and radiation models, we identified 1482 YSOs with masses
ranging from 1.5 to 22 Msol. Among these, 315 form dense clusters near IRAS
sources, with high-mass stars responsible for ionization. The non-cluster YSOs
(1168) are uniformly distributed on the field. The distribution of YSOs from
both samples on the colour-magnitude diagram and by the evolutionary ages is
different. About 75% of objects in the IRAS clusters are concentrated around
the ZAMS and have a well-defined peak at an age of Log(Age[years]) = 6.75, with
a narrow spread. The non-cluster objects have two concentrations located to the
right and left of the 0.1 Myr isochrone and two well-defined peaks at Log(Age)
= 6.25 and 5.25. The K luminosity functions alpha slopes for the IRAS clusters
and non-cluster objects are 0.32 and 0.72, respectively. The steeper alpha
slope is suggesting that the non-cluster objects are less evolved, which is
well consistent with the evolutionary age. Similar patterns were observed in
the neighboring G 45.14+00.14 region. The both regions (G 045.49+00.04 and G
45.14+00.14) are located and distinguished by their brightness and density at
the edge of the bubble around the highly variable X-ray binary GRS 1915+105,
which includes a black hole and a K-giant companion. Based on the above, we can
assume that the process of star formation in the young IRAS clusters was
triggered by the GRS 915+105-initiated shock front inside the ISM massive
condensation, through the process of ""collecting and collapse"". Most
non-cluster objects probably belong to a later generation.",2024-06-07,astro-ph.GA,
http://arxiv.org/abs/2406.05086v1,"The problem of reward design examines the interaction between a leader and a
follower, where the leader aims to shape the follower's behavior to maximize
the leader's payoff by modifying the follower's reward function. Current
approaches to reward design rely on an accurate model of how the follower
responds to reward modifications, which can be sensitive to modeling
inaccuracies. To address this issue of sensitivity, we present a solution that
offers robustness against uncertainties in modeling the follower, including 1)
how the follower breaks ties in the presence of nonunique best responses, 2)
inexact knowledge of how the follower perceives reward modifications, and 3)
bounded rationality of the follower. Our robust solution is guaranteed to exist
under mild conditions and can be obtained numerically by solving a
mixed-integer linear program. Numerical experiments on multiple test cases
demonstrate that our solution improves robustness compared to the standard
approach without incurring significant additional computing costs.",2024-06-07,math.OC,
http://arxiv.org/abs/2406.05081v1,"Pulsar Timing Array projects have found evidence of a stochastic background
of gravitational waves (GWB) using data from an ensemble of pulsars. In the
literature, minimal assumptions are made about the signal and noise processes
that affect data from these pulsars, such as pulsar spin noise. These
assumptions are encoded as uninformative priors in Bayesian searches, though
Frequentist approaches make similar assumptions. Uninformative priors are not
suitable for (noise) properties of pulsars in an ensemble, and they bias
estimates of model parameters such as gravitational-wave signal parameters.
Both Frequentist and Bayesian searches are affected. In this letter, more
appropriate priors are proposed in the language of Hierarchical Bayesian
Modeling, where the properties of the ensemble of pulsars are jointly described
with the properties of the individual components of the ensemble. Results by
Pulsar Timing Array projects should be re-evaluated using Hierarchical Models.",2024-06-07,astro-ph.IM,
http://arxiv.org/abs/2406.05074v1,"Pathology, the microscopic examination of diseased tissue, is critical for
diagnosing various medical conditions, particularly cancers. Traditional
methods are labor-intensive and prone to human error. Digital pathology, which
converts glass slides into high-resolution digital images for analysis by
computer algorithms, revolutionizes the field by enhancing diagnostic accuracy,
consistency, and efficiency through automated image analysis and large-scale
data processing. Foundational transformer pretraining is crucial for developing
robust, generalizable models as it enables learning from vast amounts of
unannotated data.
  This paper introduces the Hibou family of foundational vision transformers
for pathology, leveraging the DINOv2 framework to pretrain two model variants,
Hibou-B and Hibou-L, on a proprietary dataset of over 1 million whole slide
images (WSIs) representing diverse tissue types and staining techniques. Our
pretrained models demonstrate superior performance on both patch-level and
slide-level benchmarks, surpassing existing state-of-the-art methods. Notably,
Hibou-L achieves the highest average accuracy across multiple benchmark
datasets. To support further research and application in the field, we have
open-sourced the Hibou-B model, which can be accessed at
https://github.com/HistAI/hibou",2024-06-07,eess.IV,
http://arxiv.org/abs/2406.05073v1,"Signals from interacting brain regions display transient synchronization of
phases and amplitudes in different frequencies. Commonly, the interaction
between regions of the brain is quantitatively described by either analyzing
the correlations of amplitudes of the measured signals or by calculating
phase-synchronization measures. However, for a complete picture of the
interactions it is important to analyze the dynamics of both the amplitude and
the phase. In this work, we present a new method for finding the coupling
between brain regions by reconstructing the phase-amplitude dynamics directly
from the measured electrophysiological signals. For this purpose, we use the
recent advances in the field of phase-amplitude reduction of oscillatory
systems, which allow the representation of an uncoupled oscillatory system as a
phase-amplitude oscillator in a unique form using transformations
(parametrizations) related to the eigenfunctions of the Koopman operator. By
combining the parametrization method and the Fourier-Laplace averaging method
of finding the eigenfunctions of the Koopman operator, we developed a novel
method of assessing the transformation functions from the signals of the
interacting oscillatory systems. The resulting reconstructed dynamical system
is a network of phase-amplitude oscillators with the interactions between them
represented as coupling functions in phase and amplitude coordinates. Using
synthetic signals generated from several models with known and unknown
theoretical phase-amplitude reduced forms, we demonstrate that our method is
capable of finding the proper unique dynamic form of these oscillatory systems
in the reduced phase-amplitude space. Our method can be applied to describe any
network of interacting oscillators as a dynamical system using signals of the
network elements.",2024-06-07,math.DS,
http://arxiv.org/abs/2406.05065v1,"The rapid growth of Speech Emotion Recognition (SER) has diverse global
applications, from improving human-computer interactions to aiding mental
health diagnostics. However, SER models might contain social bias toward
gender, leading to unfair outcomes. This study analyzes gender bias in SER
models trained with Self-Supervised Learning (SSL) at scale, exploring factors
influencing it. SSL-based SER models are chosen for their cutting-edge
performance. Our research pioneering research gender bias in SER from both
upstream model and data perspectives. Our findings reveal that females exhibit
slightly higher overall SER performance than males. Modified CPC and XLS-R, two
well-known SSL models, notably exhibit significant bias. Moreover, models
trained with Mandarin datasets display a pronounced bias toward valence.
Lastly, we find that gender-wise emotion distribution differences in training
data significantly affect gender bias, while upstream model representation has
a limited impact.",2024-06-07,eess.AS,
http://arxiv.org/abs/2406.05062v1,"To aid in prediction of turbulent boundary layer flows over rough surfaces, a
new model is proposed to estimate hydrodynamic roughness based solely on
geometric surface information. The model is based on a fluid-mechanics
motivated geometric parameter called the wind-shade factor. Sheltering is
included using a rapid algorithm adapted from the landscape shadow literature,
while local pressure drag is estimated using a piecewise potential flow
approximation. Similarly to evaluating traditional surface parameters such as
skewness or average slope magnitude, the wind-shade factor is purely geometric
and can be evaluated efficiently from knowing the surface elevation map and the
mean flow direction. The wind-shade roughness model is applied to over 100
different surfaces available in a public roughness database and some others,
and the predicted sandgrain-roughness heights are compared to measured values.
Effects of various model ingredients are analyzed, and transitionally rough
surfaces are treated by adding a term representing the viscous stress
component.",2024-06-07,physics.flu-dyn,
http://arxiv.org/abs/2406.05058v1,"In the infectious disease literature, significant effort has been devoted to
studying dynamics at a single scale. For example, compartmental models
describing population-level dynamics are often formulated using differential
equations. In cases where small numbers or noise play a crucial role, these
differential equations are replaced with memoryless Markovian models, where
discrete individuals can be members of a compartment and transition
stochastically. Classic stochastic simulation algorithms, such as Gillespie's
algorithm and the next reaction method, can be employed to solve these
Markovian models exactly. The intricate coupling between models at different
scales underscores the importance of multiscale modelling in infectious
diseases. However, several computational challenges arise when the multiscale
model becomes non-Markovian. In this paper, we address these challenges by
developing a novel exact stochastic simulation algorithm. We apply it to a
showcase multiscale system where all individuals share the same deterministic
within-host model while the population-level dynamics are governed by a
stochastic formulation. We demonstrate that as long as the within-host
information is harvested at a reasonable resolution, the novel algorithm we
develop will always be accurate. Moreover, the novel algorithm we develop is
general and can be easily applied to other multiscale models in (or outside)
the realm of infectious diseases.",2024-06-07,q-bio.PE,
http://arxiv.org/abs/2406.05052v1,"Stochastic programming provides a natural framework for modeling sequential
optimization problems under uncertainty; however, the efficient solution of
large-scale multistage stochastic programs remains a challenge, especially in
the presence of discrete decisions and nonlinearities. In this work, we
consider multistage stochastic mixed-integer nonlinear programs (MINLPs) with
discrete state variables, which exhibit a decomposable structure that allows
its solution using a column generation approach. Following a Dantzig-Wolfe
reformulation, we apply column generation such that each pricing subproblem is
an MINLP of much smaller size, making it more amenable to global MINLP solvers.
We further propose a method for generating additional columns that satisfy the
nonanticipativity constraints, leading to significantly improved convergence
and optimal or near-optimal solutions for many large-scale instances in a
reasonable computation time. The effectiveness of the tailored column
generation algorithm is demonstrated via computational case studies on a
multistage blending problem and a problem involving the routing of mobile
generators in a power distribution network.",2024-06-07,math.OC,
http://arxiv.org/abs/2406.05051v1,"Type Ia Supernovae (SNe Ia) are a critical tool in measuring the accelerating
expansion of the universe. Recent efforts to improve these standard candles
have focused on incorporating the effects of dust on distance measurements with
SNe Ia. In this paper, we use the state-of-the-art Dark Energy Survey 5 year
sample to evaluate two different families of dust models: empirical extinction
models derived from SNe Ia data, and physical attenuation models from the
spectra of galaxies. Among the SNe Ia-derived models, we find that a logistic
function of the total-to-selective extinction RV best recreates the
correlations between supernova distance measurements and host galaxy
properties, though an additional 0.02 magnitudes of grey scatter are needed to
fully explain the scatter in SNIa brightness in all cases. These
empirically-derived extinction distributions are highly incompatible with the
physical attenuation models from galactic spectral measurements. From these
results, we conclude that SNe Ia must either preferentially select extreme ends
of galactic dust distributions, or that the characterisation of dust along the
SNe Ia line-of-sight is incompatible with that of galactic dust distributions.",2024-06-07,astro-ph.CO,
http://arxiv.org/abs/2406.05043v1,"We study the dispersion process on the complete graph introduced in the
recent work \cite{de_dispersion_2023} under the mean-field framework. In
contrast to the probabilistic approach taken in \cite{de_dispersion_2023} and
many other related works, our focus is on the investigation of the large time
behavior of solutions of the associated kinetic mean-field system of nonlinear
ordinary differential equations (ODEs). We establish various analytical and
quantitative convergence results for the long time behaviour of the mean-field
system and related numerical illustrations are also provided.",2024-06-07,math.PR,
http://arxiv.org/abs/2406.05042v1,"Digital Twin has emerged as a promising paradigm for accurately representing
the electromagnetic (EM) wireless environments. The resulting virtual
representation of the reality facilitates comprehensive insights into the
propagation environment, empowering multi-layer decision-making processes at
the physical communication level. This paper investigates the digitization of
wireless communication propagation, with particular emphasis on the
indispensable aspect of ray-based propagation simulation for real-time Digital
Twins. A benchmark for ray-based propagation simulations is presented to
evaluate computational time, with two urban scenarios characterized by
different mesh complexity, single and multiple wireless link configurations,
and simulations with/without diffuse scattering. Exhaustive empirical analyses
are performed showing and comparing the behavior of different ray-based
solutions. By offering standardized simulations and scenarios, this work
provides a technical benchmark for practitioners involved in the implementation
of real-time Digital Twins and optimization of ray-based propagation models.",2024-06-07,eess.SP,
http://arxiv.org/abs/2406.05040v1,"Mechatronic systems are described by an interconnection of the
electromagnetic part, i.e., a static position-dependent nonlinear relation
between currents and forces, and the mechanical part, i.e., a dynamic relation
from forces to position. Commutation inverts a model of the electromagnetic
part of the system, and thereby removes the electromagnetic part from the
position control problem. Typical commutation algorithms rely on simplified
models derived from physics-based knowledge, which do not take into account
position dependent parasitic effects. In turn, these commutation related model
errors translate into position tracking errors, which limit the system
performance. Therefore, in this work, we develop a data-driven approach to
commutation using physics-guided neural networks (PGNNs). A novel PGNN model is
proposed which structures neural networks (NNs) to learn specific motor
dependent parasitic effects. The PGNN is used to identify a model of the
electromagnetic part using force measurements, after which it is analytically
inverted to obtain a PGNN-based commutation algorithm. Motivated by industrial
applications, we develop an input transformation to deal with systems with
fixed commutation, i.e., when the currents cannot be controlled. Real-life
experiments on an industrial coreless linear motor (CLM) demonstrate a factor
10 improvement in the commutation error in driving direction and a factor 4
improvement in the position error with respect to classical commutation in
terms of the mean--squared error (MSE).",2024-06-07,eess.SY,
http://arxiv.org/abs/2406.05037v1,"We study linear stability of exponential periodic solutions of a system of
singular amplitude equations associated with convective Turing bifurcation in
the presence of conservation laws, as arises in modern biomorphology models,
binary fluids, and elsewhere. Consisting of a complex Ginzburg-Landau equation
coupled with a singular convection-diffusion equation in ""mean modes""
associated with conservation laws, these were shown previously by the authors
to admit a constant-coefficient linearized stability analysis as in the
classical Ginzburg-Landau case -- albeit now singular in wave amplitude epsilon
-- yielding useful necessary conditions for stability, both of the exponential
functions as solutions of the amplitude equations, and of the associated
periodic pattern solving the underlying PDE. Here, we show by a delicate
two-parameter matrix perturbation analysis that (strict) satisfaction of these
necessary conditions is also sufficient for diffusive stability in the sense of
Schneider, yielding a corresponding result, and nonlinear stability, for the
underlying PDE. Moreover, we show that they may be interpreted as stability
along a non-normally hyperbolic slow manifold approximated by Darcy-type
reduction, together with attraction along transverse mean modes, connecting
with finite-time approximation theorems of Hacker-Schneider-Zimmerman.",2024-06-07,math.AP,
http://arxiv.org/abs/2406.05034v1,"First-order primordial curvature perturbations are known to induce
gravitational waves at the second-order, which can in turn probe the
small-scale curvature perturbations near the end of the inflation. In this
work, we extend the previous analysis in the Gaussian case into the
non-Gaussian case, with particular efforts to obtain some thumb rules of
sandwiching the associated peaks in gravitational waves induced from multiple
peaks of non-Gaussian curvature perturbations.",2024-06-07,astro-ph.CO,
http://arxiv.org/abs/2406.05031v1,"The existence of solitons -- stable, long-lived, and localized field
configurations -- is a generic prediction for ultralight dark matter. These
solitons, known by various names such as boson stars, axion stars, oscillons,
and Q-balls depending on the context, are typically treated as distinct
entities in the literature. This study aims to provide a unified perspective on
these solitonic objects for real or complex, scalar or vector dark matter,
considering self-interactions and nonminimal gravitational interactions. We
demonstrate that these solitons share universal nonrelativistic properties,
such as conserved charges, mass-radius relations, stability and profiles.
Without accounting for alternative interactions or relativistic effects,
distinguishing between real and complex scalar dark matter is challenging.
However, self-interactions differentiate real and complex vector dark matter
due to their different dependencies on the macroscopic spin density of dark
matter waves. Furthermore, gradient-dependent nonminimal gravitational
interactions impose an upper bound on soliton amplitudes, influencing their
mass distribution and phenomenology in the present-day universe.",2024-06-07,hep-ph,
http://arxiv.org/abs/2406.05030v1,"Generic open quantum systems are notoriously difficult to simulate unless one
looks at specific regimes. In contrast, classical dissipative systems can often
be effectively described by stochastic processes, which are generally less
computationally expensive. Here, we use the paradigmatic case of a dissipative
quantum oscillator to give a pedagogic introduction into the modelling of open
quantum systems using quasiclassical methods, i.e. classical stochastic methods
that use a 'quantum' noise spectrum to capture the influence of the environment
on the system. Such quasiclassical methods have the potential to offer insights
into the impact of the quantum nature of the environment on the dynamics of the
system of interest whilst still being computationally tractable.",2024-06-07,quant-ph,
http://arxiv.org/abs/2406.05028v1,"We propose and analyze a general goal-oriented adaptive strategy for
approximating quantities of interest (QoIs) associated with solutions to linear
elliptic partial differential equations with random inputs. The QoIs are
represented by bounded linear or continuously G\^ateaux differentiable
nonlinear goal functionals, and the approximations are computed using the
sparse grid stochastic collocation finite element method (SC-FEM). The proposed
adaptive strategy relies on novel reliable a posteriori estimates of the errors
in approximating QoIs. One of the key features of our error estimation approach
is the introduction of a correction term into the approximation of QoIs in
order to compensate for the lack of (global) Galerkin orthogonality in the
SC-FEM setting. Computational results generated using the proposed adaptive
algorithm are presented in the paper for representative elliptic problems with
affine and nonaffine parametric coefficient dependence and for a range of
linear and nonlinear goal functionals.",2024-06-07,math.NA,
http://arxiv.org/abs/2406.05026v1,"The most fundamental social interactions among humans occur face to face.
Their features have been extensively studied in recent years, owing to the
availability of high-resolution data on individuals' proximity. Mathematical
models based on mobile agents have been crucial to understand the
spatio-temporal organization of face-to-face interactions. However, these
models focus on dyadic relationships only, failing to characterize interactions
in larger groups of individuals. Here, we propose a model in which agents
interact with each other by forming groups of different sizes. Each group has a
degree of social attractiveness, based on which neighboring agents decide
whether to join. Our framework reproduces different properties of groups in
face-to-face interactions, including their distribution, the correlation in
their number, and their persistence in time, which cannot be replicated by
dyadic models. Furthermore, it captures homophilic patterns at the level of
higher-order interactions, going beyond standard pairwise approaches. Our work
sheds light on the higher-order mechanisms at the heart of human face-to-face
interactions, paving the way for further investigation of how group dynamics at
a microscopic scale affects social phenomena at a macroscopic scale.",2024-06-07,physics.soc-ph,
http://arxiv.org/abs/2406.05019v1,"The traditional method for estimating weather forecast sensitivity to initial
conditions uses adjoint models, which are limited to short lead times due to
linearization around a control forecast. The advent of deep-learning frameworks
enables a new approach using backpropagation and gradient descent to
iteratively optimize initial conditions to minimize forecast errors. We apply
this approach to forecasts of the June 2021 Pacific Northwest heatwave using
the GraphCast model, yielding over 90% reduction in 10-day forecast errors over
the Pacific Northwest. Similar improvements are found for Pangu-Weather model
forecasts initialized with the GraphCast-derived optimal, suggesting that model
error is not an important part of the initial perturbations. Eliminating small
scales from the initial perturbations also yields similar forecast
improvements. Extending the length of the optimization window, we find forecast
improvement to about 23 days, suggesting atmospheric predictability at the
upper end of recent estimates.",2024-06-07,physics.ao-ph,
http://arxiv.org/abs/2406.05016v1,"The synergy of judiciously engineered nanostructures and complex topology of
light creates unprecedented opportunities for tailoring light-matter
interactions on the nanoscale. Electromagnetic waves can carry multiple units
of angular momentum per photon, stemming from both spin and orbital angular
momentum contributions, offering a potential route for modifying the optical
transition selection rules. However, the size difference between a vortex beam
and quantum objects limits the interaction strength and the angular momentum
exchange. Here, we demonstrate the sub-diffraction-limited focusing of a vortex
beam using the high in-plane wave number modes present in hyperbolic
metamaterials. The spin-orbit interaction within the hyperbolic structure gives
rise to the formation of an optical skyrmion with a deep subwavelength
structure, which may enable the exploration of new light-matter interaction
phenomena.",2024-06-07,physics.optics,
http://arxiv.org/abs/2406.05014v1,"Recent work conceptualized root cause analysis (RCA) of anomalies via
quantitative contribution analysis using causal counterfactuals in structural
causal models (SCMs). The framework comes with three practical challenges: (1)
it requires the causal directed acyclic graph (DAG), together with an SCM, (2)
it is statistically ill-posed since it probes regression models in regions of
low probability density, (3) it relies on Shapley values which are
computationally expensive to find.
  In this paper, we propose simplified, efficient methods of root cause
analysis when the task is to identify a unique root cause instead of
quantitative contribution analysis. Our proposed methods run in linear order of
SCM nodes and they require only the causal DAG without counterfactuals.
Furthermore, for those use cases where the causal DAG is unknown, we justify
the heuristic of identifying root causes as the variables with the highest
anomaly score.",2024-06-07,stat.ML,
http://arxiv.org/abs/2406.05012v1,"The TrendLSW R package has been developed to provide users with a suite of
wavelet-based techniques to analyse the statistical properties of nonstationary
time series. The key components of the package are (a) two approaches for the
estimation of the evolutionary wavelet spectrum in the presence of trend; and
(b) wavelet-based trend estimation in the presence of locally stationary
wavelet errors via both linear and nonlinear wavelet thresholding; and (c) the
calculation of associated pointwise confidence intervals. Lastly, the package
directly implements boundary handling methods that enable the methods to be
performed on data of arbitrary length, not just dyadic length as is common for
wavelet-based methods, ensuring no pre-processing of data is necessary. The key
functionality of the package is demonstrated through two data examples, arising
from biology and activity monitoring.",2024-06-07,stat.ME,
http://arxiv.org/abs/2406.05010v1,"Graph (or network) is a mathematical structure that has been widely used to
model relational data. As real-world systems get more complex, multilayer (or
multiple) networks are employed to represent diverse patterns of relationships
among the objects in the systems. One active research problem in multilayer
networks analysis is to study the common invariant subspace of the networks,
because such common invariant subspace could capture the fundamental structural
patterns and interactions across all layers. Many methods have been proposed to
estimate the common invariant subspace. However, whether real-world multilayer
networks share the same common subspace remains unknown. In this paper, we
first attempt to answer this question by means of hypothesis testing. The null
hypothesis states that the multilayer networks share the same subspace, and
under the alternative hypothesis, there exist at least two networks that do not
have the same subspace. We propose a Weighted Degree Difference Test, derive
the limiting distribution of the test statistic and provide an analytical
analysis of the power. Simulation study shows that the proposed test has
satisfactory performance, and a real data application is provided.",2024-06-07,stat.ME,
http://arxiv.org/abs/2406.05009v1,"We investigate the generic transport in a one-dimensional strongly correlated
fermionic chain beyond linear response. Starting from a Gaussian wave packet
with positive momentum on top of the ground state, we find that the numerical
time evolution splits the signal into at least three distinct fractional
charges moving with different velocities. A fractional left-moving charge is
expected from conventional Luttinger liquid theory, but for the prediction of
the two separate right-moving packets the nonlinearity of the dispersion must
also be taken into account. This out-of-equilibrium protocol therefore allows a
direct measurement of nonlinear interaction parameters, which also govern
threshold singularities of dynamic response functions. The nonlinear Luttinger
Liquid theory also predicts the correct dynamics at low energies, where it
agrees with the conventional Luttinger liquid. Moreover, at high energies, the
wave packet dynamics reveals signatures of composite excitations containing
two-particle bound states. Our results uncover a simple strategy to probe the
nonlinear regime in time-resolved experiments in quantum wires and
ultracold-atom platforms.",2024-06-07,cond-mat.str-el,
http://arxiv.org/abs/2406.05008v1,"Heavy and light particles are commonly found in many natural phenomena and
industrial processes, such as suspensions of bubbles, dust, and droplets in
incompressible turbulent flows. Based on a recent machine learning approach
using a diffusion model that successfully generated single tracer trajectories
in three-dimensional turbulence and passed most statistical benchmarks across
time scales, we extend this model to include heavy and light particles. Given
the particle type - tracer, light, or heavy - the model can generate synthetic,
realistic trajectories with correct fat-tail distributions for acceleration,
anomalous power laws, and scale dependent local slope properties. This work
paves the way for future exploration of the use of diffusion models to produce
high-quality synthetic datasets for different flow configurations, potentially
allowing interpolation between different setups and adaptation to new
conditions.",2024-06-07,physics.flu-dyn,
http://arxiv.org/abs/2406.05005v1,"Classification of gamma-ray bursts (GRBs) has been a long-standing puzzle in
high-energy astrophysics. Recent observations challenge the traditional short
vs. long viewpoint, where long GRBs are thought to originate from the collapse
of massive stars and short GRBs from compact binary mergers. Machine learning
(ML) algorithms have been instrumental in addressing this problem, revealing
five distinct GRB groups within the Swift/BAT light curve data, two of which
are associated with kilonovae (KNe). We corroborate these five classes by
extending this analysis to the Fermi/GBM data using unsupervised ML techniques.
These five clusters are well separated in fluence-duration plane, hinting at a
potential link between fluence, duration and complexities (or structures) in
the light curves of GRBs. Further, we confirm two distinct classes of
KN-associated GRBs. The presence of GRB 170817A in one of the two
KNe-associated clusters lends evidence to the hypothesis that this class of
GRBs could potentially be produced by binary neutron star (BNS) mergers. The
second KN-associated GRB cluster could potentially originate from NS-BH
mergers. Future multimessenger observations of compact binaries in
gravitational waves (GWs) and electromagnetic waves can be paramount in
understanding these clusters better.",2024-06-07,astro-ph.HE,
http://arxiv.org/abs/2406.05004v1,"A countable discrete group is called Choquet-Deny if for any non-degenerate
probability measure on the group, the corresponding space of bounded harmonic
functions is trivial. Building on the previous work of Jaworski, a complete
characterization of Choquet-Deny groups was recently achieved by Frisch,
Hartman, Tamuz, and Ferdowski. In this article, we extend the study of the
Choquet-Deny property to the framework of discrete measured groupoids. Our
primary result offers a complete characterization of this property in terms of
the isotropy groups and the equivalence relation associated with the given
groupoid. Additionally, we use the implications derived from our main theorem
to classify the Choquet-Deny property of transformation groupoids.",2024-06-07,math.FA,
http://arxiv.org/abs/2406.05002v1,"The study of effective connectivity (EC) is essential in understanding how
the brain integrates and responds to various sensory inputs. Model-driven
estimation of EC is a powerful approach that requires estimating global and
local parameters of a generative model of neural activity. Insights gathered
through this process can be used in various applications, such as studying
neurodevelopmental disorders. However, accurately determining EC through
generative models remains a significant challenge due to the complexity of
brain dynamics and the inherent noise in neural recordings, e.g., in
electroencephalography (EEG). Current model-driven methods to study EC are
computationally complex and cannot scale to all brain regions as required by
whole-brain analyses. To facilitate EC assessment, an inference algorithm must
exhibit reliable prediction of parameters in the presence of noise. Further,
the relationship between the model parameters and the neural recordings must be
learnable. To progress toward these objectives, we benchmarked the performance
of a Bi-LSTM model for parameter inference from the Jansen-Rit neural mass
model (JR-NMM) simulated EEG under various noise conditions. Additionally, our
study explores how the JR-NMM reacts to changes in key biological parameters
(i.e., sensitivity analysis) like synaptic gains and time constants, a crucial
step in understanding the connection between neural mechanisms and observed
brain activity. Our results indicate that we can predict the local JR-NMM
parameters from EEG, supporting the feasibility of our deep-learning-based
inference approach. In future work, we plan to extend this framework to
estimate local and global parameters from real EEG in clinically relevant
applications.",2024-06-07,q-bio.NC,
http://arxiv.org/abs/2406.04997v1,"Self-supervised learning (SSL) speech models have achieved remarkable
performance in various tasks, yet the biased outcomes, especially affecting
marginalized groups, raise significant concerns. Social bias refers to the
phenomenon where algorithms potentially amplify disparate properties between
social groups present in the data used for training. Bias in SSL models can
perpetuate injustice by automating discriminatory patterns and reinforcing
inequitable systems. This work reveals that prevalent SSL models inadvertently
acquire biased associations. We probe how various factors, such as model
architecture, size, and training methodologies, influence the propagation of
social bias within these models. Finally, we explore the efficacy of debiasing
SSL models through regularization techniques, specifically via model
compression. Our findings reveal that employing techniques such as row-pruning
and training wider, shallower models can effectively mitigate social bias
within SSL model.",2024-06-07,eess.AS,
http://arxiv.org/abs/2406.04994v1,"Mainly motivated by the problem of modelling directional dependence
relationships for multivariate count data in high-dimensional settings, we
present a new algorithm, called learnDAG, for learning the structure of
directed acyclic graphs (DAGs). In particular, the proposed algorithm tackled
the problem of learning DAGs from observational data in two main steps: (i)
estimation of candidate parent sets; and (ii) feature selection. We
experimentally compare learnDAG to several popular competitors in recovering
the true structure of the graphs in situations where relatively moderate sample
sizes are available. Furthermore, to make our algorithm is stronger, a
validation of the algorithm is presented through the analysis of real datasets.",2024-06-07,stat.ME,
http://arxiv.org/abs/2406.04993v1,"INTRODUCTION: The pharmacological treatment of Major Depressive Disorder
(MDD) relies on a trial-and-error approach. We introduce an artificial
intelligence (AI) model aiming to personalize treatment and improve outcomes,
which was deployed in the Artificial Intelligence in Depression Medication
Enhancement (AIDME) Study. OBJECTIVES: 1) Develop a model capable of predicting
probabilities of remission across multiple pharmacological treatments for
adults with at least moderate major depression. 2) Validate model predictions
and examine them for amplification of harmful biases. METHODS: Data from
previous clinical trials of antidepressant medications were standardized into a
common framework and included 9,042 adults with moderate to severe major
depression. Feature selection retained 25 clinical and demographic variables.
Using Bayesian optimization, a deep learning model was trained on the training
set, refined using the validation set, and tested once on the held-out test
set. RESULTS: In the evaluation on the held-out test set, the model
demonstrated achieved an AUC of 0.65. The model outperformed a null model on
the test set (p = 0.01). The model demonstrated clinical utility, achieving an
absolute improvement in population remission rate in hypothetical and actual
improvement testing. While the model did identify one drug (escitalopram) as
generally outperforming the other drugs (consistent with the input data), there
was otherwise significant variation in drug rankings. On bias testing, the
model did not amplify potentially harmful biases. CONCLUSIONS: We demonstrate
the first model capable of predicting outcomes for 10 different treatment
options for patients with MDD, intended to be used at or near the start of
treatment to personalize treatment. The model was put into clinical practice
during the AIDME randomized controlled trial whose results are reported
separately.",2024-06-07,q-bio.NC,
http://arxiv.org/abs/2406.04992v1,"The quantum-classical hybrid Variational Quantum Eigensolver (VQE) algorithm
is recognized to be the method of choice to obtain ground state energies of
quantum many-body systems in the noisy intermediate scale quantum (NISQ) era.
This study not only extends the VQE algorithm to the relativistic regime, but
also calculates a property other than energy, namely the molecular permanent
electric dipole moment (PDM). We carry out 18-qubit quantum simulations to
obtain ground state energies as well as PDMs of single-valence diatomic
molecules, ranging from the light BeH to the heavy radioactive RaH molecule. We
investigate the correlation trends in these systems as well as access the
precision in our results. Furthermore, we measure the PDM of the moderately
heavy SrH and SrF molecules on the optimized unitary coupled cluster state,
using the state-of-the-art IonQ Aria-I quantum computer in an active space of 6
qubits. The associated quantum circuits for these computations were extensively
optimized in view of limitations imposed by NISQ hardware. To that end, we
employ an array of techniques, including the use of point group symmetries,
integrating ZX-Calculus into our pipeline-based circuit optimization, and
energy sort VQE procedure. Through these methods, we compress our 6-qubit
quantum circuit from 280 two-qubit gates to 37 two-qubit gates (with a marginal
trade-off of 0.33 and 0.31 percent in the PDM for SrH and SrF in their
respective 6-spin orbital active spaces). We anticipate that our
proof-of-concept demonstration lays the groundwork for future quantum hardware
calculations involving heavy atoms and molecules.",2024-06-07,physics.atom-ph,
http://arxiv.org/abs/2406.04991v1,"Campana introduced a notion of Campana rational connectedness for Campana
orbifolds. Given a Campana fibration over a complex curve, we prove that a
version of weak approximation for Campana sections holds at places of good
reduction when the general fiber satisfies a slightly stronger version of
Campana rational connectedness. Campana also conjectured that any Fano orbifold
is Campana rationally connected; we verify a stronger statement for toric
Campana orbifolds. A key tool in our study is log geometry and moduli stacks of
stable log maps.",2024-06-07,math.AG,
http://arxiv.org/abs/2406.04986v1,"In a Bell scenario, a classical verifier interacts with two non-communicating
provers, producing a correlation. Certain correlations allow the verifier to
certify, or self-test, the underlying quantum state and measurements. Notably,
the family of tilted-CHSH inequalities has been used to self-test any two-qubit
state. Self-tests underpin numerous device-independent protocols, however, a
significant drawback of such applications is the non-communicating assumption.
To address the non-communication assumption Kalai et al. (STOC'23) give a
procedure which compiles a bipartite Bell scenario into a 2-round interaction
between a verifier and a single computationally bounded prover. In this work,
we formalize self-testing for compiled Bell scenarios. We prove the maximal
quantum value is preserved under compilation for the family of tilted-CHSH
inequalities, and that any maximal violation constitutes a compiled self-test.
More specifically, we establish the existence of an efficient isometry
recovering the state and measurements in the second round.",2024-06-07,quant-ph,
http://arxiv.org/abs/2406.04985v1,"Integrated sensing and communications (ISAC) has been considered one of the
new paradigms for sixth-generation (6G) wireless networks. In the
millimeter-wave (mmWave) ISAC system, hybrid beamforming (HBF) is considered an
emerging technology to exploit the limited number of radio frequency (RF)
chains in order to reduce the system hardware cost and power consumption.
However, the HBF structure reduces the spatial degrees of freedom for the ISAC
system, which further leads to increased interference between multiple users
and between users and radar sensing. To solve the above problem, rate split
multiple access (RSMA), which is a flexible and robust interference management
strategy, is considered. We investigate the joint common rate allocation and
HBF design problem for the HBF-based RSMA-assisted mmWave ISAC scheme. We
propose the penalty dual decomposition (PDD) method coupled with the weighted
mean squared error (WMMSE) minimization method to solve this high-dimensional
non-convex problem, which converges to the Karush-Kuhn-Tucker (KKT) point of
the original problem. Then, we extend the proposed algorithm to the HBF design
based on finite-resolution phase shifters (PSs) to further improve the energy
efficiency of the system. Simulation results demonstrate the effectiveness of
the proposed algorithm and show that the RSMA-ISAC scheme outperforms other
benchmark schemes.",2024-06-07,eess.SP,
http://arxiv.org/abs/2406.04982v1,"Does the cross-product of the position and the electromagnetic momentum
density, g, include the optical spin momentum? We answer this long-standing
question in the affirmative by evaluating, explicitly, the torque exerted on a
particle by a beam of light carrying angular momentum.",2024-06-07,physics.optics,
http://arxiv.org/abs/2406.04976v1,"The system of a charged impurity in an interacting Bose gas has gained
significant attention due to the long-range ion-atom interactions and the study
of transport properties. Here, the ground state energy of a charged Bose
polaron is calculated within the Bogoliubov approximation for both the
Fr\""ohlich and beyond-Fr\""ohlich Hamiltonians using a generalized Feynman
variational path-integral approach, which obtained accurate results for other
polaron problems. The generalized approach, which was used to improve the
energy result for the neutral polaron, has resulted in a minor improvement,
indicating that Feynman's approach is sufficient when the impurity-boson
interaction is long-range. Beyond-Fr\""ohlich corrections results in the
emergence of a divergence in the polaronic energy indicating a transition
between the repulsive and attractive polaron regime. The path-integral approach
with the beyond-Fr\""ohlich Hamiltonian is also compared to a field-theory
calculation from Christensen et al, 2021. The validity of the Bogoliubov
approximation is investigated. The optical absorption has also been calculated
within the Bogoliubov approximation for weak ion-atom interactions, and the
effect of finite temperature has been studied. We show that the coupling of the
ion to an oscillating external electric field offers a straigtforward
experimental probe for the charged polaron in a Bose gas, different from but
complementary to existing spectroscopic techniques.",2024-06-07,cond-mat.quant-gas,
